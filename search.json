[
  {
    "objectID": "01-PO91Q.html",
    "href": "01-PO91Q.html",
    "title": "Week 1",
    "section": "",
    "text": "Introduction to R",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO91Q.html#installation",
    "href": "01-PO91Q.html#installation",
    "title": "Week 1",
    "section": "Installation",
    "text": "Installation\nToday we start working with R and the first step is to install the program. Please follow these instructions:\n\nGo to https://cran.r-project.org/mirrors.html and select a server from which you want to download R. It is convention to do this from the server which is nearest to you. Follow on-screen instructions and install the program.\nGo to https://rstudio.com/products/rstudio/download/ and download RStudio Desktop which is free. Install the program.\nNow open RStudio - you do not need to open R itself, as we will be operating it through RStudio.\n\n\nWhilst you need to install both R and RStudio, we will never be working with R directly. Instead, we will be operating it through RStudio.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO91Q.html#r---getting-started",
    "href": "01-PO91Q.html#r---getting-started",
    "title": "Week 1",
    "section": "R - Getting Started",
    "text": "R - Getting Started\nIn this worksheet and also in all other presentations and documents I use on this module, I am using two different fonts:\n\nFont for plain text\nA typewriter font for R functions, values, etc.\n\nI am also regularly including “screenshots” of operations in R with their output. Whenever you see these, please replicate them on your own computer. To start, let’s have a look at RStudio itself. When you open the programme, you are presented with the following screen:\n\n\n\n\n\n\nFigure 1: RStudio\n\n\n\nIt has – for now – three components to it. On the left hand-side you see the so-called Console into which you can enter the commands, and in which also most of the results will be displayed. On the right hands side, you see the Workspace which consists of an upper and a lower window. The upper window has three tabs in it. The tab Environment will provide you with a list of all the data sets you have loaded into R, and also of the objects and values you create (more on that later). Under the History tab, you find a history (I know, who would have thought it) of all the commands you have used. This can be very useful to retrace your steps. In the Connections tab you can connect to online sources. We will not use this tab.\nIn the lower window, you have five tabs. Under Files you find the file structure of your computer. Once you have set a working directory (more on that in a moment), you can also view the files in your working directory here which gives you a good overview of the files you need to refer to for a particular project. The Plots tab will display the graphs we will be producing. Packages form the heart and soul of R and they make the program as powerful as it is (again, more on that later). RStudio also has a Help function, which is rarely very illuminating. I usually search for stuff online on “stackexchange”, as there is a large community of R users out there who share their knowledge and solutions to problems. We won’t use the last tab Viewer.\n\nIntroduction to R Studio\nIf you can’t get enough of my delightful German accent, then I have some videos for you in which I go through the respective components of the worksheet on screen. Here is the first:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO91Q.html#rscript",
    "href": "01-PO91Q.html#rscript",
    "title": "Week 1",
    "section": "RScript",
    "text": "RScript\nIf you read the previous section carefully, you will have noticed that I wrote that you can enter the commands” in the Console. You can, but you shouldn’t. What you should be using instead is an RScript. An RScript is a list of commands you use for a project (an essay, your dissertation, an article) to calculate quantities of interest, such as descriptive statistics in the form of mean, median and mode, and produce graphs.\nOne of the foundations of scientific research is “reproducibility”“, or”replicability”. This means that “sufficient information exists with which to understand, evaluate, and build upon a prior work if a third party could replicate the results without any additional information from the author.” King (1995, p. 444, emphasis removed) This principle applies in academia more generally, because only if you understand what a person has done before you, you can pick their work up whether they left it, and push the boundaries of knowledge further. But a bit closer to home, it is also relevant for conducting quantitative research in assessments. We require you to submit an RScript (or a “do file” if you use Stata) now together with your actual essay. This is not only to check what you have done; data preparation is often the most time-consuming part (as you will soon discover), and this is a way to gain recognition for this work. So it is actually to your advantage, and not a mere plagiarism check.\nThe creation of an RScript will allow you to open the raw data, and by running the script, to bring it to exactly where you left off. This saves you saving data sets which can take up a lot of work. If you back the script up properly, you also have an insurance against losing all your work a day before the assessment is due.\nTo create an RScript, click File \\(\\rightarrow\\) New File \\(\\rightarrow\\) RScript. A fourth window opens, and your screen will now look something like this:\n\n\n\n\n\n\nFigure 2: The RScript Window\n\n\n\nYou can now write your commands in the RScript, where a new line (for now) means a new command. If you want to execute a command, put the cursor on the line the command is on and press “command” / “enter” simultaneoulsy on a Mac and “Ctrl” / “Enter” on Windows.\nIf you precede a line with #, you can write annotations to yourself, for example explaining what you do with a particular command. More on this in the next sub-section.\nFigure @ref(fig:RScriptex) shows the start of the RScript for this worksheet. I prefer a dark background, it’s easier on the eyes, especially when you work with R for long periods. You can change the settings in: Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Appearance \\(\\rightarrow\\) Twilight.\n\n\n\n\n\n\nFigure 3: Example of an RScript\n\n\n\n\nMore Themes\nIf you copy and paste the following code chunks into your “Console” and run one at a time, you will have even more themes1 to choose from:\n\ninstall.packages(\n  \"rsthemes\",\n  repos = c(gadenbuie = 'https://gadenbuie.r-universe.dev', getOption(\"repos\"))\n)\n\nrsthemes::install_rsthemes()\n\nYou can also download Flo’s Dark Theme2 and then “add” it at the bottom of the “Appearance” menu.\n\n\nAppearance\n\n\n\n\nRScript Structure\nWell, I am German, and I like things neat and tidy, so I feel almost compelled to discuss how to properly organise an RScript. But apart from genetical dispositions, a well-organised RScript is also very much in the spirit of reproducibility. It simply makes sense to structure an RScript in such a way that another researcher is able to easily read and understand it.\nFirst of all, which commands to include? If you introduce me to your current girlfriend or boyfriend, I have no interest in learning about all your past relationships; they have not worked out. In a similar fashion, nobody wants to read through lines of code that are irrelvant. So you will only include in the RScript those commands which produce the output you actually include in the essay or article.\nI stated above that if you precede a line with #, you can write annotations to yourself. This is also a useful way to structure an RScript, for example into exercise numbers, sections of an essay /article, or different stages of data preparation (which we will be doing in due course).\n\nRScript Structure",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO91Q.html#first-steps-in-r",
    "href": "01-PO91Q.html#first-steps-in-r",
    "title": "Week 1",
    "section": "First Steps in R",
    "text": "First Steps in R\nBut enough of the preliminary talk, let’s get started in R. In principle, you can think of R as a massive and powerful calculator. So I will use it as such to start of with. If you want to know what the sum of 5 and 3 is, you type (in the RScript, not the Console):\nand press “command” / “enter” (or “Ctrl” / “enter” if you are on Windows). In everything that is to follow, commands will be shown in their own individual boxes. These have a clipboard button on the right, so you can copy and paste the code into your own RScript. The output is presented in a separate box directly underneath. There is no clipboard button, as R will render this result in your own console when you run the code. So, including the result, the calculation would look like this:\n\n5+3\n\nwhere the [1] indicates that the 8 is the first component of the result. In this case, we only have one component, so it’s superfluous really, but we will soon encounter situations in which results can have a number of different items.\n\nYou can copy the code from this page by clicking the clipboard icon in the top-right hand corner. You can then paste it into your RScript.\n\nA fundamental component of R is objects. You can define an object by way of a reversed arrow, and you can assign values, characters, or functions to them. If we want to assign the sum of 5 and 3 to an object called result, for example, we call3\n\nresult &lt;- 5+3\n\nIf we now call the object, R will return its value, 8.\n\nresult\n\n[1] 8\n\n\n\nMake a habit of adding a note underneath each code chunk in your RScript (preceded with a #) in which you translate the code into plain English. This is especially useful for the lengthy complex chunks.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO91Q.html#the-working-directory",
    "href": "01-PO91Q.html#the-working-directory",
    "title": "Week 1",
    "section": "The Working Directory",
    "text": "The Working Directory\nIt is imperative that you create a suitable filing system to organise the materials for all of your modules. At the very least you should have a folder called “University” or similar, in which you have a sub-folder for each module you take.\nIn those modules in which you are working with R, you need to extend this system a little. I have created a schematic of what I have in mind in Figure @ref(fig:folder).\n\n\n\n\n\n\nFigure 4: Folder Structure\n\n\n\nYou see that there is a sub-folder for each week of the module, and that each of these folders is divided into lecture and seminar in turn. Into these you can place the lecture and seminar materials, respectively. Create this system now for PO11Q.\nR works with so-called Working Directories. You can think of these as drawers from which R takes everything it needs to conduct the analysis (such as the data set), and into which it puts everything it produces (such as graph plots). As this will be an R-specific drawer within the seminar, create yet another sub-folder in your seminar folder, and call it something suitable, such as “PO11Q_Seminar_Week 1”. Do NOT call this “Working Directory”, as you will have many of those, rendering this name completely meaningless. Save the file EU.xlsx into this folder. Data are taken from European Comission (n.d.).\n\nPlease set up this structure now. If I find you using a random folder on your desktop named “working directory” in the coming weeks, I am going to implode! I mean it.\n\nNow we need to tell R to use this folder. If you know the file structure of your computer you can simply use the setwd() command, and enter the path. Here is an example from my computer:\n\nsetwd(\"~/Warwick/Modules/PO91Q/Seminars/Week 1/R Week 1\")\n\nIf you don’t know the file structure of your computer, then you can click Session \\(\\rightarrow\\) Set Working Directory \\(\\rightarrow\\) Choose Directory.\n\nWorking Directory",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO91Q.html#r-packages",
    "href": "01-PO91Q.html#r-packages",
    "title": "Week 1",
    "section": "R Packages",
    "text": "R Packages\nIt would be difficult to overstate the importance of packages in R. The program has a number of “base” functions which enable the user to do many different basic things, but packages are extensions that allow you to do pretty much anything and everything with this software - this is one of the reasons why I love it so much. The first package we need to use will enable us to load an Excel sheet into R. It is called readxl. You can install any package with the command install.packages() where the package name goes, wrapped in quotation marks, into the brackets:\n\ninstall.packages(\"readxl\")\n\nWe can then load this package into our library with the library() command.\n\nlibrary(readxl)\n\n\nOnce you close R at the end of a session, the library will be reset. When you reopen R, you have to load the packages you require again. But you do not have to install them again.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO91Q.html#saving",
    "href": "01-PO91Q.html#saving",
    "title": "Week 1",
    "section": "Saving",
    "text": "Saving\nPlease now save this RScript into the same folder (working directory) as the raw data. When R asks before closing, there is no need to save the workspace or the data, as running the RScript on the raw data will bring you precisely to where you left off.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO91Q.html#core-exercises",
    "href": "01-PO91Q.html#core-exercises",
    "title": "Week 1",
    "section": "Core Exercises",
    "text": "Core Exercises\n\nExecute the following lines of command. Each time try and explain what is happening, using statistical terms as in the slides and textbooks. If you know another statistical software, spot commonalities and differences with R. Experiment variations on your own to make sure your interpretations are right.\n\n\na&lt;-1\nclass(a)\na\nb&lt;-2\nb\nc&lt;-a+b\nc\nd&lt;-2*c\nd\ne&lt;-c(10,20,30)\nclass(e)\ne\ne*10\nf&lt;-c(\"Small\",\"Medium\",\"Big\")\nclass(f)\nf\ne[1]\nf[3]\nmean(e)\nmean(f)\nsd(e)\n?sd\nexample(sd)\ng&lt;-c(e,f)\ng\nh&lt;-rbind(e,f)\ni&lt;-cbind(e,f)\nclass(h)\nh[1,]\ni[,2]\nh[1,2]\nls()\nls.str()\n\n\nDownload the “European Social Survey” dataset from https://www.europeansocialsurvey.org/data. You will need to register and receive a confirmation email. Select round 9 (2018), SPSS version (.sav). Download also:\n\nthe questionnaire\nthe variable list\nthe “showcards”\nthe Source project instructions\n\nTake a bit of time to make sure you understand what each of these documents is for.\nCan you tell:\n\nwhat is(are) the population(s) of reference?\nhow was(were) the sample(s) composed?\nhow do interviewers reach respondents?\nwhat is the method for questionnaire administration?\n\nWe can now pick up where we left on the worksheet. Make sure that you have created an appropriate folder structure and set your working directory. Also ensure you have loaded the foreign package.\n\n\nExecute: table(cntry). What uses could you make of this variable in research?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO91Q.html#going-further",
    "href": "01-PO91Q.html#going-further",
    "title": "Week 1",
    "section": "Going Further",
    "text": "Going Further\n\nIn the ESS questionnaire, showcards and variable list, identify the questions about:\n\nlanguages spoken by respondent R\nR’s educational background\nR’s relationship to politics\n\nHow many questions about each topic can you find? How suitable would this survey be to study these three topics?\nSelect one of the three topics above according to your interest. Select a set of three variables of particular interest (called X, Y and Z below). Make sure you understand the meaning of X, Y and Z.\n\n\nX\nY\nZ\n\nWhat do these return?\n\ntable(X)\nplot(X)\n\n\nDownload the “British Social Attitudes” Survey 2016 from the UK Data service website. You will need to register, validate your registration through email, indicate that this is within an academic course and agree with conditions.\n\nChoose .tab and .sav format.\nOpen the .tab dataset with Excel and save as .xlsx file. Import this into R with the readxl package.\nImport the SPSS file into R (same as above with ESS)\n\nTry to understand the differences between the 3 softwares, their respective advantages and disadvantages. How does the BSA compare with the ESS regarding the three topics listed above?\nDownload the World Values Study 7 from Gesis: https://dbk.gesis.org/dbksearch/sdesc2.asp?no=4804. Select all relevant files. How does the EVS compare with the ESS and BSA regarding the 3 topics above?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "01-PO91Q.html#footnotes",
    "href": "01-PO91Q.html#footnotes",
    "title": "Week 1",
    "section": "",
    "text": "Source: https://www.garrickadenbuie.com/project/rsthemes/↩︎\nThis is a variation of the Dracula Theme.↩︎\nTo “call” means to execute a command.↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Week 1</span>"
    ]
  },
  {
    "objectID": "02-PO91Q.html",
    "href": "02-PO91Q.html",
    "title": "Week 2",
    "section": "",
    "text": "Self-Reflection Questions – Group Work\nI do not usually provide answers to these questions, as I am expecting you to either attend the seminar, or – in case you cannot make it – to contact me for advice or to share notes with another student. But I have nonetheless put this document together, so that you get an idea about my expectation horizon for answering these. Please bear this in mind in weeks to come.\nAnswers to Self-Reflection Questions (available from October 15)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO91Q.html#opening-your-data-set",
    "href": "02-PO91Q.html#opening-your-data-set",
    "title": "Week 2",
    "section": "Opening your Data Set",
    "text": "Opening your Data Set\nWe are now ready to open a data set in R - where it is called a “data frame”. For this, we create a new object EU, and ask R to read “Sheet 1”” of the Excel file “EU.xlsx” which we placed in the working directory earlier\n\nlibrary(readxl)\nEU &lt;- read_excel(\"EU.xlsx\", sheet=\"Sheet1\")\n\nWe can now use our data in R!\n\nLoading the Data Set\n\n\n\n\nPlease do not use the “Import Dataset” button in the Environment, but do this properly, manually. We sometimes need to set options for importing data sets, and the “pointy, clicky” approach won’t be able to offer you what you need.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO91Q.html#viewing-the-data",
    "href": "02-PO91Q.html#viewing-the-data",
    "title": "Week 2",
    "section": "Viewing the Data",
    "text": "Viewing the Data\nUnless you have been cheeky and opened the file in Excel to have a look, you have no idea yet, what the data look like. So it’s a good idea to view the data frame before doing anything with it. You can use the View() command to see the data frame:\n\nView(EU)\n\nIf you only want to see the first 6 observations of each variable, use the head() command:\n\nhead(EU)\n\n# A tibble: 6 × 5\n  country     pop18 access   area GDP_2015\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 Belgium  11413058   1951  30280  4.66e11\n2 Bulgaria  7050034   2007 108560  1.22e11\n3 Czechia  10610055   2004  77230  3.19e11\n4 Denmark   5781190   1973  42430  2.46e11\n5 Germany  82850000   1951 348540  3.60e12\n6 Estonia   1319133   2004  42390  3.51e10\n\n\nIf you simply want to know the variable names in the data frame, type:\n\nnames(EU)\n\n[1] \"country\"  \"pop18\"    \"access\"   \"area\"     \"GDP_2015\"\n\n\nThe next one is a very important command, because it reveals not only the variable names and their first few observations, but also the nature of each variable (numerical, character, etc.). It is the str() command, where “str” stands for structure:\n\nstr(EU)\n\ntibble [28 × 5] (S3: tbl_df/tbl/data.frame)\n $ country : chr [1:28] \"Belgium\" \"Bulgaria\" \"Czechia\" \"Denmark\" ...\n $ pop18   : num [1:28] 11413058 7050034 10610055 5781190 82850000 ...\n $ access  : num [1:28] 1951 2007 2004 1973 1951 ...\n $ area    : num [1:28] 30280 108560 77230 42430 348540 ...\n $ GDP_2015: num [1:28] 4.66e+11 1.22e+11 3.19e+11 2.46e+11 3.60e+12 ...\n\n\nYou can see that R has recognised most variables as numerical, one is displayed as a character variable. This is appropriate for some variables, such as pop18, but not for the ordinal variable access which is ordinal. We need to recode it, and all other variables we are unhappy with.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO91Q.html#variable-types-in-r",
    "href": "02-PO91Q.html#variable-types-in-r",
    "title": "Week 2",
    "section": "Variable Types in R",
    "text": "Variable Types in R\nR distinguishes between a number of different variable types and here is a broad overview of them. This will help you in deciding which descriptive statistics to calculate, or into which variable type you need to recode (next step) to achieve what you want. There are two general types:\n\nnumeric – numbers\ncharacter (also called string) – letters\n\nWithin numeric we can distinguish between the following:\n\nfactor - nominal\nordered factor - ordinal\ninteger - numeric, but only “whole” numbers (discrete)\nnumeric - any number (interval or ratio)\n\nNumerical variables are already in the data set, we have to attend to nominal and ordinal variables.\n\nNominal Variables\nIn terms of the variable types we encountered in the lecture this week, the country name is a nominal variable. So we need to tell R to turn this into a factor variable. We do this as follows:\n\nEU$country = factor(EU$country)\n\n\n\nOrdinal Variables\nAs mentioned above, the variable access should be ordinal, and therefore has to be turned into an ordered factor. The command which follows is almost identical to producing a factor variable, only that we add the option ordered = TRUE at the end:\n\nEU$access_fac = factor(EU$access, ordered = TRUE)\n\nIf you are familiar with European Studies, you will know that each accession wave has got a particular name. The 1973 enlargement, for example, is called the “First Enlargement”, the 1981 wave the Mediterranean Enlargement, and so forth. Let us create a new variable which uses these names instead of the years.\nThis process is a little more involved, and requires a new package to be installed and loaded: dplyr. This package is part of the so-called tidyverse which is a suite of packages designed to make working with R simpler and commands shorter. You can install all of them by calling install.packages(\"tidyverse\"). We then load the tidyverse with:\n\nlibrary(tidyverse)\n\nThe command which follows takes a little explaining. We start by stating the dataframe we wish to assign the result to, EU. Then we name the data frame that contains the data we wish to manipulate, here also EU. The symbol which follows, \\%&gt;\\%, reads as “and then”, and is called a “pipe”. So we take the data frame EU “and then” carry out a function called mutate. It creates a new variable called wave by recoding the variable access_fac. The command then specifies all categories of the “old” variable access_fac and what their respective values in the “new” variable wave are going to be. The categories in each are set in quotation marks, as they are factor / character categories.\n\nEU &lt;- EU %&gt;%\n  mutate(wave = recode(access_fac, '1951'=\"Founding\", \n                       '1973'= \"First\",\n                       '1981'= \"Mediterranean\",\n                       '1986' = \"Mediterranean\",\n                       '1995' = \"Cold War\",\n                       '2004' = \"Eastern\",\n                       '2007' = \"Eastern\",\n                       '2013' = \"Balkans\"))\n\nPlease note that some colleagues in the department object to the use of the tidyverse as a “dialect” of R, and require you to use base R in their modules. However, on this module I am still using the tidyverse, as:\n\nmy textbook, which is going to be the main textbook for this module from 2026, uses the tidyverse, and I think it is pedagogically wrong to divert from the main text in my seminars\nI do not think it makes sense to exclude one of currently 22,505 packages2\nGGPLOT2 which is part of the tidyverse simplifies code for generating figures significantly and will do for all but the most specific requirements\na lot of support on stackexchange is geared toward the tidyverse as a lot of US-based data scientists work with this package, and so you will find it easier to solve problems\n\nBut to keep everybody happy, I am providing the base R code whenever possible in a collapsible section like this one:\n\n\n\nBase R Solution\n\n\nEU$wave &lt;- NA\nEU$wave[EU$access_fac=='1951'] &lt;- \"Founding\"\nEU$wave[EU$access_fac=='1973'] &lt;- \"First\"\nEU$wave[EU$access_fac=='1981'] &lt;- \"Mediterranean\"\nEU$wave[EU$access_fac=='1986'] &lt;- \"Mediterranean\"\nEU$wave[EU$access_fac=='1995'] &lt;- \"Cold War\"\nEU$wave[EU$access_fac=='2004'] &lt;- \"Eastern\"\nEU$wave[EU$access_fac=='2007'] &lt;- \"Eastern\"\nEU$wave[EU$access_fac=='2013'] &lt;- \"Balkans\"\n\nEU$wave &lt;- factor(EU$wave, ordered = TRUE)\n\nHere, we first create a new, empty variable called wave in the EU data set. We then create new values , for example Founding in the variable EU$wave for the condition (this is what the square brackets [ ] do) that the variable access_fac in the EU data set, equals a specific value. For Founding this is is 1951. The last step is to turn the wave variable into an ordered factor.\n\n\nBut back to the recoding exercise itself. Please note that the original variable access_fac was already an ordered factor. Therefore, R (or the mutate function to be precise) also returns wave as an ordered factor. Had access_fac been an unordered factor (aka nominal variable), wave would also have been an unorderd factor. You can specify in an option to the mutate function whether you want the factor to be ordered or not:\n\nEU &lt;- EU %&gt;%\n  mutate(wave = recode(access_fac, '1951'=\"Founding\", \n                       '1973'= \"First\",\n                       '1981'= \"Mediterranean\",\n                       '1986' = \"Mediterranean\",\n                       '1995' = \"Cold War\",\n                       '2004' = \"Eastern\",\n                       '2007' = \"Eastern\",\n                       '2013' = \"Balkans\"), ordered=TRUE)\n\naccess only had a handful of numbers, and it turned out that each numerical value of this variable turned into a category in the new one. But often you have to recode variables of a more continuous nature into categories. Suppose, for example, we have the variable age which contains the age of respondents in a survey. If we wanted to recode this into categories such as 20-25, 26-35, etc., it would be tedious to assign a category to each individual value of age. This is where the cut() function comes in handy, as it literally cuts up a variable into chunks at the points we specify. Let’s apply this to the access variable.\nAgain, we use the mutate function, this time naming our new variable wave1 (so as not to overwrite the wave variable we created with the recode() function). The cut() function splits a numeric variable into intervals using the breaks we specify in the function. Importantly, these intervals are left-open and right-closed. This means that each interval includes its upper boundary, but not the lower one. This sounds complicated, so let me give you an example. If, for example, we called\n\ncut(x, breaks = c(10, 20, 30))\n\nthen the first interval would start at 10 and include all values greater than 10 and up to and including 20. The next interval would contain all values that are larger than 20 and up to and including 30.\nWhen we apply this to our access variable, we therefore need to specify the breaks as follows:\n\ncut(access, breaks=c(1950, 1951, 1973, 1986, 1995, 2007, 2013))\n\nThe first category (Founding) thus contains all years up to and including 1951, the next category (First), all years larger than 1951 and up to and including 1973, and so forth. Having created these categories (or levels in R terminology), we can then label them accordingly with labels.\n\nEU &lt;- EU %&gt;% \n  mutate(wave1=cut(access, \n                  breaks=c(1950, 1951, 1973, 1986, 1995, 2007, 2013), \n                  labels=c(\"Founding\",\"First\",\n                           \"Mediterranean\", \n                           \"Cold War\", \n                           \"Eastern\", \n                           \"Balkans\"))) \n\n\nlevels(EU$wave)\n\n[1] \"Founding\"      \"First\"         \"Mediterranean\" \"Cold War\"     \n[5] \"Eastern\"       \"Balkans\"      \n\n\n\n\n\nBase R Solution\n\n\nEU$wave &lt;- cut(EU$access, \n                  breaks=c(1950, 1951, 1973, 1986, 1995, 2007, 2013), \n                  labels=c(\"Founding\",\"First\",\n                           \"Mediterranean\", \n                           \"Cold War\", \n                           \"Eastern\", \n                           \"Balkans\"))\n\n\n\n\nRecoding a Factor Variable\n\n\n\n\nRecoding Ordered Factor Variables\n\n\n\n\n\nBinary Dummy\nVery often in political science we have yes/no scenarios, such as democracy yes or no, civil war, yes or no, etc. To analyse these scenarios, we can create so-called “dummy variables”. In the present example, let’s specify for each country whether it has been a founding member of the EU. It is a factor variable and so we do could do this exactly the same way as our initial recoding of the wave variable above:\n\nEU &lt;- EU %&gt;%\n  mutate(founding = recode(access_fac, '1951'=\"Yes\", \n                       '1973' = \"No\",\n                       '1981' = \"No\",\n                       '1986' = \"No\",\n                       '1995' = \"No\",\n                       '2004' = \"No\",\n                       '2007' = \"No\",\n                       '2013' = \"No\"))\n\nThere is a much shorter way to do this, however, and shorter is always preferred in coding so long as it leads to the same result. We can apply the ifelse() function which follows the rationale: ifelse('condition', 'if condition met, then', 'otherwise'). Here, if access_fac==\"1951\" we want to assign the value \"Yes\", if not \"No\":\n\nEU &lt;- EU %&gt;% \n   mutate(founding = factor(ifelse(access_fac==\"1951\", \"Yes\", \"No\"), \n                              levels =c(\"Yes\", \"No\")))\n\n\n\n\nBase R Solution\n\nUsing ifelse, this is very similar, only the pipe disappears:\n\nEU$founding &lt;- factor(ifelse(EU$access_fac==\"1951\", \"Yes\", \"No\"), \n                              levels =c(\"Yes\", \"No\"))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO91Q.html#sub-setting-data",
    "href": "02-PO91Q.html#sub-setting-data",
    "title": "Week 2",
    "section": "Sub-Setting Data",
    "text": "Sub-Setting Data\nWhen we start analysing data, we rarely need all data at the same time. We might not need some variables, at all, for example, or we only want to work with certain observations, such as those countries in the “founding” wave. In these cases, we can subset the data. I will show you some examples of subsetting now.\n\nBy Variable\nIf you are sure you won’t need a variable (remember, there is no back button), you can simply drop (i.e. delete) it. Let’s do this with the area variable:\n\nEU$area &lt;- NULL\n\nIf we are dropping multiple variables, we can either perform this operation each time, or use another command which allows us to operate with multiple variables at the same time. The select() command comes from the tidyverse package and specifies which variables we wish to keep:\n\nEU_pop &lt;- select(EU, country, pop18, access_fac, founding)\n\nThis creates a new data frame called EU_pop containing only the variables country, pop18, access_fac, and founding.\n\n\n\nBase R Solution\n\nThe package documentation offers some basic instructions how to convert the tidyverse (or dyplyr, to be precise) code into base R. But here is the solution for the previous code chunk:\n\nEU_pop &lt;- subset(EU, country, pop18, access_fac, founding)\n\n\n\nWe can, however, use the same command and tell R which variables to drop by adding a minus sign in from of the variables we want to delete. The following command produces exactly the same result as the one before:\n\nEU_pop1 &lt;- select(EU, -access, -GDP_2015)\n\n\n\nBy Observation\nInstead of dropping and keeping variables, we can do the same thing to individual observations. Here, we use the slice() command (like a cake) and specify which slices we want to drop or keep. For example to drop the Benelux countries we would delete observations 1, 16 and 19:\n\nEU_nobenelux &lt;- slice(EU, -1, -16, -19)\n\n\n\n\nBase R Solution\n\n\nEU_pop &lt;- EU[c(-1, -16, -19),]\n\n\n\nAlternatively, if we were only interested in Benelux countries we would subset to only those observations:\n\nEU_benelux &lt;- slice(EU, 1, 16, 19)\n\n\n\n\nBase R Solution\n\n\nEU_pop &lt;- EU[c(1, 16, 19),]\n\n\n\n\n\nKeep if a variable has a certain value\nOne of the most useful commands is filter(), as it allows us to keep all observations for which the value of a variable is of a particular number. For example if we wanted to conduct an analysis with all countries which have a population in excess of 10 million we could subset by:\n\nEU_pop_large &lt;- filter(EU, pop18 &gt; 10000000)\n\n\n\n\nBase R Solution\n\n\nEU_pop_large &lt;- subset(EU, pop18 &gt; 10000000)\n\n\n\nHere is a list of some operators you can use for this purpose:\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOperator\nDescription\n\n\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal to\n\n\n==\nexactly equal to\n\n\n!=\nnot equal to\n\n\n!x\nNot x\n\n\nx | y\nx OR y\n\n\nx & y\nx AND y\n\n\n\n\n\n\nTable 1: Operators in R\n\n\n\n\n\n\n \n\nSubsetting Data",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO91Q.html#ordering-data",
    "href": "02-PO91Q.html#ordering-data",
    "title": "Week 2",
    "section": "Ordering Data",
    "text": "Ordering Data\nThe data set in its original state is purposely not ordered by any criterion, such as alphabetical order of countries, etc. But we can use R to do exactly that. Let us work with a subset containing only three variables:\n\nEU_subset &lt;- select(EU, country, pop18, access)\n\nIt would be lovely if the command for ordering data would be called order(), but it is called arrange()3. Let’s order countries by ascending population in a new data frame called eu_order:\n\neu_order &lt;- arrange(EU_subset, pop18)\n\n\n\n\nBase R Solution\n\n\neu_order &lt;- EU_subset[order(EU_subset$pop18),]\n\n\n\nWe can now display the first 10 rows with the following command:\n\neu_order[1:10,]\n\n# A tibble: 10 × 3\n   country      pop18 access\n   &lt;fct&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n 1 Malta       475701   2004\n 2 Luxembourg  602005   1951\n 3 Cyprus      864236   2004\n 4 Estonia    1319133   2004\n 5 Latvia     1934379   2004\n 6 Slovenia   2066880   2004\n 7 Lithuania  2808901   2004\n 8 Croatia    4105493   2013\n 9 Ireland    4838259   1973\n10 Slovakia   5443120   2004\n\n\nThe content in the brackets refers to the rows (before the comma), and to the columns (after the comma). As we only want certain rows and displaying all variables, I have left the space after the comma blank.\nWe can do the same thing in descending order by calling:\n\neu_order &lt;- arrange(EU_subset, desc(pop18))\neu_order[1:10,]\n\n# A tibble: 10 × 3\n   country           pop18 access\n   &lt;fct&gt;             &lt;dbl&gt;  &lt;dbl&gt;\n 1 Germany        82850000   1951\n 2 France         67221943   1951\n 3 United Kingdom 66238007   1973\n 4 Italy          60483973   1951\n 5 Spain          46659302   1986\n 6 Poland         37976687   2004\n 7 Romania        19523621   2007\n 8 Netherlands    17181084   1951\n 9 Belgium        11413058   1951\n10 Greece         10738868   1981\n\n\n\n\n\nBase R Solution\n\n\neu_order &lt;- EU_subset[order(desc(EU_subset$pop18)),]\n\n\n\nA neat feature of R is that it allows us to order observations by more than one variable. So for example, we could order them by ascending accession wave first, and then by ascending population in 2018 as follows:\n\neu_order &lt;- arrange(EU_subset, access, pop18)\n\neu_order[1:10,]\n\n# A tibble: 10 × 3\n   country           pop18 access\n   &lt;fct&gt;             &lt;dbl&gt;  &lt;dbl&gt;\n 1 Luxembourg       602005   1951\n 2 Belgium        11413058   1951\n 3 Netherlands    17181084   1951\n 4 Italy          60483973   1951\n 5 France         67221943   1951\n 6 Germany        82850000   1951\n 7 Ireland         4838259   1973\n 8 Denmark         5781190   1973\n 9 United Kingdom 66238007   1973\n10 Greece         10738868   1981\n\n\n\n\n\nBase R Solution\n\n\neu_order &lt;- EU_subset[order(EU_subset$access,EU_subset$pop18),]\n\n# or slightly shorter \n\neu_order &lt;- EU_subset[order(with(EU_subset, access,pop18)),]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO91Q.html#grouping-data",
    "href": "02-PO91Q.html#grouping-data",
    "title": "Week 2",
    "section": "Grouping Data",
    "text": "Grouping Data\nLooking at the last example, a question that might spring up is in which accession wave the joining countries brought the largest population increase on average to the EU. We can calculate summary statistics for a particular group by, well, grouping them. The first step is to group data into rows with the same value:\n\neu_access &lt;- group_by(EU_subset, access)\n\nBy the way: whenever you have grouped anything, and finished analysing data in this grouped version it is essential that you ungroup the data afterwards, so that you don’t unintentionally keep using the groups:\n\nungroup(EU_subset)\n\n# A tibble: 28 × 3\n   country     pop18 access\n   &lt;fct&gt;       &lt;dbl&gt;  &lt;dbl&gt;\n 1 Belgium  11413058   1951\n 2 Bulgaria  7050034   2007\n 3 Czechia  10610055   2004\n 4 Denmark   5781190   1973\n 5 Germany  82850000   1951\n 6 Estonia   1319133   2004\n 7 Ireland   4838259   1973\n 8 Greece   10738868   1981\n 9 Spain    46659302   1986\n10 France   67221943   1951\n# ℹ 18 more rows\n\n\nBut let’s calculate the average population size per accession wave in an elegant command which combines multiple steps by using pipes:\n\neu_popaccess &lt;- EU_subset %&gt;% \n  group_by(access) %&gt;% \n  summarise(avg = mean(pop18))\n\neu_popaccess\n\n# A tibble: 8 × 2\n  access       avg\n   &lt;dbl&gt;     &lt;dbl&gt;\n1   1951 39958677.\n2   1973 25619152 \n3   1981 10738868 \n4   1986 28475164.\n5   1995  8151880.\n6   2004  7327746.\n7   2007 13286828.\n8   2013  4105493 \n\n\n\n\n\nBase R Solution\n\n\neu_popaccess1 &lt;- aggregate(pop18 ~ access, \n                           data = EU_subset, \n                           FUN = mean )\n\n\n\nYou now see a new variable called avg which contains the average population increase for each wave. In which wave did the joining countries have the largest population on average?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO91Q.html#combining-ordering-and-grouping-data",
    "href": "02-PO91Q.html#combining-ordering-and-grouping-data",
    "title": "Week 2",
    "section": "Combining Ordering and Grouping Data",
    "text": "Combining Ordering and Grouping Data\nThe question was easy to answer here, as we only have a few accession waves. It starts to get unwieldy though, the more groups we have, but we can let R do the job by combining first grouping, and then ordering. So we take the grouped data frame eu_popaccess and order it by descending avg:\n\neu_popaccess_order &lt;- arrange(eu_popaccess, desc(avg))\n\neu_popaccess_order\n\n# A tibble: 8 × 2\n  access       avg\n   &lt;dbl&gt;     &lt;dbl&gt;\n1   1951 39958677.\n2   1986 28475164.\n3   1973 25619152 \n4   2007 13286828.\n5   1981 10738868 \n6   1995  8151880.\n7   2004  7327746.\n8   2013  4105493 \n\n\n\n\n\nBase R Solution\n\n\neu_popaccess_order &lt;- eu_popaccess[order(desc(eu_popaccess$avg)),]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO91Q.html#calculations-by-hand",
    "href": "02-PO91Q.html#calculations-by-hand",
    "title": "Week 2",
    "section": "Calculations by Hand",
    "text": "Calculations by Hand\nIf you want to practice calculating descriptive statistics by hand (might be exam relevant), I have written a small app that will help you check the calculations for any data set up to five observations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO91Q.html#basic-graphs",
    "href": "02-PO91Q.html#basic-graphs",
    "title": "Week 2",
    "section": "Basic Graphs",
    "text": "Basic Graphs\nLet’s start with a histogram of the variable pop18. The range of the pop18 variable is about 82 million - this is rather unwieldy to imagine and also to put onto axes of graphs, as they would mostly consist of zeros. So let’s express the population of each countries in million instead:\n\nEU$popmio &lt;- EU$pop18/1000000\n\nWe can now produce a histogram. Before doing this, it is sensible to think about the number of bars we want in the histogram. The smallest country has just shy of 500,000 inhabitants, whereas the largest has over 82 million. So, I would like the x-axis to run from zero to 100 (million) and divide this into 5 bars. Accordingly, we are introducing 4 breaks on the x-axis with the following command:\n\nhist(EU$popmio, breaks = 4)\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nThis is certainly a histogram, but it does not conform to the principle of graphs that they should be able to communicate their message independently, yet. Take the label of the x-axis, for example, what does EU$popmio mean? You and I know, but somebody who doesn’t know R language wouldn’t. We can tell R to adjust the axis label, as well as the main title of the histogram as follows:\n\nhist(EU$popmio, breaks = 4,\n     xlab = \"Population in million\",\n     main = \"Histogram of EU Population (2018)\")\n\n\n\n\n\n\n\nFigure 2: Histogram of EU Population (2018)\n\n\n\n\n\n\nHistograms\n\n\n\nThis is fine now. Ugly, but fine. I will show you how to do a boxplot next, and then I will take you through the process of making all this look a bit more jazzy. The command for the boxplot is very intuitive. The default in R is to arrange the boxplot vertically. I prefer them horizontally, and you can set this in an equally intuitive option.\n\nboxplot(EU$popmio, horizontal = TRUE)\n\n\n\n\n\n\n\nFigure 3: Boxplot of EU Population\n\n\n\n\n\nYou will recognise the descriptives we calculated earlier with the summary() function:\n\nsummary(EU$popmio)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.4757  3.7813  9.3003 18.3111 17.7667 82.8500 \n\n\n\nExplain the outliers on the right mathematically.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO91Q.html#advanced-graphs",
    "href": "02-PO91Q.html#advanced-graphs",
    "title": "Week 2",
    "section": "Advanced Graphs",
    "text": "Advanced Graphs\nThe graphs we have produced so far are functional, but let’s be honest, they wouldn’t win any beauty contests. There is, as mentioned earlier, an amazing package called ggplot2 which changes this dramatically. You have already installed it as a part of the tidyverse. Otherwise, the function would be:\n\ninstall.packages(\"ggplot2\")\n\nWe can just load it:\n\nlibrary(ggplot2)\n\nThe “gg” in ggplot2 stands for “grammar of graphs”. You will be familiar with the term “grammar”” from learning a language already. In this context, we use grammar to build sentences by choosing and arranging a variety different components, such as subjects, verbs and objects. If you know how to do this properly, you can express exactly what you want to say. The grammar of graphs adopts this logic and specifies a number of different components which allow you to create a graph which is able to communicate exactly what you wish to show.\nggplot2 has eight basic grammatical arguments:\n \n\n\n\n\n\n\nData Frame\nThe data you wish to visualize.\n\n\nAesthetic Mappings\nHere you specify how the data are assigned to colour, size, etc. For now, this is the variable for which we want to create a graphical distribution.\n\n\nGeom\nShort for \"geometry\". Use a geom function to represent data points through geometric objects, such as points, lines, etc. Each function returns a layer.\n\n\nStat\nYou can include statistical summaries through this, such as smoothing, or regression lines.\n\n\nPosition\nPosition adjustments determine how to arrange geoms that would otherwise occupy the same space.\n\n\nFacets\nFacets divide a plot into sub-plots based on the values of one or more discrete variables.\n\n\nScale\nMaps data values to the visual values of an aesthetic. For example female=pink, male=blue.\n\n\nCoordinates\nHow do the numbers get translated onto the plot? We are not going to look at this on this module.\n\n\n\n\n\n\n \nI like to think of using these arguments like dressing myself in the morning. The minimum that common decency requires me to wear if I wish to leave the house is some underwear, some trousers, and a top. Depending on how I feel and what the weather is like, I can add more layers, like socks, a jumper, or a scarf. It is exactly the same with ggplot2. As a minimum to produce a plot you need a data frame, the aesthetic mapping and a geom. Once you have produced this minimalistic graph, you can modify it, by adding more components / arguments. As you can imagine the possibilities are almost endless, and we only have time to deal with the minimum here. This is not a problem, however, as most of the other grammatical arguments (Stats, Position, Facets and Scales) generally have sensible defaults.\nSo how does this work in practice? Let us reproduce the histogram of the age variable. We start by calling ggplot2 and advise the function which data frame we wish to use (EU). In a second step, we add a geometry – in our case geom_histogram. Within the geometry, we need to specify for which variable we wish to create a distribution, or in the language of ggplot2 which variable we wish to map to the geom as our Aesthetic. To produce 5 bars again, we specify a bandwidth of 20 million (this refers to popmio).\n\nggplot(data = EU) +\n  geom_histogram(mapping = aes(popmio), binwidth = 20)\n\n\n\n\n\n\n\nFigure 4: Histogram of EU Population (binwidth = 20)\n\n\n\n\n\nAnnoyingly, ggplot places the axis ticks in the middle of each bar which is WRONG for histograms. They need to align with the boundaries of the bars. We do this by telling R the boundary of the plot:\n\nggplot(data = EU) +\n  geom_histogram(mapping = aes(popmio), binwidth = 20, \n                 boundary = 0)\n\n\n\n\n\n\n\nFigure 5: Histogram of EU Population (binwidth = 20, boundary = 0)\n\n\n\n\n\nThis has shifted the ticks to the left, but now R has decided to label the x-axis in steps of 25, whereas our bars have a bandwidth of 20. Once again, we have the variable name on the x-axis, instead of a label which anybody could understand. I also prefer “Frequency” on the y-axis, instead of “Count”. To address both of these concerns, we simply add a layer for each. First up are the axis ticks. Our variable is continuous, so we choose the scale_x_continuous option, and tell R to break the axis up into a sequence which starts with zero, ends at 100 and has steps of 20 in between. In the labs argument we adjust the labelling as intended:\n\nggplot(data=EU) +\n  geom_histogram(mapping=aes(popmio), binwidth = 20, \n                 boundary = 0) +\n  scale_x_continuous(breaks = seq(0, 100, 20)) +\n  labs(x=\"Population (in million)\", y=\"Frequency\") +\n  theme_classic()\n\n\n\n\nBase R Solution\n\n\nhist(EU$popmio, breaks = 4,\n     xlab = \"Population (in million)\", ylab = \"Frequency\")\n\n\n\n\n\n\n\n\n\nFigure 6: Histogram of EU Population with GGPLOT\n\n\n\nI have also removed the background in line with the principles set out by Tufte (2001, p. 96) by adding theme_classic(). This is it. A graph which can communicate its message independently, and which looks aesthetically pleasing.\nIn the present case we have the population, so displaying the frequency on the y-axis is sort of sensible, but usually we would be dealing with a sample. Here the count is not very telling and we would be using percentages, instead. Let’s do it!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO91Q.html#even-more-advanced-graphs",
    "href": "02-PO91Q.html#even-more-advanced-graphs",
    "title": "Week 2",
    "section": "Even More Advanced Graphs",
    "text": "Even More Advanced Graphs\nUnfortunately, there is no easy, default way to do this in R, but necessitates a calculation within the ggplot command. Once more we call ggplot and use the EU data set, and select the geom geom_histogram. Again we specify the binwidth as 20 with a boundary of zero, and put popmio on the x-axis. Now comes the point where we need to do something new, because y is not equivalent to the frequency any more, but should be percentage. To achieve this we advise R to put the density there (which is the relative frequency from the the lecture in week 5), and multiply this density by 100 to get percentage. Nothing has changed on the scaling of the x-axis from the previous plot, so we can copy and paste the scale_x_continuous section, as well as the labelling of the x-axis. In this last step, we now also need to adjust the label of the y-axis, because this has now percentage on it, and not frequency. The result is this:\n\nggplot(data = EU) + \n  geom_histogram(binwidth = 20, boundary = 0,\n                 aes(x= popmio, \n                     y = (..count..)/sum(..count..)*100)) +\n  scale_x_continuous(breaks = seq(0, 100, 20)) +\n  labs(x=\"Population (in million)\", y=\"percent\")  +\n  theme_classic()\n\n\n\n\nBase R Solution\n\n\n# Create histogram data without plotting\nh &lt;- hist(EU$popmio, \n         breaks = seq(0, 100, by = 20), \n         plot = FALSE)\n\n# Normalize counts to percentages\nh$counts &lt;- h$counts / sum(h$counts) * 100\n\n# Plot histogram\nplot(h, \n    freq = FALSE, \n    xlab = \"Population (in million)\", \n    ylab = \"percent\", \n    main = \"\")\n\n\n\n\n\n\n\n\n\nFigure 7: Histogram of EU Population with GGPLOT in Percent\n\n\n\n\nJazzy Graphs with GGPLOT",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO91Q.html#exercises",
    "href": "02-PO91Q.html#exercises",
    "title": "Week 2",
    "section": "Exercises",
    "text": "Exercises\nUsing these commands, and moving beyond with the help of today’s reading, complete the following tasks:\n\nProduce two base-R graphs of different types (e.g. histogram, bar chart, box-and-whisker plot) for separate variables in the EU data set.\nProduce two ggplot graphs of different types (e.g. histogram, bar chart, box-and-whisker plot) for separate variables in the EU data set. Google to find more geoms.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO91Q.html#exploring-the-ess-core-exercises",
    "href": "02-PO91Q.html#exploring-the-ess-core-exercises",
    "title": "Week 2",
    "section": "Exploring the ESS – Core Exercises",
    "text": "Exploring the ESS – Core Exercises\n\nOpen the ESS9 dataset in R and attach it.\nIdentify the variable about “Left-right placement”.\n\nCheck its formatting in the questionnaire: what do 77, 88 and 99 mean?\nWhat is the level of this variable?\nSummarise it with functions class, str, and head\nWhat do these functions return?\n\nApply the same steps to the variable about the “European unification:”should go further or already gone too far”.\n\nThem display the first 20 values of the two variables above\nCheck the structure of the ‘table’ function and tabulate the two variables\nCan you see a major difference between them regarding non positive answers (ie outside proposed scale)?\n\nConsider the function rm(list=ls())\n\nWhat function rm stand for?\nHow useful can this function be for future exercises?\n\nUnivariate Statistics and Recoding\n\nCalculate the two means, using only valid values (check the mean function beforehand)\nIn Left-right, regroup values, using three different methods:\n\nrecode\nconvert to factor format using as.factor\nassign value labels using levels\n\n0-1 into “far left”\n2-3 into “left”,\n4-6 into “centre”,\n7-8 into “right”\n9-10 into “far right”",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO91Q.html#exploring-the-ess-going-further",
    "href": "02-PO91Q.html#exploring-the-ess-going-further",
    "title": "Week 2",
    "section": "Exploring the ESS – Going Further",
    "text": "Exploring the ESS – Going Further\n\nUsing the new, transformed variables:\n\nCalculate means in the UK only. You may use the [variable == \"value\"] subscript\nHow does this mean compare with the average in Europe? and with France?\nPresent the frequencies and means of these three samples in one unique table\nInstall and load the questionr package. Use its function ‘freq’ to calculate the same as above, but with percentages.\nCompare deviations from the mean in the UK and Germany using the function abs (absolute value, which means any value with all negative signs deleted)\nCompare mean, median, mode, variance and standard deviation in the UK and Germany\n\nTry 3 or more kinds or graphs with these two variables separately, using the most appropriate of original or transformed values. Assess and compare the relevance of each graph\nTabulate the two variables in original format against each other using table(X,Y). Interpret the output.\nRepeat this with the transformed format: Interpret the output.\nTry the same using questionr::cprop\nGraph the two variables against each other using ‘plot’ (make sure you choose the right versions of the variables). What can you conclude from this graph?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "02-PO91Q.html#footnotes",
    "href": "02-PO91Q.html#footnotes",
    "title": "Week 2",
    "section": "",
    "text": "These questions are taken from Reiche (forthcoming).↩︎\nhttps://cran.r-project.org/web/packages/↩︎\nThere is a command called order(), but it is not part of the tidyverse, and as this package is steadily on the rise in coding, I am only showing you this here.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Week 2</span>"
    ]
  },
  {
    "objectID": "03-PO91Q.html",
    "href": "03-PO91Q.html",
    "title": "Week 3",
    "section": "",
    "text": "Self-Assessment Questions1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "03-PO91Q.html#how-to-read-a-z-table",
    "href": "03-PO91Q.html#how-to-read-a-z-table",
    "title": "Week 3",
    "section": "How to read a z-table",
    "text": "How to read a z-table\nIn the lecture you have learned about the Normal Distribution. Under the Normal Distribution, the area under the curve is determined by the number of standard deviations around our mean \\(\\mu\\). This number is expressed in the form of the z-score which is defined as:\n\\[\\begin{equation}\nz=\\frac{\\text{Observation} - \\text{Mean}}{\\text{Standard Deviation}}=\\frac{y-\\mu}{\\sigma}\n\\end{equation}\\]\nTo put this in words, z takes the difference between a particular value we are interested in and the mean. It then divides this distance by the standard deviation, in order to express the distance in units of standard deviations. Why do we do this? We know that under the Normal Distribution the area of the interval mean \\(\\pm\\) one standard deviation is equal to 68%. This is equivalent to the blue area in Figure 1. This also means that the remaining white area is equal to 32%, or the white section on each side 16%.\n\n\n\n\n\n\nFigure 1: Area under the Normal Distribution\n\n\n\nImagine now, we took the point of minus one standard deviation as a starting point, and turn right, as in Figure 2. The white area is still 16%, so that the blue area needs to be 84%.\n\n\n\n\n\n\nFigure 2: Right-Tail Probability\n\n\n\nSo the probability of finding a value larger than what is equivalent to minus one standard deviation is 84%. We call this a right-tail probability. The beauty is that we can do this for any point on the x-axis. Once we know how many standard deviations a value is removed from the mean, we can use the right-tail probability to assess how likely a value higher (or lower) than this value is to occur.\nThe number of standard deviations is the z-score. Every z-score has a right-tail probability associated with it. These probabilities are listed in the Normal Table. How do we read this Table? Let me take you through the example used in the lecture once more. The crime rate in our 32 Scottish councils is normally distributed, with \\(\\mu=280.36\\) and \\(\\sigma=69.26\\). You can see this distribution visualised in Figure 3.\n\n\n\n\n\n\nFigure 3: Distribution of Crime Rate in 32 Scotting Councils, 2020\n\n\n\nThe question then was how likely it was for us to find a council with a crime rate larger than 400. If we wanted to visualise this, we would need the area to the right of 400 on the x-axis. This would look like this:\n\n\n\n\n\n\nFigure 4: Probability of a Council with a Crime Rate \\(\\geq\\) 400\n\n\n\nIn order to calculate the size of this area, we first took the difference between 400 and 280.36 which is 119.64. We then divided 119.64 by the standard deviation of 69.26, to express the distance in units of the standard deviation. The result is 1.727404. We know, therefore, that a crime rate of 400 on the x-axis is located 1.727404 standard deviations to the right of the mean.\nWe now need to find the right-tail probability that belongs to this value. In the left-most column of the Normal Table you find the z-values with the first decimal place. Move down to 1.7. From here you turn right, until you hit the second decimal place. As our value is 1.73 you will have to go four columns to the right (the first one is for decimal 0). For a z-score of 1.73 the area is 0.0418, or 4.2%. We can therefore say that with an average crime rate of 280.36 and a standard deviation of 69.26, the probability of finding a council with a crime rate higher than 400 was 4.2%.\nBefore moving on, I need to note that z can be negative. If we were assessing the probability of finding a council with a crime rate of less than 160.5 (280.36 - 1.73*69.26) we would get a z-score of -1.73. Because the Normal Distribution is symmetrical, we can use the same process, but need to reverse the logic. Because the right tail probability gives us the area to the right of the z-score, a negative z-score would give us the area to the left of the z-score. So, the probability of finding a council with a crime rate of less than 160.5 is also 4.2%.\nWith this knowledge at hand, let’s do some exercises!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "03-PO91Q.html#exercises-calculations",
    "href": "03-PO91Q.html#exercises-calculations",
    "title": "Week 3",
    "section": "Exercises – Calculations",
    "text": "Exercises – Calculations\n\nThe mean weight of a bag of apples is 1 kg. The weight of bags is normally distributed around this mean with a standard deviation of 50g.\n\nBilly is looking for the heaviest bag possible and finds one that is 1082 g. What is the probability of finding a heavier bag?\nWhat is the probability that Billy will find a bag lighter than 870g?\nHow would the results of a. and b. change if the standard deviation was only 40g? Why?\n\nWhich z-value gives you a right-tail probability of 2.5%?\nWith a z-value of 1.13 what is the right-tail probability in %?\nThe average mark on a fictitious module was 62.3 with a standard deviation of 8.5. What was the probability to fail the module (pass-mark=50)?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "03-PO91Q.html#r-exercises---core",
    "href": "03-PO91Q.html#r-exercises---core",
    "title": "Week 3",
    "section": "R Exercises - Core",
    "text": "R Exercises - Core\n\nOpen the European Social Survey data, but this time use Wave 7 (2014) which you need to download from the ESS website. Name it conveniently and attach it.\nUnivariate Statistics\n\nIdentify height and weight variables\nUse an appropriate function to summarise them in a few lines\nPlot the two variables separately as boxplots\nPlot the two variables separately as histograms using function ‘hist’\nAdjust the bars to 5 cm and 5 kg (or as close to these numbers as you can)\nDiscuss the advantage of various break values\nWhat are weight’s and height’s modes?\nPlot the same using the ‘density’ of each variable (density is a function that smoothes a distribution)\nCalculate relevant central values for both variables. Do they match your intuition?\n\nSampling\n\nExecute the following three times, then explain what function ‘sample’ is:\n\n\nsample(1:20,5)\n\n\nExecute the following three times, then explain what function ‘set.seed’ is:\n\n\nset.seed(1)\nsample(1:20,5)\n\n\nSelect a random sample of 100 from the ESS data frame, using “set.seed(1)”.\n\nSampling Distributions\n\nGenerate a vector “w” containing 10000 normally distributed numbers with mean 0 and standard deviation 1. Convert the vector into a data frame “dfw”.\nUsing ggplot, plot a histogram of variable w (hint: use “after_stat(density)”), and overlay the histogram with a red density curve. Label the graph appropriately.\nCreate an empty vector, called “n30”. Write a function which draws 9000 samples of size 30 from variable w and stores the mean of each sample in vector “n30”. Convert the vector “n30” into a data frame.\nUsing ggplot, plot a histogram of variable n30 (hint: use “after_stat(density)”), and overlay the histogram with a red density curve. Label the graph appropriately.\nPlot the two graphs we just created in a grid with two columns and 1 row. (Hint: use package ‘gridExtra’)\nInterpret the juxtaposition of these graphs.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "03-PO91Q.html#r-exercises---going-further",
    "href": "03-PO91Q.html#r-exercises---going-further",
    "title": "Week 3",
    "section": "R Exercises - Going Further",
    "text": "R Exercises - Going Further\n\n\nComplete Section 4 of the Core Exercises.\nRepeat steps a-d for the following distributions:\n\nA uniform distribution with min = 0, max = 1.\nA beta distribution, Beta(2,5).\n\nWhat does the beta distribution represent?\nCreate a graph in which each distribution type has a new row, and population and sampling distributions are placed next to each other.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "03-PO91Q.html#solutions",
    "href": "03-PO91Q.html#solutions",
    "title": "Week 3",
    "section": "Solutions",
    "text": "Solutions\nYou can find the Solutions in the Downloads Section.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "03-PO91Q.html#footnotes",
    "href": "03-PO91Q.html#footnotes",
    "title": "Week 3",
    "section": "",
    "text": "Some of the content of this worksheet is taken from Reiche (forthcoming).↩︎\nSome of these are taken from Reiche (forthcoming).↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Week 3</span>"
    ]
  },
  {
    "objectID": "04-PO91Q.html",
    "href": "04-PO91Q.html",
    "title": "Week 4",
    "section": "",
    "text": "Self-Assessment Questions1\nIn your own words:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO91Q.html#we-know-sigma",
    "href": "04-PO91Q.html#we-know-sigma",
    "title": "Week 4",
    "section": "1. We know \\(\\sigma\\)",
    "text": "1. We know \\(\\sigma\\)\nIf the population distribution is known and the population standard deviation (\\(\\sigma\\)) is available, we can construct a confidence interval using the standard normal distribution (z-distribution).\nAssume we have a sample of student ages with sample size \\(n = 81\\) and a sample mean \\(\\bar{y} = 26\\). The population standard deviation is known to be \\(\\sigma = 9\\). We aim to construct a 99% confidence interval for the true population mean age of students. (In the lecture, we considered the 95% confidence level for this same scenario.)\nWe begin by calculating the standard error of the sample mean:\n\\[\\begin{equation*}\n    \\sigma_{\\bar{y}} = \\frac{\\sigma}{\\sqrt{n}}\n\\end{equation*}\\]\n\\[\\begin{equation*}\n    \\sigma_{\\bar{y}} = \\frac{9}{\\sqrt{81}} = 1\n\\end{equation*}\\]\nWhat is important to bear in mind for the construction of confidence intervals, is that we do not just need the right-tail probability, because the area we are trying to cover under the distribution is symmetrical around the mean. This is visualised in Figure 1 where the area is defined between \\(\\pm\\) 1.96 standard deviations around the mean.\n\n\n\n\n\n\nFigure 1: 95 Percent Confidence Interval around the Mean\n\n\n\nAs the orange area needs to be 95%, the remaing areas to the left and right need to be equal to 5% jointly. So each of them is 2.5%. When we look into our Table with right-tail probabilities, we therefore need to look for the z-score that corresponds to 0.025 (or 2.5%). When you look in the Normal Table (see Statistical Tables), then you will find this at z=1.96.\nNow, the question arises, how many standard deviations we need for a 99% confidence interval. The area to the left and right needs to be jointly 1%, or 0.05% each side. We consult the Normal Table again, and try to find the z-score for 0.005. The exact value is not available, only either 0.0051, or 0.0049. 0.0051 would lead to a confidence interval of 98.98%, so not quite large enough. We therefore need to go for 0.0049, and the corresponding z-score of z=2.58.\n\\[\\begin{equation*}\ny = \\bar{y} \\pm 2.58 \\times \\sigma_{\\bar{Y}}\n\\end{equation*}\\]\nPopping the values in we receive\n\\[\\begin{equation*}\n26 \\pm 2.58 \\times 1\n\\end{equation*}\\]\nAs a result, we can say that we are 99% confident that the true average age of students at Warwick lies between 23.42 and 28.58. Note that this confidence interval is wider than the 95% one where the boundaries were defined as 24.04 and 27.96. As we went for higher certainty here, the confidence interval became wider. If we want our interval to contain the true average in 99 out of 100 samples, we need to cast our net wider than if we were content with 95.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO91Q.html#we-dont-know-sigma",
    "href": "04-PO91Q.html#we-dont-know-sigma",
    "title": "Week 4",
    "section": "2. We don’t know \\(\\sigma\\)",
    "text": "2. We don’t know \\(\\sigma\\)\nIf we don’t know the population distribution and its standard deviation, we need to use the t-distribution. As you know from the lecture, the t-distribution is a shape shifter. Its width depends on the degrees of freedom: the more degrees of freedom we have, the more narrow, or the closer to the normal distribution it comes. With df=30 the shapes of the t-distribution and the normal distribution are almost identical. You can see this in the following Figure, comparing the yellow t-distribution for \\(df=30\\) and the black (normal) distribution.\n\n\n\n\n\n\nFigure 2: Comparison of t-Distributions\n\n\n\nIn reversed logic, this also means that the t-distribution is rather wide for small sample sizes (and small df). Consequently, a confidence interval of a given level (e.g. 99%) would be much wider for a small sample size than under the normal distribution (or t-distributions with higher sample sizes). Let me illustrate this using the same sample average as for the normal distribution example above, \\(\\bar{y}=26\\). We have a sample standard deviation (\\(s\\)) of 2. Our sample size is very small, we only have \\(n=4\\). Again, we want an interval within which we find with 99% confidence the true average age of students.\nWe start by estimating the standard error:\n\\[\\begin{equation*}\n    se=\\frac{s}{\\sqrt{n}}\n\\end{equation*}\\]\n\\[\\begin{equation*}\nse=\\frac{2}{\\sqrt{4}}=1\n\\end{equation*}\\]\nThis time we have to search in the t-Table, taking into account the degrees of freedom. As \\(n=4\\), this means that \\(df=3\\). Conveniently, the Table lists at the top the desired confidence interval. For \\(df=3\\) and a 99% confidence interval, the corresponding t-value is 5.841. Once again, we calculate:\n\\[\\begin{equation*}\n\\bar{y} \\pm 5.841 \\times se\n\\end{equation*}\\]\nand\n\\[\\begin{equation*}\n26 \\pm 5.841 \\times 1\n\\end{equation*}\\]\nThe resulting boundaries are 20.159 as the lower boundary, and 31.841 as the upper boundary. This is far wider than the 99% confidence interval we obtained for the normal distribution, where the lower and upper boundaries were 23.42 and 28.58, respectively. This is a reflection of the fact that we only had a very small sample (\\(n=4\\)) to base our inference on and therefore have a lot of uncertainty in our inference.\nTo conclude, it is fair to say that the procedure is essentially the same as with the normal procedure, only that have to go the extra step of taking into account the degrees of freedom.\nYou are now ready to do some exercises.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO91Q.html#considerations-for-interpreting-significance-tests",
    "href": "04-PO91Q.html#considerations-for-interpreting-significance-tests",
    "title": "Week 4",
    "section": "Considerations for interpreting Significance Tests",
    "text": "Considerations for interpreting Significance Tests\n\nInterpreting Significance Tests\nIn these exercises, and when reporting statistical results more generally, you’ll be required to interpret the outcome of a significance test. To do this well, it’s important to use language that reflects what null hypothesis significance testing (NHST) actually tells us, without overstating what the test can conclude.\nNHST works by starting with the null hypothesis - usually a claim like “there is no effect” or “there is no difference”. We use the data to test whether that assumption is plausible. If the p-value is small, it means that results as extreme as the one we observed — or more extreme — would be rare if the null hypothesis were true. So, we take that as evidence against the null. But if the p-value is large, then the data are consistent with the null. And that’s where your interpretation should stop (at least at the introductory level, to avoid stretching and overstating).\nThere are two possible outcomes of a significance test: (1) your p-value is below the required significance level, and (2) your p-value is above the required significance level. Let’s look at these in turn:\n\nIf your p-value is below the required significance level, you can say things like:\n\n“There is evidence of an effect.”\n“We reject the null hypothesis.”\n“The data provide evidence against the null hypothesis.”\n“There is statistically significant evidence of a difference (or relationship, or effect).”\n\n\nAvoid phrasing that implies certainty, such as:\n\n🚫 “We proved the alternative hypothesis.”\n🚫 “We proved that there is an effect.”\n🚫 “We accept the alternative hypothesis.”\n\n\n\n\nIf your p-value is above the required significance level to reject the null hypothesis, you can say things like:\n\n“There is insufficient evidence for an effect (of a difference, for a relationship).”\n“We cannot reject the null hypothesis.”\n“We fail to reject the null hypothesis.”\n“The results are consistent with the null hypothesis.”\n\n\nAvoid phrasing that overstates what the test can tell you, such as:\n\n🚫 “We proved the null hypothesis.”\n🚫 “We reject the alternative hypothesis.”\n🚫 “The null hypothesis is true.”\n\n\n\nHaving taught quantitative political analysis for many years, I know that students love to use the word “prove” when they interpret their results. But there is no proof in significance testing, because we deal with probability statements, not certainty.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO91Q.html#the-t-distribution",
    "href": "04-PO91Q.html#the-t-distribution",
    "title": "Week 4",
    "section": "The t-distribution",
    "text": "The t-distribution\nThis section is optional. I have included it so that you can familiarise yourself a little bit more with the t-distribution if this still seems a little unclear to you. In this case, complete the following exercises, using the app below to find your answers. Otherwise, skip straight to “How effect size, sample size, and power shape inference”.\n\nHow the degrees of freedom \\(df\\) change the shape and critical values\n\nSet tail type = two-tailed and \\(\\alpha = 0.05\\). Move \\(n\\) from 3 to 50 (\\(df\\) from 2 to 49). Record the critical values \\(t_{\\mathrm{crit}}\\) at \\(n \\in \\{3, 5, 10, 20, 50\\}\\).\n\nCompare these to the normal critical value \\(z_{0.025} \\approx 1.960\\).\nWhich \\(n\\) (or \\(df\\)) makes \\(t_{\\mathrm{crit}}\\) within 0.05 of 1.96?\n\nExplain why \\(t\\) approaches the standard normal as \\(df \\to \\infty\\).\n\nOne-tailed vs two-tailed critical regions\n\nFix \\(df = 14\\). Toggle between two-tailed and one-tailed (upper) with \\(\\alpha = 0.05\\).\nNote the two cutpoints for the two-tailed test (\\(\\pm t_{0.025,14}\\)) and\nthe single cutpoint for the one-tailed test (\\(t_{0.05,14}\\)).\n\nFor a test with \\(H_0: \\mu = \\mu_0\\) and \\(H_1: \\mu &gt; \\mu_0\\), which cutpoint is relevant and why?\n\nVerify with shading that the total shaded probability equals \\(\\alpha\\) in each case.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO91Q.html#how-effect-size-sample-size-and-power-shape-inference",
    "href": "04-PO91Q.html#how-effect-size-sample-size-and-power-shape-inference",
    "title": "Week 4",
    "section": "How effect size, sample size, and power shape inference",
    "text": "How effect size, sample size, and power shape inference\nIn statistical inference we care about two questions:\n\nIs there evidence of a real effect? (a hypothesis test / \\(p\\)-value question)\n\nHow big is the effect, and is it meaningful? (an effect size / estimation question)\n\nThree ideas determine how confidently we can answer these:\n\nEffect size: the magnitude of the difference or relationship. Larger effects are easier to spot and usually more practically important.\nSample size (\\(n\\)): how much data we have. Bigger \\(n\\) reduces sampling noise, tightens confidence intervals, and increases the chance of detecting real effects.\nPower (aka “Sensitivity”): the probability that a test will detect a true effect (reject \\(H_0\\) when it is false). Power grows with larger effects and larger samples, and shrinks when you demand stronger evidence (smaller \\(\\alpha\\)) or when variability is high.\n\nIt is important to consider effect size, sample size, and statistical power together when designing or interpreting a study. A very small effect can become statistically significant with a sufficiently large \\(n\\), yet still be of little practical importance. Przeworski et al. (2000) and Boix & Stokes (2003), for example, agree “that the impact of GDP on democratization in the postwar period is negligible, even though it may be statistically significant.” (Epstein et al., 2006) Conversely, with too small a sample, even a moderate effect may fail to reach significance simply because statistical power is inadequate. Good study planning therefore requires balancing the smallest effect that is considered meaningful (the minimum important difference) against a sample size that provides adequate power at the chosen significance level \\(\\alpha\\). As a rough guide, I have summarised some scenarios for the interplay of effect size and sample size on statistical power.\n\n\n\n\n\n\n\n\n\nEffect Size\nSample Size\nLikely Outcome\n\n\n\n\nLarge\nSmall\nMay still detect effect\n\n\nSmall\nLarge\nLikely to detect even small effect\n\n\nSmall\nSmall\nLow power – may miss real effect\n\n\nLarge\nLarge\nHigh chance of detecting effect\n\n\n\n\n\nTable 1: Rule-of-thumb scenarios",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO91Q.html#what-is-cohens-d",
    "href": "04-PO91Q.html#what-is-cohens-d",
    "title": "Week 4",
    "section": "What is Cohen’s \\(d\\)?",
    "text": "What is Cohen’s \\(d\\)?\nTo measure effect size, we will be using Cohen’s \\(d\\). Cohen’s \\(d\\) is a way of describing how big an effect is, in units that don’t depend on the original scale. Let me explain this by comparing how far the average exam mark (42) from a sample of students is to the pass mark of 40. In short, instead of saying “the mean was 42 vs the null value of 40”, we will ask: “how big is that 2-point shift compared to the typical variation in marks?”\nWe start by identifying the observed difference. With a sample mean of 42 (mean indeed!) and a null hypothesis value of 40, the observed difference is\n\\[\n\\text{Difference} = 42 - 40 = 2\n\\]\nThis raw difference might seem informative at first glance, but it will be hard to compare this across different contexts. Why? Suppose we look at exam marks in two different modules. In course A students’ marks are close bunched together, most within about 5 points of the average. In this setting, a 2-point increase is fairly big compared to the usual spread. In module B, meanwhile, students’ marks vary a lot, often 20 points above or below the average. In this setting, the same 2-point increase is tiny compared to the usual spread.\nThis is why we cannot stop at raw differences – we need to find a way to standardise the difference so that we can compare across different contexts. And this is precisely what Cohen’s \\(d\\) does. It is defined as\n\\[\nd = \\frac{\\bar x - \\mu_0}{\\text{s}}\n\\]\nwhere \\(\\mu_0\\) is the null hypothesis value. This formula standardises the raw distance by dividing it by the standard deviation (s) of marks.\nTo continue with our example, if the typical standard deviation of exam marks is about 10 points, then:\n\\[\nd = \\frac{42 - 40}{10} = \\frac{2}{10} = 0.20\n\\]\nSo our observed effect is \\(d = 0.20\\). How do we interpret this result? Cohen (2013) suggested some rough rules of thumb for interpreting \\(d\\):\n\nInterpretation\n\n\\(d \\approx 0.2 \\Rightarrow\\) small effect (a subtle shift, often hard to detect)\n\n\\(d \\approx 0.5 \\Rightarrow\\) medium effect (a clear, noticeable shift)\n\\(d \\approx 0.8 \\Rightarrow\\) large effect (a big, obvious shift)\n\nNote that these are rough guidelines, only. The onus is on you as a researcher to judge what is required/acceptable in the context of your project.\n\nA value of \\(d = 0.20\\) in our example means the average mark is 0.2 standard deviations above the null value. That is considered a small effect: the group did a little better than the pass mark, but the difference is modest relative to the variability in marks.\nIn summary, Cohen’s \\(d\\) encourages us to think not just about whether an effect exists, but whether it is meaningfully large. We are now going to use it to determine the statistical power of a one-sample significance test. You will see that smaller \\(d\\) values need larger sample sizes to be detected reliably.\n\nWhat is statistical power?\nStatistical power is the probability that a study will detect an effect if the effect really exists. Formally, we can write this as:\n\\[\n\\text{Power} = P(\\text{reject } H_0 \\mid \\text{effect exists})\n\\]\n\nHigh power means we are likely to detect real effects.\n\nLow power means we might miss them, even if they exist.\n\nA common benchmark is 80% power (see Lakens (2013) and Button et al. (2013)), which means: if the true effect is at least as big as the one we are testing for, then in the long run we would detect it in 80% of studies with the same sample size and test procedure.\nBut how do we get from \\(d\\) to power? The link is through the t-statistic. When you read the fomula and description for Cohen’s \\(d\\) earlier, you might have had a deja-vue, because the t-score is defined as\n\\[\nt = \\frac{\\bar x - \\mu_0}{\\text{se}}, \\qquad \\text{where se}=\\frac{s}{\\sqrt{n}}\n\\]\nThis means that \\(t\\) and \\(d\\) are very closely related, namely through this little exercise in algebra:\n\\[\\begin{align}\nt &\\;=\\; \\frac{\\bar{x}-\\mu_0}{\\,s/\\sqrt{n}\\,} = \\frac{(\\bar{x}-\\mu_0)\\cdot(1/s)}{(s/\\sqrt{n})\\cdot(1/s)} \\;= \\\\ \\\\\n  &\\;=\\; \\frac{(\\bar{x}-\\mu_0)/s}{(s/\\sqrt{n})/s} \\;=\\; \\frac{(\\bar{x}-\\mu_0)/s}{\\,1/\\sqrt{n}\\,} \\;= \\\\ \\\\\n  &\\;=\\; \\left( \\frac{\\bar{x}-\\mu_0}{s} \\right) \\cdot \\frac{\\sqrt{n}}{1}  \\;=\\; \\left( \\frac{\\bar{x}-\\mu_0}{s} \\right) \\cdot \\sqrt{n} \\;= \\\\ \\\\\n  &\\;=\\; d\\cdot \\sqrt{n}.\n\\end{align}\\]\n\n\\(d\\) tells us the size of the effect relative to the spread.\n\n\\(\\sqrt{n}\\) tells us how sample size sharpens our estimate: larger \\(n\\) shrinks sampling noise.\n\nSo for a fixed \\(d\\), increasing \\(n\\) makes \\(t\\) larger and pushes us past the critical threshold for significance more often — which is exactly what raises power. The key is that statistical power is defined as the probability of correctly rejecting the null when the effect size is truly \\(d\\). Equivalently, it is the probability that the test statistic \\(t\\), which depends on both \\(d\\) and \\(n\\), will exceed the critical cut-off. Thus, as \\(n\\) increases, the distribution of \\(t\\) shifts further into the rejection region, and the probability of crossing the cut-off – the power – goes up.\nThe calculation of power itself is slightly beyond the scope of an introductory level module, because we need a different distribution, a noncentral \\(t\\) distribution to be precise, for this purpose. Instead, I have designed an app which calculates this automatically for you (see below). What might be interesting to know in the context of the lecture, however, is that it is equal to \\(1-\\beta\\) where \\(\\beta\\) is the Type II Error. Let’s conclude this section with the following statements:\n\n\nFor a small effect (\\(d = 0.20\\)), increasing \\(n\\) pushes \\(t\\) upward by \\(\\sqrt{n}\\).\n\nWith small \\(n\\), sampling noise keeps \\(t\\) below the cut-off most of the time (low power).\n\nWith larger \\(n\\), the standard error shrinks, \\(t\\) grows, and power increases.\n\nIn terms of the exam example, you could use this information for planning a survey: if you care about detecting a small but meaningful 2-point difference, you generally need larger samples than 25 – often well above 100 – to achieve comfortable power (for example, around 80%).\n\n\n\n\nAn Application\nTo illustrate how this works in practice, I have written an application that lets you evaluate a completed one-sample t-test and plan a replication. You enter the results you observed in that test, and the app shows the statistical power at your observed \\(n\\) (two-sided, at your chosen \\(\\alpha\\)). As a special feature, you can switch to the “smallest effect size of interest” (SESOI), which lets you plan the \\(n\\) required for a meaningful effect.\n\nHere is how to use it:\n\nEnter the observed t-statistic and the observed sample size \\(n\\).\nChoose \\(\\alpha\\) (significance level).\nInterpret the statistical power of your test.\nOptional: turn on SESOI and pick a Cohen’s \\(d\\) (Small \\(\\approx 0.2\\), Medium \\(\\approx 0.5\\), Large \\(\\approx 0.8\\)).\n\nSESOI tells the app that only effects at least this big are meaningful.\n\nIf SESOI is off, the app uses the observed effect \\(\\hat d = t/\\sqrt{n}\\).\n\nIf SESOI is on, the app fixes the curve at your chosen \\(d\\) and, for convenience, shows the required \\(n\\) for 80% power as an orange dotted line.\n\n\nIn order to choose \\(d\\) appropriately, pick a minimum meaningful change in real units and divide by a typical standard deviation. For example, if you think that a difference of 2 exam points from an assumed null value (such as the fail mark of 40) matters and the SD is about 10, then \\(d \\approx 2/10 = 0.2\\) (small).\n\n\n\nHow the “Power vs n” curve is calculated\n\nWhen you choose a fixed standardized effect size \\(d\\) (either your observed \\(\\hat d = \\tfrac{t}{\\sqrt{n_{\\text{obs}}}}\\) or a SESOI), the curve shows the power of a two-sided one-sample \\(t\\)-test across many candidate sample sizes \\(n\\).\nFor each \\(n\\):\n\nDegrees of freedom: \\(\\mathrm{df} = n - 1\\)\n\nNoncentrality parameter: \\(\\delta = d\\sqrt{n}\\)\n\nCritical cut-off (two-sided, level \\(\\alpha\\)): \\(t_{\\alpha/2,\\,\\mathrm{df}}\\)\n\nUnder a true effect \\(d\\), the test statistic follows a noncentral \\(t\\) distribution:\n\\[\nT \\sim t_{\\mathrm{df}}(\\text{ncp} = \\delta)\n\\]\nThe power is the probability that \\(|T|\\) exceeds the critical cut-off:\n\\[\n\\mathrm{power}(n)\n= \\Pr\\!\\left(|T|\\ge t_{\\alpha/2,\\ \\mathrm{df}} \\,\\middle|\\, \\delta\\right)\n= \\big[1 - F_{\\text{nct}}(t_{\\alpha/2,\\ \\mathrm{df}};\\ \\mathrm{df},\\ \\delta)\\big]\n+ F_{\\text{nct}}(-t_{\\alpha/2,\\ \\mathrm{df}};\\ \\mathrm{df},\\ \\delta)\n\\]\nwhere \\(F_{\\text{nct}}\\) is the CDF of the noncentral \\(t\\) distribution.\nWhat the curve means\n\nSESOI off: The curve treats the observed \\(\\hat d = t_{\\text{obs}}/\\sqrt{n_{\\text{obs}}}\\) as the “true” effect and shows how power would change if you repeated the study with different \\(n\\).\n\nSESOI on: The curve instead uses your chosen meaningful effect size \\(d\\). In this mode, the app also shows the required \\(n\\) to reach 80% power with an orange marker.\n\nReproducing the curve in R\n\nlibrary(tidyverse)\n\n# Power of a two-sided one-sample t-test via the noncentral t distribution\npower_one_sample &lt;- function(d, n, alpha = 0.05) {\n  df    &lt;- n - 1\n  tcrit &lt;- qt(1 - alpha/2, df = df)\n  delta &lt;- d * sqrt(n)\n  p_upper &lt;- 1 - pt(tcrit, df = df, ncp = delta)\n  p_lower &lt;-     pt(-tcrit, df = df, ncp = delta)\n  p_upper + p_lower\n}\n\n# Example settings\nalpha &lt;- 0.05\nd_use &lt;- 0.50          # e.g., \"medium\" SESOI; or d_use &lt;- tobs/sqrt(nobs)\nn_seq &lt;- 5:200\n\n# Compute the curve\npower_vals &lt;- sapply(n_seq, power_one_sample, d = d_use, alpha = alpha)\ndf_curve   &lt;- data.frame(n = n_seq, power = power_vals)\n\n# Optional: check against power.t.test at a single n\nn_check &lt;- 30\np1 &lt;- power_one_sample(d = d_use, n = n_check, alpha = alpha)\np2 &lt;- stats::power.t.test(\n  n = n_check, delta = d_use, sd = 1,\n  sig.level = alpha, type = \"one.sample\", alternative = \"two.sided\"\n)$power\nsprintf(\"Check at n=%d: custom=%.4f, power.t.test=%.4f\", n_check, p1, p2)\n\n[1] \"Check at n=30: custom=0.7540, power.t.test=0.7540\"\n\n# Plot the curve\nggplot(df_curve, aes(n, power)) +\n  geom_line(linewidth = 1) +\n  geom_hline(yintercept = 0.80, linetype = 2) +\n  coord_cartesian(ylim = c(0, 1)) +\n  labs(\n    title = sprintf(\"Power vs n (one-sample t-test; d = %.2f, alpha = %.2f)\", d_use, alpha),\n    x = \"Sample size n\",\n    y = \"Power\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nWhy sd = 1 in the check? Because Cohen’s \\(d\\) is already standardized (\\(d = \\Delta/\\sigma\\)). The function power.t.test expects the unstandardized difference \\(\\Delta\\) and the population standard deviation \\(\\sigma\\). Setting sd = 1 and delta = d makes the inputs consistent with a standardized effect size.\n\n\n\nHow to read the Power vs \\(n\\) plot\n\nAxes and lines\n\nX-axis: sample size \\(n\\) (one-sample test).\nY-axis: power (0 to 1).\nSolid curve: analytical power for the current effect and \\(\\alpha\\).\n\nWith SESOI off: uses \\(\\hat d = t/\\sqrt{n_{\\text{obs}}}\\).\nWith SESOI on: uses your chosen \\(d\\) (SESOI).\n\nDashed horizontal line at 0.80: a common 80% power target.\nDotted vertical line (grey): your observed \\(n\\); the dot shows power at that \\(n\\).\nDotted vertical line (orange): shown only when SESOI is on; marks the smallest \\(n\\) that reaches 0.80 power (labelled as \\(n \\approx \\dots\\) @ 80% power).\n\nFinding the sample size you need\n\nWith SESOI on, read the orange line and label to get the sample size to plan for 80% power at your chosen \\(\\alpha\\).\nIf the curve never hits 0.80 within the plot range, you would need a larger \\(n\\), a larger meaningful effect (bigger \\(|d|\\)), a one-sided test (only if justified), or a larger \\(\\alpha\\) (with caution).\n\nHow the controls shift the curve\n\nLarger \\(|d|\\) moves the curve up and left (fewer \\(n\\) needed).\nIncreasing \\(\\alpha\\) (for example, 0.10 vs 0.05) moves the curve up; decreasing \\(\\alpha\\) moves it down.\nChanging the observed \\(t\\) or \\(n\\) with SESOI off updates \\(\\hat d\\) and thus the curve. With SESOI on, the curve stays fixed by your chosen \\(d\\); changing \\(n\\) only moves the dot.\n\nShape and diminishing returns\n\nThe curve is steepest at small \\(n\\) and flattens as power approaches 1.\nBecause \\(t = d\\sqrt{n}\\), halving \\(|d|\\) typically requires about four times the \\(n\\) to maintain similar power.\n\nInterpreting the dot and the lines\n\nThe dot is your current power at the observed \\(n\\).\nIf the dot is below 0.80, you likely need a bigger \\(n\\) (or a larger \\(d\\) of interest, higher \\(\\alpha\\), or a justified one-sided test).\n\nRemember\n\nPower is the probability to reject the null-hypothesis given that an effect exists, or formally \\(P(\\text{reject } H_0 \\mid \\text{effect exists})\\). It is not a \\(p\\)-value and not the probability that \\(H_0\\) is true.\nYou can pair power planning with a SESOI: decide what \\(d\\) would be meaningful in your context and size your study to detect it.\n\n\n\nWhat this means in practice:\n\nThat a result is statistically significant does not mean that it is practically important. With very large \\(n\\), even tiny effects can be significant.\n\nEqually, insignificant results do no mean “no true effect”. With small \\(n\\), a real effect can be missed because power is low.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO91Q.html#exercises-working-with-the-app",
    "href": "04-PO91Q.html#exercises-working-with-the-app",
    "title": "Week 4",
    "section": "Exercises – Working with the app",
    "text": "Exercises – Working with the app\n\nSame effect size, different n (mapping \\(t\\), \\(n\\), and \\(\\hat d\\))\n\nSet observed \\(t=1.6\\) and \\(n=16\\). Note \\(\\hat d = t/\\sqrt{n}\\) and the power at the observed \\(n\\).\n\nNow keep \\(\\hat d\\) the same but increase \\(n\\): set \\(n=64\\) and \\(t=3.2\\) (since \\(t = d \\times \\sqrt{n}\\)). Confirm that \\(\\hat d\\) is unchanged while power increases.\n\nExplain why power rises with \\(n\\) even when the underlying effect size \\(\\hat d\\) stays the same.\n\nPlanning with a SESOI and \\(\\alpha\\) sensitivity\n\nEnter \\(t=2.0\\) and \\(n=30\\) (\\(\\alpha=0.05\\)). Record \\(\\hat d\\) and the power at the observed \\(n\\).\n\nTurn on “Use smallest effect size of interest” and choose SESOI \\(d=0.5\\). Read the orange marker: the required \\(n\\) for \\(80\\%\\) power at \\(\\alpha=0.05\\).\n\nChange \\(\\alpha\\) to \\(0.10\\) and then to \\(0.01\\). How does the required \\(n\\) for \\(80\\%\\) power shift as \\(\\alpha\\) changes? Explain why a stricter \\(\\alpha\\) requires a larger \\(n\\) for the same power.\n\nWas the study well powered? Post-hoc check and replication planning\n\nSuppose your study reported \\(t=2.1\\) with \\(n=25\\) at \\(\\alpha=0.05\\). Enter these and record \\(\\hat d\\) and the power at the observed \\(n\\). Is the power comfortably high?\n\nIf you wanted \\(80\\%\\) power to detect an effect around this magnitude in a replication, turn on SESOI and choose the closest option to your \\(\\hat d\\) (for example, Small \\(d=0.2\\) if \\(\\hat d\\) is small, Medium \\(d=0.5\\) if \\(\\hat d \\approx 0.5\\)). Note the orange “\\(n\\) for \\(80\\%\\) power” marker.\n\nExplain why a significant result with low power can be fragile, and how choosing a SESOI (a target \\(d\\)) helps you plan a more reliable replication sample size.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO91Q.html#r-exercises---core",
    "href": "04-PO91Q.html#r-exercises---core",
    "title": "Week 4",
    "section": "R Exercises - Core",
    "text": "R Exercises - Core\nThese exercises will use the ks2 dataset. This data comprises fictitious3 average grades of Key Stage 2 (KS2) students in the UK, with 1,980 KS2 students’ test scores being included for reading (reading), mathematics (maths), and grammar, punctuation, and spelling (gps), as well as the mean of these three test scores (avg_all). The test scores have been standardised, with 80 representing the lowest possible mark, 120 representing the highest possible mark, 100 representing the minimum passing mark, and -1 representing an ungraded test.\n\nLoad the ks2 dataset into R. Remove any observations that have any ungraded test scores.\nCalculate the means and standard deviations of each of the three subjects. Write a brief description of the insights that can be drawn from these values.\nConduct a t-test to see if the means of each of the three subjects’ marks are statistically different from 100 at the 95% confidence level. Identify three ways that suggest statistically significant or insignificant differences.\nConduct a t-test to see if the mean of the average score of the three tests is statistically less than 105 at the 99% confidence level. Interpret the results.\nThe following questions will look at students’ English abilities generally.\n\nCreate a new variable called english, which consists of the average of the reading and grammar, punctuation, and spelling variables.\nConduct a t-test to see if the mean of the average score of the new English variable is statistically less than 105 at the 99.9% confidence level. Interpret the results.\nConduct a t-test to see if the mean of the average score of the new English variable is statistically different from 105 at the 99.9% confidence level. Interpret the results.\nIs the any difference in the interpretation between the two above tests? Are there any differences in the results? Why?\n\nYou are tasked with investing the performance of students who passed in mathematics those who did not. For the following tests, use a 95% confidence level.\n\nCreate a binary variable that has two categories: those who passed mathematics (100 \\(\\leq\\) mark) and those who failed mathematics (mark \\(&lt;\\) 100).\nConduct a proportion test to see if the proportion of students that fail mathematics is 10% or greater. Interpret the results.\nConduct a t-test on the group who fail mathematics to see if they, on average, have marks for English statistically less than 100. Interpret the results.\nConduct a t-test on the group who pass mathematics to see if they, on average, have marks for grammar, spelling, and punctuation statistically greater than 105. Interpret the results.\n\nNow it is worth investigating how students who fail at least one subject perform.\n\nCreate a binary variable that has two categories: those who passed all three subjects (all marks greater than or equal to 100) and those who failed at least one subject (one or more marks less than 100).\nIt can be hypothesised that the group of students who failed will have a mean of all of the test marks significantly below the pass mark of 100. Test this and interpret the findings with respect to the statistical and practical significance of the test.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO91Q.html#r-exercises---going-further",
    "href": "04-PO91Q.html#r-exercises---going-further",
    "title": "Week 4",
    "section": "R Exercises - Going Further",
    "text": "R Exercises - Going Further\n\nImagine you are part of a team work working within the Department for Education, tasked with investigating this sample to produce recommendations for policymakers.\n\nNormalise the variable that contains the average of all three marks by setting the lowest mark (80) to 0, the highest mark (120) to 100, and the minimum pass mark (100) to 50. Justify why it might be useful to normalise these marks to this scale for non-specialist policymakers.\nConstruct a categorical variable that consists of five categories: 0-49.99 (Fail), 50-59.99 (Pass), 60-60.99 (Merit), 70-79.99 (Distinction), and 80+ (Distinction+).\nAnswer the following questions but write your answers as if intended for a non-specialist policy making with no knowledge of statistics:\n\nAre the averages of the Pass, Merit, and Distinction groups different from their middle marks (55, 65, and 75, respectively)? If so, which direction?\nIs the average mark of the Distinction+ group lower than the maximum mark of the group?\nIs the mean mark of the Fail group higher than the median mark of the group? Which skew does this indicate in the distribution?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO91Q.html#solutions",
    "href": "04-PO91Q.html#solutions",
    "title": "Week 4",
    "section": "Solutions",
    "text": "Solutions\nYou can find the Solutions in the Downloads Section.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "04-PO91Q.html#footnotes",
    "href": "04-PO91Q.html#footnotes",
    "title": "Week 4",
    "section": "",
    "text": "Some of the content of this worksheet is taken from Reiche (forthcoming).↩︎\nThese are taken from Reiche (forthcoming).↩︎\nThe means are based on KS2 scaled score averages which have been averaged over 2016-2019.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Week 4</span>"
    ]
  },
  {
    "objectID": "05-PO91Q.html",
    "href": "05-PO91Q.html",
    "title": "Week 5",
    "section": "",
    "text": "Self-Assessment Questions1",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO91Q.html#codebook",
    "href": "05-PO91Q.html#codebook",
    "title": "Week 5",
    "section": "Codebook",
    "text": "Codebook\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nlabel\n\n\n\n\nCouncil_area\nCouncil area name\n\n\nalc16\nHospital stays related to alcohol use: standardised ratio\n\n\nmortality16\nStandardised mortality ratio (2011-2014)\n\n\nmortality20\nStandardised mortality ratio (2014-2018)\n\n\n\n\n\n\nTable 1: SIMD Codebook",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO91Q.html#data-exploration",
    "href": "05-PO91Q.html#data-exploration",
    "title": "Week 5",
    "section": "Data Exploration",
    "text": "Data Exploration\nBefore starting, we need to load libraries and install packages if not already installed. In these exercises we will be using the tidyverse package.\n\nSet your working directory, place the data set in it, and load it into R.\nCreate a new RScript for this case study and annotate it as you go through the exercises presented here.\nLoad the tidyverse package.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO91Q.html#descriptive-statistics",
    "href": "05-PO91Q.html#descriptive-statistics",
    "title": "Week 5",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\nProduce descriptive statistics for all three numerical variables.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO91Q.html#visualisation",
    "href": "05-PO91Q.html#visualisation",
    "title": "Week 5",
    "section": "Visualisation",
    "text": "Visualisation\nLet’s visualise the distribution of the variable alc16.\n\n\n\n\n\n\nFigure 1: Distribution of Alcohol-Related Hospital Admissions (2011-2014)\n\n\n\n\nReproduce Figure 1.\nWhat does the distribution tell us about alcohol-related admissions to hospital?\nHow does the shape of the distribution in Figure 1 relate to the descriptive statistics calculated in the previous Section?\nWhat would happen to the shape of the distribution if the median was smaller than the mean?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO91Q.html#hypothesis",
    "href": "05-PO91Q.html#hypothesis",
    "title": "Week 5",
    "section": "Hypothesis",
    "text": "Hypothesis\nWe are interested in how alcohol-related admissions to hospital have affected mortality rates in Scotland. The following scatter plot uses the variables alc16 and mortality20.\n\n\n\n\n\n\nFigure 2: Alcohol-Related Hospital Admissions and Mortality\n\n\n\n\nReproduce Figure 2.\nBased on this scatter plot, formulate the alternative and the null hypotheses:\n\n   H\\(\\pmb{_0}\\):\n\n   H\\(\\pmb{_\\text{A}}\\):",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO91Q.html#sampling",
    "href": "05-PO91Q.html#sampling",
    "title": "Week 5",
    "section": "Sampling",
    "text": "Sampling\nThe data frame simd which we have been using so far represents the population. Let us now draw a random sample of 15 councils as follows:\n\nset.seed(6)\nsample &lt;- sample_n(simd, 15)\n\n\nExplain the purpose of the set.seed function.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO91Q.html#inferential-statistics",
    "href": "05-PO91Q.html#inferential-statistics",
    "title": "Week 5",
    "section": "Inferential Statistics",
    "text": "Inferential Statistics\nLet us now see if the mortality ratio has changed between the two waves of 2016 and 2020. This is the worked example from the lecture, but I am repeating it here deliberately, so that you can carry out the example yourself.\n\nAs a first step, create a new variable measuring the difference between mortality20 and mortality16. Make sure that increases are positive and decreases negative.\nWhat is the sample mean of the differences in mortality rates, variable diff?\nThe sample size of 15 is small. Will it be appropriate to conduct a t-test? Why? Why not?\nFind out whether the difference in mortality rates is significantly different from zero.\nDraw a graph which depicts the direction of the alternative hypothesis and the p-value. Try not to look at the lecture slides.\nSuppose the Scottish Government claims that mortality rates have decreased. Test this claim.\nAgain, draw a graph which depicts the direction of the alternative hypothesis and the p-value. Try not to look at the lecture slides.\nDrawing on the results from Exercises 5 and 7, reason about the p-value you would obtain if you tested the hypothesis that mortality rates have increased between the waves of 2016 and 2020.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO91Q.html#causality",
    "href": "05-PO91Q.html#causality",
    "title": "Week 5",
    "section": "Causality",
    "text": "Causality\n\nIdentify the elements of symmetry and asymmetry in the setup of this case study.\nConsider again Figure @ref(fig:causalityfw) from the lecture. Which aspects of establishing causality has the case study addressed? What is missing?\n\n\n\n\n\n\n\nFigure 3: Causality Framework",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO91Q.html#solutions",
    "href": "05-PO91Q.html#solutions",
    "title": "Week 5",
    "section": "Solutions",
    "text": "Solutions\nYou can find the Solutions in the Downloads Section.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "05-PO91Q.html#footnotes",
    "href": "05-PO91Q.html#footnotes",
    "title": "Week 5",
    "section": "",
    "text": "Some of the content of this worksheet is taken from Reiche (forthcoming).↩︎",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Week 5</span>"
    ]
  },
  {
    "objectID": "06-PO91Q.html",
    "href": "06-PO91Q.html",
    "title": "Week 6",
    "section": "",
    "text": "Reading Week\nReading week is not an institutionalised holiday, and I do expect you to put in about 10 hours of work for this module over the course of this week. Here are a few suggestions for activities to fill these 10 hours:\n\nCatch up on the reading\nRevise the material of Weeks 1-5, as we will switch to bivariate and multivariate analysis in Week 7.\nGo through the worksheets of Weeks 1-5 to make sure you are on top of things with R.\nRevise all R functions up to this point. I have created a Reading Week Consolidation Flashcard Section for this.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Week 6</span>"
    ]
  },
  {
    "objectID": "07-PO91Q.html",
    "href": "07-PO91Q.html",
    "title": "Week 7",
    "section": "",
    "text": "Methods, Methods, Methods\nIn the coming weeks, you will find this section on the Companion to help you apply the material we cover in the lecture in R. It will contain an RScript for some preliminary data preparation. This is not an actual part of introducing the method, but you are certainly encouraged to read it and to try and understand it.\nThese sections will work with the American National Election Studies (ANES), and to be more precise with the pilot survey conducted before the 2020 presidential election. You can download the required data set by following this link. The actual data set itself is available here.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO91Q.html#data-prep",
    "href": "07-PO91Q.html#data-prep",
    "title": "Week 7",
    "section": "Data Prep",
    "text": "Data Prep\nPlace the ANES data in a folder which you will be using as a working directory for this session. Open the “Code for Data Preparation” below, and copy this into an RScript. Remember to adjust the working directory with the setwd() command at the beginning. Then run the RScript and you will be ready to proceed to the video.\n\n\n\nCode for Data Preparation\n\n\n######################################\n# MMM - Week 1 - Data Preparation\n######################################\n\n\n# Set WD\nsetwd()\n\n# Load packages\n\nlibrary(tidyverse)\n\n# Load data set\n\nanes &lt;- read_csv(\"anes.csv\")\n\n# Get rid of missing values for variables used in analysis today\n\n## 999 is equivalent to NA, so needs to be recoded\nanes$fttrump1 &lt;- with(anes, replace(fttrump1, fttrump1 == 999, NA)) \n\nanes &lt;- filter(anes, \n               !is.na(fttrump1),\n               !is.na(sex))\n\n# turn support for Trump `fttrump1` into ordered factor with three levels\n\nanes &lt;- anes %&gt;% \n  mutate(trump=\n           ordered(\n             cut(fttrump1, breaks=c(0, 33, 66, 100), \n                 labels=c(\"low\",\"medium\",\"high\"))))\n\n# Label variable `sex`\nanes$sex &lt;- factor(anes$sex)\n\nanes &lt;- anes %&gt;% \n  mutate(sex=\n           recode(sex,\"1\"=\"Male\",\n                  \"2\"=\"Female\"))\n\n# save data set for use in video\n\nwrite.csv(anes, \"anes_week1.csv\")\n\n\n\n\nYou can copy the code from this page by hovering over the code chunk and clicking the icon in the top-right hand corner. You can then paste it into your RScript.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO91Q.html#video-and-rscript",
    "href": "07-PO91Q.html#video-and-rscript",
    "title": "Week 7",
    "section": "Video and RScript",
    "text": "Video and RScript\nYou can find the video introducing you to this week’s method by way of a worked example below. You can also access the code I am typing up in the video in the “Code for Data Analysis” section. I would encourage you to type it yourself, though, as then code tends to better sink into the depths of your brain 😉\n\n\n\nCode for Data Analysis\n\n\n######################################\n# MMM - Week 1 - Crosstabulations\n######################################\n\n# Set WD\nsetwd()\n\n# Load packages\n\nlibrary(tidyverse)\n\n# Load data set\n\nanes &lt;- read_csv(\"anes_week1.csv\")\nanes$sex &lt;- as.factor(anes$sex)\nanes$trump &lt;- as.factor(anes$trump)\n\n# Tabulate relationship between sex and support for Trump\n\ntable(anes$sex,anes$trump)\n\nprop.table(table(anes$sex,anes$trump))\n\nprop.table(table(anes$sex,anes$trump), margin = 1)\n\n# save table for analysis\n\ntrumpsex &lt;- table(anes$sex,anes$trump)\n\ntrumpsex\n\nxsq &lt;- chisq.test(trumpsex, correct=FALSE)\n\nxsq\n\n# display observed and expected values\n\nxsq$expected\n\nxsq$observed\n\n\n\n\nCross Tabulations in R",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO91Q.html#crosstabs",
    "href": "07-PO91Q.html#crosstabs",
    "title": "Week 7",
    "section": "Crosstabs",
    "text": "Crosstabs\nI have given you an example of a cross-tabulation in the lecture. Consider the following Table:\n \n\n\n\n\n\n\n\nMode of Transport\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear of Study\n\n\nBike\n\n\nBus\n\n\nCar\n\n\nTotal\n\n\n\n\n\n\nPAIS\n\n\n10\n\n\n5\n\n\n5\n\n\n20\n\n\n\n\nCIM\n\n\n8\n\n\n12\n\n\n10\n\n\n30\n\n\n\n\nTotal\n\n\n18\n\n\n17\n\n\n15\n\n\n50\n\n\n\n\n\n \nCalculate the Expected Values and fill in the following table:\n \n\n\n\n\n\n\n\nMode of Transport\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear of Study\n\n\nBike\n\n\nBus\n\n\nCar\n\n\nTotal\n\n\n\n\n\n\nPAIS\n\n\n\n\n\n\n\n\n\n\n\n\nCIM\n\n\n\n\n\n\n\n\n\n\n\n\nTotal\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\nCalculate the \\(\\chi^{2}\\)-value\nHow many degrees of freedom does this table have? Why?\nUsing the \\(\\chi^2\\) Table, what is the p-value?\nAre mode of transport and departmental association independent in the population?\n\nThe remaining questions can be answered with the help of the app below (it might take a moment to load, as it is a web-based application).\n\n\n\n\n\n\nSmall effect, big N\n\nBuild a table with a large total N but only a slight row difference (for example, a=210, b=190, c=190, d=210).\nNote the χ² p-value. Now scale the same proportions down (for example, a=21, b=19, c=19, d=21).\nWhat happens to the \\(\\chi^2\\) p-value as N changes, and why? Use “Row %” to confirm the effect size stayed about the same.\n\nSame row-percentage gap, different base rates\n\nConstruct two tables with a similar difference between the rows’ “Yes” percentages but very different column totals (base rates).\n\nExample A (balanced): a=60, b=40, c=40, d=60 (N=200).\nExample B (skewed): a=90, b=10, c=70, d=30 (N=200).\n\nCompare χ² p-values and the “Row %” and “Column %” views. How do base rates (margins) influence the test, even when the row-percentage gap is similar?\n\nExploring Fisher’s exact test\n\nBuild some very small tables (with low cell counts). Try cases where at least one expected count is less than \\(5\\), and cases where all expected counts are above \\(5\\). Watch for when the app switches from showing a \\(\\chi^2\\) p-value to also showing a Fisher p-value.\nCompare the \\(\\chi^2\\) and Fisher results in these tiny-\\(N\\) situations. Do they always agree? When do they differ most?\nBased on your experiments, summarize in your own words:\n\nWhat condition seems to trigger Fisher’s test to appear?\nWhy might researchers prefer Fisher’s exact test in those situations?\nIn larger samples, why is \\(\\chi^2\\) usually preferred instead?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO91Q.html#correlation",
    "href": "07-PO91Q.html#correlation",
    "title": "Week 7",
    "section": "Correlation",
    "text": "Correlation\nI have written yet another app! Once it loads (it will take a little time, it’s a beast), it lets you generate and explore bivariate relationships and see how Pearson’s r responds to different shapes, amounts of noise, and sample sizes. You can overlay a straight-line fit (the regression line we will explore next week) and a smooth non-linear fit (LOESS) to compare them. The current Pearson r is shown as a label on the plot.\nPlease note that in the explanations below I will refer to some relationships as “monotonic”, so here is a brief definition:\n\nMonotonic means “always moving in one direction”.\nA relationship is monotonic if, as x increases, y never reverses direction: it either never decreases (monotone increasing) or never increases (monotone decreasing). The rate can change and the curve can be curved or flat in places – it just must not switch from increasing to decreasing (or vice versa).\n\nBy the way, in the lecture we have focused on continuous data and this is preset in the app. But from the reading you (hopefully) know that other correlation coefficients exist for categorical data. You can explore an example for binary variables in the app, as well.\n\n\n\n\n\nGetting started\n\nUnder Data type, choose Continuous (scatter).\nPick a Relationship pattern:\n\nLinear (bivariate normal): straight-line trend with a controllable target r.\nQuadratic: U-shaped, clearly non-linear and non-monotonic.\nCubic: monotonic but curved, a single overall direction with curvature.\n\nAdjust Sample size (n) and either Target Pearson r (for Linear) or Noise (for Quadratic and Cubic).\nToggle the line of best fit (regression line) and LOESS smooth to compare linear versus non-linear fits.\nUse Seed to regenerate the sample while keeping settings the same.\nRead the Pearson r label on the plot.\n\nControls at a glance\n\nRelationship pattern\n\nLinear (bivariate normal): set Target Pearson r.\nQuadratic and Cubic: set Noise to make the pattern tighter or looser.\n\nn (Sample size): larger n stabilizes estimates.\nSeed: changes the random draw while keeping other settings fixed.\nShow least-squares line: straight-line fit.\nShow smooth (LOESS): flexible curve that can reveal non-linear structure.\n\nTips for interpretation\n\nPearson’s r captures linear association.\nQuadratic is not monotonic, so r can be near zero even when a strong U-shape is visible.\nCubic is monotonic but curved; r can be pulled down by curvature even when a strong relationship exists.\nLOESS helps reveal curvature and local structure that a straight line can miss.\nIncreasing n reduces sampling variability; increasing Noise weakens visible structure.\n\n\n\nExercises\nI. Linear relationships\n\nSet Linear, Target r = 0.60, n = 50. Record the Pearson r on the plot. Now set n = 300. How does r change, and what does this show about sample size?\nWith Linear, try r = 0.20, n = 60. Change Seed several times. How much does r bounce around, and why?\n\nII. Non-linear and non-monotonic (Quadratic)\n\nChoose Quadratic, set Noise = 0.10, n = 200. Is r near zero despite a clear U-shape? Explain why.\nIncrease Noise until the U-shape is hard to see. At what point do the linear and LOESS fits become similarly uninformative?\n\nIII. Non-linear but monotonic (Cubic)\n\nChoose Cubic, set Noise = 0.15, n = 200. Compare the linear line to the LOESS curve. Does the straight line understate the visible relationship?\nIncrease Noise gradually. How quickly does r decline as the pattern gets noisier? What does the LOESS curve reveal as noise grows?\n\nIV. Comparing fits (lines)\n\nFor Cubic with moderate Noise, toggle Least-squares line on and off and compare to LOESS. How does the straight line mislead about strength or direction?\nIn Linear mode, when do the LOESS and linear fits essentially coincide, and why should they?\n\nV. Sampling variability and reproducibility\n\nFix a setting, for example Linear r = 0.40, n = 80, and change Seed a few times. How much does r vary? Repeat with n = 400 and compare.\nFor Cubic with low Noise, change Seed repeatedly. How stable is r across seeds?\n\nVI. Same r, different story\n\nFind two scenarios with similar Pearson r but different shapes (for example, Linear versus Cubic with noise tuned). How do the plots and the LOESS curve help you tell them apart?\n\nVII. Reflections\n\nWhen is Pearson’s r an adequate summary, and when would you favor a non-linear description?\nBased on what you see in the app, how would you justify using a non-linear model rather than a linear one?\n\n\nTakeaways\n\nPearson r summarizes linear, directional association.\n\nNonlinear, non-monotonic relationships (e.g., U-shapes) can be strong yet have r ≈ 0. Always plot your data.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO91Q.html#applied-exercises-in-r-going-further",
    "href": "07-PO91Q.html#applied-exercises-in-r-going-further",
    "title": "Week 7",
    "section": "Applied Exercises in R (Going Further)",
    "text": "Applied Exercises in R (Going Further)\nThere is a lot to choose from this week, take your pick. There is no need to do all of these. Aim for two sections.\n\nSection 1\n\nUse the ‘prop.table’ function to look at happiness against health in a comparison between genders.Compare genders as precisely as possible.\nDoes health drive happiness more for men or women?\nIdentify the two countries with lowest and highest happiness\nCompare happiness distributions between these two countries\nWhat benefit do distributions have compared to just looking at means?\n\n\n\nSection 2\n\nFor this section use Wave 7 (2014). Name it conveniently and attach it.\nCalculate the Body Mass Index variable with BMI=weight/height.\nCompare mean and standard deviation of BMI between the UK and Spain\nApply ‘t.test’ function to test the difference between the two means (note this is now a two-sample t-test)\nHave the inhabitants of one country a significantly better BMI? Do you know why?\n\n\n\nSection 3\n\nFor this section use Wave 7 (2014). Name it conveniently and attach it.\nPlot the age at which respondents completed full-time education against the same for their fathers (note that these variables are only valid for the UK sample).\nThere might be outlying values – can you see them?\nPlot the same, excluding outlying values\nIs there a significant link between fathers and children regarding this variable?\nIs there a significant link between mothers and children?\nAre these links, if any, stronger for sons or for daughters? Can you interpret?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO91Q.html#solutions",
    "href": "07-PO91Q.html#solutions",
    "title": "Week 7",
    "section": "Solutions",
    "text": "Solutions\nYou can find the Solutions in the Downloads Section.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "07-PO91Q.html#footnotes",
    "href": "07-PO91Q.html#footnotes",
    "title": "Week 7",
    "section": "",
    "text": "Some of the content of this worksheet is taken from Reiche (forthcoming).↩︎",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Week 7</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html",
    "href": "08-PO91Q.html",
    "title": "Week 8",
    "section": "",
    "text": "MMM – Bivariate Regression\nThis week we will be starting to conduct linear regression analysis in R.\nJust as in week 7 we will be working with the American National Election Studies (ANES), and to be more precise with the pilot survey conducted before the 2020 presidential election. If you haven’t already done so, you will have to register with ANES in order to download the data set. To do so, please follow this link.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html#data-prep",
    "href": "08-PO91Q.html#data-prep",
    "title": "Week 8",
    "section": "Data Prep",
    "text": "Data Prep\nPlace the ANES data in a folder which you will be using as a working directory for this session. Open the “Code for Data Preparation” below, and copy this into an RScript. Remember to adjust the working directory with the setwd() command at the beginning. Then run the RScript and you will be ready to proceed to the video.\n\n\n\nCode for Data Preparation\n\n\n######################################\n# MMM - Week 4 - Data Preparation\n######################################\n\n\n# Set WD\nsetwd()\n\n# Load packages\n\nlibrary(tidyverse)\n\n# Load data set\n\nanes &lt;- read_csv(\"anes.csv\")\n\n# Get rid of missing values for variables used in analysis today\n\n## 999 is equivalent to NA, so needs to be recoded\nanes$fttrump1 &lt;- with(anes, replace(fttrump1, fttrump1 == 999, NA)) \nanes$income &lt;- with(anes, replace(income, income == 99, NA)) \n\nanes &lt;- filter(anes, \n               !is.na(fttrump1),\n               !is.na(age),\n               !is.na(income))\n\n# Turn income variable into a numerical variable with mid-points of each level\n\nanes$income &lt;- factor(anes$income)\ntable(anes$income)\nanes &lt;- anes %&gt;%\n  mutate(income_fac = recode(income,\n                             '1'= \"2500\",\n                             '2'= \"7499.5\",\n                             '3'= \"12499.5\",\n                             '4'= \"17499.5\",                       \n                             '5'= \"22499.5\",\n                             '6'= \"27499.5\",\n                             '7'= \"32499.5\",\n                             '8'= \"37499.5\",\n                             '9'= \"42499.5\",\n                             '10'= \"47499.5\",\n                             '11'= \"52499.5\",\n                             '12'= \"57499.5\",\n                             '13'= \"62499.5\",\n                             '14'= \"67499.5\",\n                             '15'= \"72499.5\",\n                             '16'= \"77499.5\",\n                             '17'= \"82499.5\",\n                             '18'= \"87499.5\",\n                             '19'= \"92499.5\",\n                             '20'= \"97499.5\",\n                             '21'= \"112499.5\",\n                             '22'= \"137499.5\",\n                             '23'= \"162499.5\",\n                             '24'= \"187499.5\",\n                             '25'= \"224999.5\",\n                             '26'= \"500000\"))\n\nanes$inc &lt;- as.numeric(as.character(anes$income_fac))\n\n# save data set for use in video\n\nwrite.csv(anes, \"anes_week4.csv\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html#video-and-rscript",
    "href": "08-PO91Q.html#video-and-rscript",
    "title": "Week 8",
    "section": "Video and RScript",
    "text": "Video and RScript\nYou can find the video introducing you to this week’s method by way of a worked example below. You can also access the code I am typing up in the video in the “Code for Data Analysis” section. I would encourage you to type it yourself, though, as then code tends to better sink into the depths of your brain 😉\n\n\n\nCode for Data Analysis\n\n\n######################################\n# MMM - Week 4 - Bivariate Regression\n######################################\n\n# Set WD\nsetwd()\n\n# Load packages\n\nlibrary(tidyverse)\n\n# Load data set\n\nanes &lt;- read_csv(\"anes_week4.csv\")\n\n# Visualisation\n############################\n\nggplot(anes, aes(x = inc, y = fttrump1)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n# Regression\n############################\n\nmodel1 &lt;- lm(fttrump1 ~ inc, data = anes)\n\nmodel1\n\n\n\n\nBivariate Regression in R",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html#data-prep-1",
    "href": "08-PO91Q.html#data-prep-1",
    "title": "Week 8",
    "section": "Data Prep",
    "text": "Data Prep\nPlace the ANES data in a folder which you will be using as a working directory for this session. Open the “Code for Data Preparation” below, and copy this into an RScript. Remember to adjust the working directory with the setwd() command at the beginning. Then run the RScript and you will be ready to proceed to the video.\n\n\n\nCode for Data Preparation\n\n\n######################################\n# MMM - Week 5 - Data Preparation\n######################################\n\n\n# Set WD\nsetwd()\n\n# Load packages\n\nlibrary(tidyverse)\n\n# Load data set\n\nanes &lt;- read_csv(\"anes.csv\")\n\n# Get rid of missing values for variables used in analysis today\n\n## 999 is equivalent to NA, so needs to be recoded\nanes$fttrump1 &lt;- with(anes, replace(fttrump1, fttrump1 == 999, NA)) \nanes$income &lt;- with(anes, replace(income, income == 99, NA)) \n\nanes &lt;- filter(anes, \n               !is.na(fttrump1),\n               !is.na(age),\n               !is.na(income))\n\n# Turn income variable into a numerical variable with mid-points of each level\n\nanes$income &lt;- factor(anes$income)\ntable(anes$income)\nanes &lt;- anes %&gt;%\n  mutate(income_fac = recode(income,\n                             '1'= \"2500\",\n                             '2'= \"7499.5\",\n                             '3'= \"12499.5\",\n                             '4'= \"17499.5\",                       \n                             '5'= \"22499.5\",\n                             '6'= \"27499.5\",\n                             '7'= \"32499.5\",\n                             '8'= \"37499.5\",\n                             '9'= \"42499.5\",\n                             '10'= \"47499.5\",\n                             '11'= \"52499.5\",\n                             '12'= \"57499.5\",\n                             '13'= \"62499.5\",\n                             '14'= \"67499.5\",\n                             '15'= \"72499.5\",\n                             '16'= \"77499.5\",\n                             '17'= \"82499.5\",\n                             '18'= \"87499.5\",\n                             '19'= \"92499.5\",\n                             '20'= \"97499.5\",\n                             '21'= \"112499.5\",\n                             '22'= \"137499.5\",\n                             '23'= \"162499.5\",\n                             '24'= \"187499.5\",\n                             '25'= \"224999.5\",\n                             '26'= \"500000\"))\n\nanes$inc &lt;- as.numeric(as.character(anes$income_fac))\n\n# save data set for use in video\n\nwrite.csv(anes, \"anes_week5.csv\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html#video-and-rscript-1",
    "href": "08-PO91Q.html#video-and-rscript-1",
    "title": "Week 8",
    "section": "Video and RScript",
    "text": "Video and RScript\nYou can find the video introducing you to this week’s method by way of a worked example below. You can also access the code I am typing up in the video in the “Code for Data Analysis” section. I would encourage you to type it yourself, though, as then code tends to better sink into the depths of your brain 😉\n\n\n\nCode for Data Analysis\n\n\n######################################\n# MMM - Week 5 - Model Fit\n######################################\n\n# Set WD\nsetwd()\n\n# Load packages\n\nlibrary(tidyverse)\n\n# Load data set\n\nanes &lt;- read_csv(\"anes_week5.csv\")\n\n# Visualisation\n############################\n\nggplot(anes, aes(x = inc, y = fttrump1)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n# Regression\n############################\n\nmodel1 &lt;- lm(fttrump1 ~ inc, data = anes)\n\nmodel1\n\nsummary(model1)\n\n\n\n\nModel Fit for Regression in R",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html#data-prep-2",
    "href": "08-PO91Q.html#data-prep-2",
    "title": "Week 8",
    "section": "Data Prep",
    "text": "Data Prep\nPlace the ANES data in a folder which you will be using as a working directory for this session. Open the “Code for Data Preparation” below, and copy this into an RScript. Remember to adjust the working directory with the setwd() command at the beginning. Then run the RScript and you will be ready to proceed to the video.\n\n\n\nCode for Data Preparation\n\n\n######################################\n# MMM - Week 7 - Data Preparation\n######################################\n\n\n# Set WD\nsetwd()\n\n# Load packages\n\nlibrary(tidyverse)\n\n# Load data set\n\nanes &lt;- read_csv(\"anes.csv\")\n\n# Get rid of missing values for variables used in analysis today\n\n## 999 is equivalent to NA, so needs to be recoded\nanes$fttrump1 &lt;- with(anes, replace(fttrump1, fttrump1 == 999, NA)) \nanes$income &lt;- with(anes, replace(income, income == 99, NA)) \n\nanes &lt;- filter(anes, \n               !is.na(fttrump1),\n               !is.na(age),\n               !is.na(income))\n\n# Turn income variable into a numerical variable with mid-points of each level\n\nanes$income &lt;- factor(anes$income)\ntable(anes$income)\nanes &lt;- anes %&gt;%\n  mutate(income_fac = recode(income,\n                             '1'= \"2500\",\n                             '2'= \"7499.5\",\n                             '3'= \"12499.5\",\n                             '4'= \"17499.5\",                       \n                             '5'= \"22499.5\",\n                             '6'= \"27499.5\",\n                             '7'= \"32499.5\",\n                             '8'= \"37499.5\",\n                             '9'= \"42499.5\",\n                             '10'= \"47499.5\",\n                             '11'= \"52499.5\",\n                             '12'= \"57499.5\",\n                             '13'= \"62499.5\",\n                             '14'= \"67499.5\",\n                             '15'= \"72499.5\",\n                             '16'= \"77499.5\",\n                             '17'= \"82499.5\",\n                             '18'= \"87499.5\",\n                             '19'= \"92499.5\",\n                             '20'= \"97499.5\",\n                             '21'= \"112499.5\",\n                             '22'= \"137499.5\",\n                             '23'= \"162499.5\",\n                             '24'= \"187499.5\",\n                             '25'= \"224999.5\",\n                             '26'= \"500000\"))\n\nanes$inc &lt;- as.numeric(as.character(anes$income_fac))\n\n# save data set for use in video\n\nwrite.csv(anes, \"anes_week7.csv\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html#video-and-rscript-2",
    "href": "08-PO91Q.html#video-and-rscript-2",
    "title": "Week 8",
    "section": "Video and RScript",
    "text": "Video and RScript\nYou can find the video introducing you to this week’s method by way of a worked example below. You can also access the code I am typing up in the video in the “Code for Data Analysis” section. I would encourage you to type it yourself, though, as then code tends to better sink into the depths of your brain 😉\n\n\n\nCode for Data Analysis\n\n\n######################################\n# MMM - Week 7 - Hypothesis Testing\n######################################\n\n# Set WD\nsetwd()\n\n# Load packages\n\nlibrary(tidyverse)\n\n# Load data set\n\nanes &lt;- read_csv(\"anes_week8.csv\")\n\n# Visualisation\n############################\n\nggplot(anes, aes(x = inc, y = fttrump1)) +\n  geom_point() +\n  geom_smooth(method = lm)\n\n\n# Regression\n############################\n\nmodel1 &lt;- lm(fttrump1 ~ inc, data = anes)\n\nmodel1\n\nsummary(model1)\n\n\n\n\nSignificance Test for Coefficients in R",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html#conceptual",
    "href": "08-PO91Q.html#conceptual",
    "title": "Week 8",
    "section": "Conceptual",
    "text": "Conceptual\nRecall that:\n\\[\\begin{equation}\\label{eq:beta1est}\n\\hat{\\beta_{0}} =  \\bar{y} - \\hat{\\beta_{1}}\\bar{x}\n\\end{equation}\\]\n   \n\\[\\begin{equation}\n\\hat{\\beta_{1}} = \\dfrac{\\Sigma_{i=1}^{N} (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\Sigma_{i=1}^{N} (x_{i} - \\bar{x})^{2}}\n\\end{equation}\\]\nNow consider the following data set:\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nage (x)\nincome (y)\n\n\n\n\n1\n22\n700\n\n\n2\n19\n650\n\n\n3\n56\n2300\n\n\n4\n45\n1900\n\n\n5\n37\n2000\n\n\n6\n23\n900\n\n\n7\n32\n1000\n\n\n8\n65\n2500\n\n\n9\n43\n1800\n\n\n10\n48\n1200\n\n\n\n\n\n\nTable 1: Regression Data Set\n\n\n\n\n\n\n \n\nAssuming a regression model of the type \\(y_{i}=\\beta_{0}+ \\beta_{1}x_{i}+\\epsilon_{i}\\), calculate the estimates for \\(\\beta_{0}\\) and \\(\\beta_{1}\\) by hand (yes, you have read that correctly). Use the Table below as a guide to the required intermediary calculations.\nSpecify the SRF and interpret the estimates of \\(\\beta_{0}\\) and \\(\\beta_{1}\\).\nDetermine whether the slope coefficient is significant at a 95% confidence level.\nCalculate the coefficient of determination, \\(R^{2}\\), with \\(\\hat{y}_{i}= -53.1 + 39.7 x_{i}\\).\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ni\nage (x)\nincome (y)\n\\(y-\\bar{y}\\)\n\\(x-\\bar{x}\\)\n\\((x-\\bar{x})^2\\)\n\\((x-\\bar{x})(y-\\bar{y})\\)\n\n\n\n\n1\n22\n700\n-795\n\n\n\n\n\n2\n19\n650\n-845\n\n\n\n\n\n3\n56\n2300\n805\n\n\n\n\n\n4\n45\n1900\n405\n\n\n\n\n\n5\n37\n2000\n505\n\n\n\n\n\n6\n23\n900\n-595\n\n\n\n\n\n7\n32\n1000\n-495\n\n\n\n\n\n8\n65\n2500\n1005\n\n\n\n\n\n9\n43\n1800\n305\n\n\n\n\n\n10\n48\n1200\n-295\n\n\n\n\n\n\n\n\n\nTable 2: Regression Data Set",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html#applied",
    "href": "08-PO91Q.html#applied",
    "title": "Week 8",
    "section": "Applied",
    "text": "Applied\nFor the applied exercises you will be using the data set london_exercises. We will analyse patterns of crime in London2. The dataset contains several variables for each of the London wards, describing, amongst other things, demographics of their population and the number of crimes committed in each of them in 2015. A full codebook is provided in the following Table:\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nlabel\nyear\n\n\n\n\nward\nWard name\nn/a\n\n\ninner\nBinary classification of whether the ward is inner or outer London (based on ONS classification)\nn/a\n\n\narea\nLand area (km2)\nn/a\n\n\npopulation\nPopulation 2015\n2015\n\n\nchildren\nNumber of people aged 0-15\n2015\n\n\nadults\nNumber of people aged 16-64\n2015\n\n\nelderly\nNumber of people aged 65+\n2015\n\n\nage\nMean age of population\n2013\n\n\neducation\nPercentage of population with level 4 qualifications and above\n2011\n\n\ncrime\nCrimes committed per 1,000 residents\n2015\n\n\nemployed\nNumber of people aged 16-64 in employment\n2015\n\n\nbenefits\nPercentage of population claiming work-related benefits\n2011\n\n\nmigration\nNet rate of worker-aged migration\n2012\n\n\nincome\nMedian household income (GBP)\n2013\n\n\nhouseprice\nMedian house price (GBP)\n2014\n\n\ncars\nAverage number of cars per household\n2011\n\n\nturnout\nTurnout at the 2012 mayoral election (%)\n2012\n\n\n\n\n\n\nTable 3: Codebook for london_exercises Data Set\n\n\n\n\n\n \nThe data are taken from London Data Store (2013).\n\nExamine the London data. What hypothesis could be tested using the variables contained in the dataset?\nCrime rate is defined as the number of crimes committed per 1,000 people in a given area.\n\nEvaluate the variable’s distribution using a histogram and summary statistics.\nWhich wards had the most crimes committed in them? Find the names of wards in which the crime rate was higher than the 99th percentile of the variable’s distribution.\n\nLondon is divided into 32 Boroughs and the City of London. Examine the variation in crime on Borough level.\n\nCalculate crime rate for each of the Boroughs.\nPresent the Borough-level crime rates using a table and a bar chart, with Borough order descending by crime rate.\n\nUnemployment rate, defined as the ratio of people in full time employment to population of working age is often said to be related to crime.\n\nGenerate an unemployment rate variable for each of the wards.\nExamine the distribution of the unemployment rate in a similar manner as in Exercise 2.\nCreate a scatter plot of the relationship between unemployment and crime. Make sure to choose the right axis for each of the variables and to label the axes correctly. Interpret the results.\nCalculate the correlation coefficient between these two variables and interpret its value.\n\nEstimate a regression model for the relationship between unemployment rate and crime rate.\n\nInterpret the following:\n\nThe intercept and its significance.\nThe slope and its significance.\nThe distribution of residuals. (Use a histogram).\nThe \\(R^2\\) statistic.\n\nAdd the regression line to the scatter plot from Exercise 4.\nCalculate the 95% CI for the coefficient and the slope of the model.\nUsing the model, predict the value of crime rate for an hypothetical ward with unemployment rate of \\(0.4\\).\n\nIt can be hypothesised that a ward with higher median household income will have a lower rate of crime compared to a ward with lower median household income.\n\nRun a regression model testing this hypothesis.\nInterpret the coefficients, their significance levels, and the \\(R^2\\) of the model.\nProduce a scatter plot and add the regression line.\nWhat conclusions can you draw from the model and plot with regards to the hypothesis?\n\nThe mean age of a ward can be said to influence the rate of crime in that ward.\n\nCome up with a hypothesis for the relationship between the mean age of a ward and the rate of crime in that ward. Briefly justify why you chose this hypothesis.\nRun a regression model testing this hypothesis and interpret the results of the model. What does this interpretation suggest about your hypothesis?\n\nIt can be hypothesised that a ward with a larger number of benefit recipients will have a higher rate of crime compared to a ward with fewer benefit recipients.%8\n\nExplain some reasons why this might be the case.\nRun a regression model testing this hypothesis and interpret the results of the model. What does this interpretation suggest about your hypothesis?\nProduce a scatter plot and add the regression line.\nCompare the regression model with the models from Exercises 6 and 7. Which is better able to explain the crime rate of a ward and why?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html#load-packages-and-data",
    "href": "08-PO91Q.html#load-packages-and-data",
    "title": "Week 8",
    "section": "Load Packages and Data",
    "text": "Load Packages and Data\nBefore starting, we need to load libraries and install packages if not already installed. In these exercises we will be using the following packages:\n\nhaven\nggplot2\nmodelsummary\n\nWe will be using the london.csv data set from the lecture, but with a different independent variable this time. These will be of particular interest:\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nlabel\nyear\n\n\n\n\nconst\nParliamentary constituency\nn/a\n\n\ngcse\nAn average score based on a pupil’s best eight grades in a group of GCSEs. The maximum a pupil can achieve is 90 points.\n2019\n\n\neth_min\nPercent of population composed by ethnic minorities\n2011\n\n\nidaci\nThe Income Deprivation Affecting Children Index rank - how it compares to other constituencies\n2015/16\n\n\nincome\nMean income by constituency\n2017/18\n\n\n\n\n\n\nTable 4: london Codebook\n\n\n\n\n\n \nData are taken from House of Commons Library (n.d.), GOV.UK (2013), and London Data Store (2010). Set your working directory and load the data.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html#inspect-your-data",
    "href": "08-PO91Q.html#inspect-your-data",
    "title": "Week 8",
    "section": "Inspect your data",
    "text": "Inspect your data\nHere you can use several basic functions. The dataset does not contain too many variables, so you can start by using names(), str(), etc.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html#preliminary-analysis",
    "href": "08-PO91Q.html#preliminary-analysis",
    "title": "Week 8",
    "section": "Preliminary Analysis",
    "text": "Preliminary Analysis\nLet’s say we want to look at the relationship between income deprivation affecting children and GCSE scores. The two variables are, respectively, idaci and gcse.\nNow, formulate the working (alternative) and the null hypothesis. Write them down.\nH\\(\\pmb{_0}\\):\nH\\(\\pmb{_1}\\):\nWhich is your dependent variable?\nRun a frequency table on the idaci variable. Does this distribution make sense? Why/why not?\nDo the same for the other variable. And guess what is the level of measurement.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html#visualisation",
    "href": "08-PO91Q.html#visualisation",
    "title": "Week 8",
    "section": "Visualisation",
    "text": "Visualisation\nLet’s start with the visualisation of the relationship between the two variables. What is the best way to visualise the relationship considering the level of measurement of our variables?\nHint: Probably a scatterplot, right? So, use a scatterplot to visualise the relationship and add the regression line.\nYou can use ggplot, but also the base R plot() function.\nImprove the graph by:\n\nAdding a regression line.\nAdding up a relevant title, also possibly a subtitle.\nAdding axes labels and making them readable.\n\n\n\n\n\n\n\nFigure 1: Impact of Deprivation on GCSE Scores",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html#visualisation-2.0",
    "href": "08-PO91Q.html#visualisation-2.0",
    "title": "Week 8",
    "section": "Visualisation 2.0",
    "text": "Visualisation 2.0\nNow, draw a vertical and horizontal line corresponding to the mean of your variables using geom_hline and geom_vline. You can thus check if the regression line passes through the mean of X and Y. (see: https://www.rdocumentation.org/packages/ggplot2/versions/0.9.1/topics/geom_hline).\nYou can improve the scatterplot using a series of arguments (e.g., alpha) in the geom_point() function in ggplot. Try to improve the Aesthetics of the scatterplot playing with alpha, for instance. (see: https: //www.rdocumentation.org/packages/ggplot2/versions/3.4.0/topics/geom_point).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html#saving-the-scatterplot",
    "href": "08-PO91Q.html#saving-the-scatterplot",
    "title": "Week 8",
    "section": "Saving the Scatterplot",
    "text": "Saving the Scatterplot\nYou can save a graph as .png, .JPG (even .pdf) that you can then import in a word document. Although there are many way to use your R output, saving a graph might be sometimes useful.\nUse the function ggsave() to save your scatterplot. Again, there are tons of examples online, google it.\nHint: You first need to store the graph in an object.\nHint 2: The file will end up in your working directory.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html#regression-analysis-yes-finally",
    "href": "08-PO91Q.html#regression-analysis-yes-finally",
    "title": "Week 8",
    "section": "Regression Analysis (yes, finally)",
    "text": "Regression Analysis (yes, finally)\nNow we can finally run a linear regression with gcse as the outcome variable and idaci as the predictor using the lm() function. Store the results in an object called model and visualise the regression output using summary().\n\n# Store the results in an object called model #\nmodel&lt;-lm(gcse ~ idaci, london)\n# Visualise the regression output using summary() #\nsummary(model)\n\n\nCall:\nlm(formula = gcse ~ idaci, data = london)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4921 -2.5321 -0.1722  2.7077  7.7815 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 45.852958   0.657251  69.765  &lt; 2e-16 ***\nidaci        0.019765   0.002894   6.831  2.4e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.317 on 71 degrees of freedom\nMultiple R-squared:  0.3965,    Adjusted R-squared:  0.388 \nF-statistic: 46.66 on 1 and 71 DF,  p-value: 2.398e-09\n\n\nYou can also extract specific blocks of the output table. One way of doing it is to use the brackets [] after the summary() function. For example summary()[8]. Try to extract the block of Coefficients from the table, like this:\n\n\n$coefficients\n              Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 45.8529580 0.65725150 69.764707 3.767825e-67\nidaci        0.0197651 0.00289364  6.830532 2.397995e-09",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html#interpretation",
    "href": "08-PO91Q.html#interpretation",
    "title": "Week 8",
    "section": "Interpretation",
    "text": "Interpretation\nInterpret the results, starting with model evaluation.\n\nIs the p-value of the F-statistics statistically significant? We will be discussing this in our following lectures.\nHow much variation in the outcome variable does the model explain? What does this tell us about the model?\nWhat’s the value of the slope? What does it mean?\nWhat’s the value of the intercept? How do we interpret it? Is it statistically significant? What does it mean in practice?\nInterpret the results (in plain language) referring to the hypothesis you formulated above.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html#exporting-the-results",
    "href": "08-PO91Q.html#exporting-the-results",
    "title": "Week 8",
    "section": "Exporting the Results",
    "text": "Exporting the Results\nAs for the graphs, you can also export and save the results of the regression model in a Word table. To do that you can use the ‘modelsummary’ package.\nJust like writing a shopping list, we start by creating a list of models for which we want modelsummary to produce a table. You can merely list the names of the objects in which the models are stored, or you can give them specific names which will appear as column titles in the table. This addition is by no means a must, but sometimes you might wish to give models a particular name, for example if you have used different methods of estimation, different components of a theory, etc. In the table we are creating here, I wish to distinguish between bivariate and multivariate models. By default, modelsummary just numbers models in ascending order.\n\nmodels &lt;- list(\n  \"Bivariate\"    = model\n)\n\nThis simple step is already sufficient to produce – an admittedly somewhat crude – results table. All we have to do is to load the modelsummary package and to use the previously defined list of models as the argument of the modelsummary() function:\n\nlibrary(modelsummary)\nmodelsummary(models)\n\n\nExecute these code chunks as we go through this Section, so you can see the alterations we make to the table in real-time.\n\nThis table is a vast improvement on the raw R output you would receive when calling summary(model1). But it is far from finished. One characteristic which is conspicuously absent is an assessment of statistical significance. In model summaries of this kind this information is usually provided in the form of asterisks or other symbols next to the respective coefficient. We can add these simply by adding the option stars=TRUE to the code.\n\nmodelsummary(models,\n             stars = TRUE)\n\nNext up is the modification of the stub. The stub is the leftmost column in which you name the indicators for which coefficients will be presented.In the stub, “[a]bbreviate nothing. And never ever ever use computer variable names to stand for concepts. These are personal code words that convey no meaning to readers.” (Stimson, n.d., p. 10) Following this advice, let us add proper labels to the independent variables.\nTo let modelsummary know how to replace each variable name, we create a so-called coefficient map. This is really just a character vector which follows a “before-after” logic in each of its rows. For example 'age'='Age' replaces the variable name age with the label Age. We need to do this for all the variables we used in our models. I am storing this in a vecor called cm which stands for coefficient map.\n\ncm &lt;- c('idaci'    = 'Income Deprivation Affecting Children Index',\n        '(Intercept)' = 'Intercept')\n\nThe next step is easy, as we only need to feed this coefficient map into the modelsummary code with the option coef_map=cm:\n\nmodelsummary(models,\n             stars = TRUE,\n             coef_map = cm)\n\nThis is a personal thing, but I also like to show the label of the dependent variable in results tables. They just feel incomplete to me without this information. There is no default to achieve this in modelsummary and so we need to use a little trick.\nWhat I want to add in the first row, but only in the second columm, the text . This requires us to alter the style of the table slightly. modelsummary uses another package called tinytable to style the output. We load the package with library(tinytable) and instruct R to place the text where we want it.\n\nlibrary(tinytable)\nmodelsummary(models,\n             stars = TRUE,\n             coef_map = cm)|&gt;\n  group_tt(j = list(\"Dependent Variable: GCSE Score\" = 2))\n\nLastly, let us tackle the model fit statistics. By default, modelsummary prints a whole festival of these into the bottom section of the table, but I want to concentrate only on R\\(^2\\).\n\nmodelsummary(models,\n             stars = TRUE,\n             coef_map = cm,\n             gof_omit = 'DF|Deviance|Log.Lik|F|AIC|BIC|RMSE')|&gt;\n  group_tt(j = list(\"Dependent Variable: GCSE Score\" = 2))\n\nThis should produce Table 5.\n\n\n\n\n\n\n\n    \n\n    \n    \n      \n        \n\n \nDependent Variable:GCSE Score\n\n        \n              \n                 \n                Bivariate\n              \n        \n        + p &lt; 0.1, * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  Income Deprivation Affecting Children Index\n                  0.020***\n                \n                \n                  \n                  (0.003)\n                \n                \n                  Intercept\n                  45.853***\n                \n                \n                  \n                  (0.657)\n                \n        \n      \n    \n\n\n\n\nTable 5: Regression Models\n\n\n\n\nYou can export your tables for use in MS Word, or Apple Pages. But you will not be able to export the full table, only up to the point at which you are specifying the options for tinytable with |&gt;. This is probably as good a reason as any to start working with Markdown or LaTeXBut if you are happy with this caveat, the following code will render the table in an MS Word document (output=\"table.docx\") that is placed in your working directory.\n\nmodelsummary(models,\n             stars = TRUE,\n             coef_map = cm,\n             gof_omit = 'AIC|BIC|Log.Lik|F|RMSE',\n             output = \"table.docx\")",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html#comparing-models",
    "href": "08-PO91Q.html#comparing-models",
    "title": "Week 8",
    "section": "Comparing Models",
    "text": "Comparing Models\nYou can now run another regression model with . Compare your original model with the new one. How do you know which independent variable is doing a better job in explaining your dependent variable?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html#causality",
    "href": "08-PO91Q.html#causality",
    "title": "Week 8",
    "section": "Causality",
    "text": "Causality\nConsider the causality framework of Week 5. To what extent has this case study established causality? What is missing from the framework?",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html#solutions",
    "href": "08-PO91Q.html#solutions",
    "title": "Week 8",
    "section": "Solutions",
    "text": "Solutions\nYou can find the Solutions in the Downloads Section.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "08-PO91Q.html#footnotes",
    "href": "08-PO91Q.html#footnotes",
    "title": "Week 8",
    "section": "",
    "text": "Some of the content of this worksheet is taken from Reiche (forthcoming).↩︎\nThese exercises are taken from Chapter 9 in Reiche (forthcoming).↩︎\nThe structure of this study was originally designed by Oleksiy Bondarenko. I have slightly altered it in subsequent years.↩︎",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 8</span>"
    ]
  },
  {
    "objectID": "09-PO91Q.html",
    "href": "09-PO91Q.html",
    "title": "Week 9",
    "section": "",
    "text": "Methods, Methods, Methods\nThis week we are extending our bivariate regression model to approximate the real world in which nothing is mono-causal. We call this multiple regression.\nJust as last week we will be working with the American National Election Studies (ANES), and to be more precise with the pilot survey conducted before the 2020 presidential election. If you haven’t already done so, you will have to register with ANES in order to download the data set. To do so, please follow this link.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "09-PO91Q.html#data-prep",
    "href": "09-PO91Q.html#data-prep",
    "title": "Week 9",
    "section": "Data Prep",
    "text": "Data Prep\nPlace the ANES data in a folder which you will be using as a working directory for this session. Open the “Code for Data Preparation” below, and copy this into an RScript. Remember to adjust the working directory with the setwd() command at the beginning. Then run the RScript and you will be ready to proceed to the video.\n\n\n\nCode for Data Preparation\n\n\n######################################\n# MMM - Week 8 - Data Preparation\n######################################\n\n\n# Set WD\nsetwd()\n\n# Load packages\n\nlibrary(tidyverse)\n\n# Load data set\n\nanes &lt;- read_csv(\"anes.csv\")\n\n# Get rid of missing values for variables used in analysis today\n\n## 999 is equivalent to NA, so needs to be recoded\nanes$fttrump1 &lt;- with(anes, replace(fttrump1, fttrump1 == 999, NA)) \nanes$income &lt;- with(anes, replace(income, income == 99, NA)) \n\nanes &lt;- filter(anes, \n               !is.na(fttrump1),\n               !is.na(age),\n               !is.na(income))\n\n# Turn income variable into a numerical variable with mid-points of each level\n\nanes$income &lt;- factor(anes$income)\ntable(anes$income)\nanes &lt;- anes %&gt;%\n  mutate(income_fac = recode(income,\n                             '1'= \"2500\",\n                             '2'= \"7499.5\",\n                             '3'= \"12499.5\",\n                             '4'= \"17499.5\",                       \n                             '5'= \"22499.5\",\n                             '6'= \"27499.5\",\n                             '7'= \"32499.5\",\n                             '8'= \"37499.5\",\n                             '9'= \"42499.5\",\n                             '10'= \"47499.5\",\n                             '11'= \"52499.5\",\n                             '12'= \"57499.5\",\n                             '13'= \"62499.5\",\n                             '14'= \"67499.5\",\n                             '15'= \"72499.5\",\n                             '16'= \"77499.5\",\n                             '17'= \"82499.5\",\n                             '18'= \"87499.5\",\n                             '19'= \"92499.5\",\n                             '20'= \"97499.5\",\n                             '21'= \"112499.5\",\n                             '22'= \"137499.5\",\n                             '23'= \"162499.5\",\n                             '24'= \"187499.5\",\n                             '25'= \"224999.5\",\n                             '26'= \"500000\"))\n\nanes$inc &lt;- as.numeric(as.character(anes$income_fac))\n\n# save data set for use in video\n\nwrite.csv(anes, \"anes_week8.csv\")",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "09-PO91Q.html#video-and-rscript",
    "href": "09-PO91Q.html#video-and-rscript",
    "title": "Week 9",
    "section": "Video and RScript",
    "text": "Video and RScript\nYou can find the video introducing you to this week’s method by way of a worked example below. You can also access the code I am typing up in the video in the “Code for Data Analysis” section. I would encourage you to type it yourself, though, as then code tends to better sink into the depths of your brain 😉\n\n\n\nCode for Data Analysis\n\n\n######################################\n# MMM - Week 8 - Multiple Regression\n######################################\n\n# Set WD\nsetwd()\n\n# Load packages\n\nlibrary(tidyverse)\n\n# Load data set\n\nanes &lt;- read_csv(\"anes_week9.csv\")\n\n\n# Bivariate Regression\n############################\n\nmodel1 &lt;- lm(fttrump1 ~ inc, data = anes)\n\nsummary(model1)\n\nmodel2 &lt;- lm(fttrump1 ~ age, data = anes)\n\nsummary(model2)\n\n# Multiple Regression\n############################\n\nmodel3 &lt;- lm(fttrump1 ~ inc + age, data = anes)\n\nsummary(model3)\n\n\n\n\nMultiple Regression in R",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "09-PO91Q.html#multiple-regression-in-r-guided-example",
    "href": "09-PO91Q.html#multiple-regression-in-r-guided-example",
    "title": "Week 9",
    "section": "Multiple Regression in R – Guided Example",
    "text": "Multiple Regression in R – Guided Example\n\nDownload the WDI_PO91Q.csv data set. Data are taken from World Bank (n.d.), Miller et al. (2022), and Marshall & Gurr (2020).\nPut it into an appropriate working directory for this seminar and create a dedicated RScript and save it into the same working directory.\n\n\nsetwd(\"~/Warwick/Modules/PO91Q/Seminars/Week 8\")\n\n\nImport the data set into R:\n\n\nwdi &lt;- read.csv(\"WDI_PO91Q.csv\")\n\n\nOnce again, here is the overview of the variables available and their respective label in Table 1:\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nlabel\n\n\n\n\nCountry Name\nCountry Name\n\n\nCountry Code\nCountry Code\n\n\nyear\nyear\n\n\ndemocracy\n0 = Autocracy, 1 = Dictatorship (Boix et al., 2018)\n\n\ngdppc\nGDP per capita (constant 2010 US$)\n\n\ngdpgrowth\nAbsolute growth of per capita GDP to previous year (constant 2010 US Dollars)\n\n\nenrl_gross\nSchool enrollment, primary (% gross)\n\n\nenrl_net\nSchool enrollment, primary (% net)\n\n\nagri\nEmployment in agriculture (% of total employment) (modeled ILO estimate)\n\n\nslums\nPopulation living in slums (% of urban population)\n\n\ntelephone\nFixed telephone subscriptions (per 100 people)\n\n\ninternet\nIndividuals using the Internet (% of population)\n\n\ntax\nTax revenue (% of GDP)\n\n\nelectricity\nAccess to electricity (% of population)\n\n\nmobile\nMobile cellular subscriptions (per 100 people)\n\n\nservice\nServices, value added (% of GDP)\n\n\noil\nOil rents (% of GDP)\n\n\nnatural\nTotal natural resources rents (% of GDP)\n\n\nliteracy\nLiteracy rate, adult total (% of people ages 15 and above)\n\n\nprim_compl\nPrimary completion rate, total (% of relevant age group)\n\n\ninfant\nMortality rate, infant (per 1,000 live births)\n\n\nhosp\nHospital beds (per 1,000 people)\n\n\ntub\nIncidence of tuberculosis (per 100,000 people)\n\n\nhealth_ex\nCurrent health expenditure (% of GDP)\n\n\nineq\nIncome share held by lowest 10%\n\n\nunemploy\nUnemployment, total (% of total labor force) (modeled ILO estimate)\n\n\nlifeexp\nLife expectancy at birth, total (years)\n\n\nurban\nUrban population (% of total population)\n\n\npolity5\nCombined Polity V score\n\n\n\n\n\n\nTable 1: WDI Codebook\n\n\n\n\n\n \n\nLet’s take the example from Week 7 back up. First re-run the regression I used back then.\n\n\nwdi_life &lt;- lm(gdppc ~ lifeexp, data = wdi)\nsummary(wdi_life)\n\n\nCall:\nlm(formula = gdppc ~ lifeexp, data = wdi)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-18457  -9877  -4187   4963  78242 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -94306.8    10406.7  -9.062 3.33e-16 ***\nlifeexp       1503.2      144.4  10.412  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14690 on 167 degrees of freedom\n  (26 observations deleted due to missingness)\nMultiple R-squared:  0.3936,    Adjusted R-squared:   0.39 \nF-statistic: 108.4 on 1 and 167 DF,  p-value: &lt; 2.2e-16\n\n\n\nInterpret the coefficients.\n\n\nInterpreting a coefficient\nThe order in which to interpret a coefficient is as follows:\n\nIs it significant? If not, all you can say is that there is no influence. There is insufficient evidence to support the alternative hypothesis.\nIf it is significant, you can interpret its size and direction according to the statistical model (for example, slope coefficient vs. partial slope coefficient).\nWhat does the coefficient mean for the hypothesis? Look at the direction. Is the direction as predicted by the hypothesis? Then you have evidence to support the hypothesis. If the direction is inverse, then you have falsified the hypothesis, even though you have a significant coefficient.\n\n\n\nNow we use a different regressor, say “Urban population (% of total)”.\nSpecify the null and the alternative hypotheses.\n\n\nwdi_urban &lt;- lm(gdppc ~ urban, data = wdi)\nsummary(wdi_urban)\n\n\nCall:\nlm(formula = gdppc ~ urban, data = wdi)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-28588 -10681  -3110   6863 152303 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -16983.43    3872.34  -4.386 1.98e-05 ***\nurban          542.03      61.74   8.779 1.42e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19120 on 176 degrees of freedom\n  (17 observations deleted due to missingness)\nMultiple R-squared:  0.3045,    Adjusted R-squared:  0.3006 \nF-statistic: 77.07 on 1 and 176 DF,  p-value: 1.417e-15\n\n\n\nInterpret the coefficients.\nWhat does this mean for the hypotheses?\nIf we want to assess the influence of both independent variables together, we type:\n\n\nwdi_joint &lt;- lm(gdppc ~ lifeexp + urban, data = wdi)\nsummary(wdi_joint)\n\n\nCall:\nlm(formula = gdppc ~ lifeexp + urban, data = wdi)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-22570  -8825  -2143   5594  74445 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -74786.14   10690.13  -6.996 6.17e-11 ***\nlifeexp       1012.17     172.70   5.861 2.41e-08 ***\nurban          273.74      59.13   4.629 7.37e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13860 on 166 degrees of freedom\n  (26 observations deleted due to missingness)\nMultiple R-squared:  0.463, Adjusted R-squared:  0.4565 \nF-statistic: 71.55 on 2 and 166 DF,  p-value: &lt; 2.2e-16\n\n\n\nInterpret the coefficients.\n\n\nBear in mind for your interpretation that these are partial slope coefficients!\n\n\nBecause I am nice, I am producing an overview2 of all three regressions in Table 2:\n\n   \n\n\n\n\n\n\n\n\n    \n\n    \n    \n      \n        \n\n \nDependent Variable:per capita GDP\n\n        \n              \n                 \n                Bivariate(1)\n                Bivariate(2)\n                Multiple(3)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  Life Expectancy (years)\n                  1503.211***\n                  \n                  1012.168***\n                \n                \n                  \n                  (144.371)\n                  \n                  (172.696)\n                \n                \n                  Urbanisation\n                  \n                  542.033***\n                  273.735***\n                \n                \n                  \n                  \n                  (61.743)\n                  (59.134)\n                \n                \n                  Constant\n                  -94306.813***\n                  -16983.431***\n                  -74786.145***\n                \n                \n                  \n                  (10406.727)\n                  (3872.336)\n                  (10690.128)\n                \n        \n      \n    \n\n\n\n\nTable 2: Regression Models 1\n\n\n\n\n\n   \n\nDrawing on what you have learned about the modelsummary package in the additional exercises last week, replicate this table.\n\n\nHow have the slope coefficients changed? Why?\nWhich model explains the level of GDP best? Why?\nSpecify the SRF for Model 3, paying special attention to notation.\nNow assume, we want to know whether education has a bearing on the level of GDP. We call:\n\n\nwdi_lit &lt;- lm(gdppc ~ literacy, data = wdi)\n\nand also add it to the joint model:\n\nwdi_joint1 &lt;- lm(gdppc ~ lifeexp + urban + literacy, data = wdi)\n\nThis should lead to these results:\n   \n\n\n\n\n\n\n\n\n    \n\n    \n    \n      \n        \n\n \nDependent Variable:per capita GDP\n\n        \n              \n                 \n                (1)\n                (2)\n                (3)\n                (4)\n                (5)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  Life Expectancy (years)\n                  1503.211***\n                  \n                  1012.168***\n                  \n                  973.985*\n                \n                \n                  \n                  (144.371)\n                  \n                  (172.696)\n                  \n                  (411.375)\n                \n                \n                  Urbanisation\n                  \n                  542.033***\n                  273.735***\n                  \n                  227.126*\n                \n                \n                  \n                  \n                  (61.743)\n                  (59.134)\n                  \n                  (83.427)\n                \n                \n                  Literacy\n                  \n                  \n                  \n                  258.407*\n                  -221.731\n                \n                \n                  \n                  \n                  \n                  \n                  (97.381)\n                  (142.455)\n                \n                \n                  Constant\n                  -94306.813***\n                  -16983.431***\n                  -74786.145***\n                  -12967.003\n                  -55246.837**\n                \n                \n                  \n                  (10406.727)\n                  (3872.336)\n                  (10690.128)\n                  (8529.810)\n                  (19311.126)\n                \n        \n      \n    \n\n\n\n\nTable 3: Regression Models 2\n\n\n\n\n\n   \n\nIn Model 5 the coefficient for literacy has turned insignificant. Reproduce the results in Table 4 to find out which variable takes away the significance.\n\n   \n\n\n\n\n\n\n\n\n    \n\n    \n    \n      \n        \n\n \nDependent Variable:per capita GDP\n\n        \n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  Life Expectancy (years)\n                  \n                  1483.785***\n                  \n                \n                \n                  \n                  \n                  (397.567)\n                  \n                \n                \n                  Urbanisation\n                  \n                  \n                  315.375***\n                \n                \n                  \n                  \n                  \n                  (77.633)\n                \n                \n                  Literacy\n                  258.407*\n                  -223.305\n                  28.390\n                \n                \n                  \n                  (97.381)\n                  (154.620)\n                  (99.706)\n                \n                \n                  Constant\n                  -12967.003\n                  -77851.339***\n                  -12399.608+\n                \n                \n                  \n                  (8529.810)\n                  (18924.058)\n                  (7189.937)\n                \n        \n      \n    \n\n\n\n\nTable 4: Regression Models 3\n\n\n\n\n\n   \n\nWhat can we conclude from this investigation?\nDoes the variable infant have the same effect? What do you conclude from this?\n\n\nWhich measurement explains GDP better, life or infant?\n\n   \n\n\n\n\n\n\n\n\n    \n\n    \n    \n      \n        \n\n \nDependent Variable:per capita GDP\n\n        \n              \n                 \n                life\n                infant\n              \n        \n        + p &lt; 0.1, * p &lt; 0.1, ** p &lt; 0.05, *** p &lt; 0.01\n        \n                \n                  Life Expectancy (years)\n                  1503.211***\n                  \n                \n                \n                  \n                  (144.371)\n                  \n                \n                \n                  Infant Mortality (per 1,000 live births)\n                  \n                  -524.111***\n                \n                \n                  \n                  \n                  (73.885)\n                \n                \n                  Constant\n                  -94306.813***\n                  26467.135***\n                \n                \n                  \n                  (10406.727)\n                  (2257.385)\n                \n        \n      \n    \n\n\n\n\nTable 5: Regression Models 4",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "09-PO91Q.html#multiple-regression-in-r-independent-analysis",
    "href": "09-PO91Q.html#multiple-regression-in-r-independent-analysis",
    "title": "Week 9",
    "section": "Multiple Regression in R – Independent Analysis",
    "text": "Multiple Regression in R – Independent Analysis\n\nBefore you start with these, please pause and let Flo know that you are done. We will compare notes on your answers up to this point, to make sure that you are on the right track for the independent exercises.\n\n\nUse the wdi data frame. Set polity5 as the dependent variable, and choose three sensible variables which you believe could influence democracy. Note that the Polity V Score codes regimes from -10 (indicating perfect autocracy) to +10 (indicating perfect democracy).\nState the null- and alternative hypotheses for each of the independent variables chosen.\nPlot two of the bivariate models in a scatter plot (black points) with fitted regression line (in red). Use base R, or ggplot. Does the direction of influence agree with your hypotheses?\nRun all possible regression models, using a bottom-up strategy. Construct a stargazer table to present the results.\nSpecify the Sample Regression Function (SRF) for a bivariate model (Model A), a multivariate model with two independent variables (Model B), and for the model using all three independent variables (Model C).\nInterpret the intercept and one of the slope coefficients in Models A, B, and C.\nInterpret the model fit measure for Models A, B, and C.\nWhich model explains democracy best? Why?.\nWhat do we conclude with respect to the hypotheses stated in Exercise 2 from this analysis?\nCollate a PowerPoint (Keynote) presentation with one slide for each of the preceding nine points. We will discuss this in Week 9.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "09-PO91Q.html#going-further",
    "href": "09-PO91Q.html#going-further",
    "title": "Week 9",
    "section": "Going Further",
    "text": "Going Further\n\nPlot the scatter plots for all models up to two independent variables. Add a line of best fit, or hyperplane of best fit as appropriate. Use the rockchalk package, and function plotPlane() for the 3D plots.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "09-PO91Q.html#solutions",
    "href": "09-PO91Q.html#solutions",
    "title": "Week 9",
    "section": "Solutions",
    "text": "Solutions\nYou can find the Solutions in the Downloads Section.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "09-PO91Q.html#footnotes",
    "href": "09-PO91Q.html#footnotes",
    "title": "Week 9",
    "section": "",
    "text": "All exercises are a reproduction from Reiche (forthcoming).↩︎\nThe following regression tables have been produced with the package modelsummary (Arel-Bundock, 2022).↩︎",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 9</span>"
    ]
  },
  {
    "objectID": "10-PO91Q.html",
    "href": "10-PO91Q.html",
    "title": "Week 10",
    "section": "",
    "text": "Methods, Methods, Methods\nWe have sort of covered how to create dummy variables already in Week 2, but for completeness’ sake here is an exploration on how to create them, include them in regression models, and how to interpret them (that all-important reference category…).\nJust as last week we will be working with the American National Election Studies (ANES), and to be more precise with the pilot survey conducted before the 2020 presidential election. If you haven’t already done so, you will have to register with ANES in order to download the data set. To do so, please follow this link.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 10</span>"
    ]
  },
  {
    "objectID": "10-PO91Q.html#data-prep",
    "href": "10-PO91Q.html#data-prep",
    "title": "Week 10",
    "section": "Data Prep",
    "text": "Data Prep\nPlace the ANES data in a folder which you will be using as a working directory for this session. Open the “Code for Data Preparation” below, and copy this into an RScript. Remember to adjust the working directory with the setwd() command at the beginning. Then run the RScript and you will be ready to proceed to the video.\n\n\n\nCode for Data Preparation\n\n\n######################################\n# MMM - Week 9 - Dummy Variables\n######################################\n\n\n# Set WD\nsetwd()\n\n# Load packages\n\nlibrary(tidyverse)\n\n# Load data set\n\nanes &lt;- read_csv(\"anes.csv\")\n\n# Get rid of missing values for variables used in analysis today\n\n## 999 is equivalent to NA, so needs to be recoded\nanes$fttrump1 &lt;- with(anes, replace(fttrump1, fttrump1 == 999, NA)) \nanes$income &lt;- with(anes, replace(income, income == 99, NA)) \n\nanes &lt;- filter(anes, \n               !is.na(fttrump1),\n               !is.na(income))\n\n# Label variable `sex`\nanes$sex &lt;- factor(anes$sex)\n\nanes &lt;- anes %&gt;% \n  mutate(sex=\n           recode(sex,\"1\"=\"Male\",\n                  \"2\"=\"Female\"))\n\nanes$sex &lt;- as.factor(anes$sex)\n\n\n# Turn income variable into a numerical variable with mid-points of each level\n\nanes$income &lt;- factor(anes$income)\n\nanes &lt;- anes %&gt;%\n  mutate(income_fac = recode(income,\n                             '1'= \"2500\",\n                             '2'= \"7499.5\",\n                             '3'= \"12499.5\",\n                             '4'= \"17499.5\",                       \n                             '5'= \"22499.5\",\n                             '6'= \"27499.5\",\n                             '7'= \"32499.5\",\n                             '8'= \"37499.5\",\n                             '9'= \"42499.5\",\n                             '10'= \"47499.5\",\n                             '11'= \"52499.5\",\n                             '12'= \"57499.5\",\n                             '13'= \"62499.5\",\n                             '14'= \"67499.5\",\n                             '15'= \"72499.5\",\n                             '16'= \"77499.5\",\n                             '17'= \"82499.5\",\n                             '18'= \"87499.5\",\n                             '19'= \"92499.5\",\n                             '20'= \"97499.5\",\n                             '21'= \"112499.5\",\n                             '22'= \"137499.5\",\n                             '23'= \"162499.5\",\n                             '24'= \"187499.5\",\n                             '25'= \"224999.5\",\n                             '26'= \"500000\"))\n\nanes$inc &lt;- as.numeric(as.character(anes$income_fac))\n\n# save data set for use in video\n\nwrite.csv(anes, \"anes_week9.csv\")",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 10</span>"
    ]
  },
  {
    "objectID": "10-PO91Q.html#video-and-rscript",
    "href": "10-PO91Q.html#video-and-rscript",
    "title": "Week 10",
    "section": "Video and RScript",
    "text": "Video and RScript\nYou can find the video introducing you to this week’s method by way of a worked example below. You can also access the code I am typing up in the video in the “Code for Data Analysis” section. I would encourage you to type it yourself, though, as then code tends to better sink into the depths of your brain 😉\n\n\n\nCode for Data Analysis\n\n\n######################################\n# MMM - Week 9 - Dummy Variables\n######################################\n\n# Set WD\nsetwd()\n\n# Load packages\n\nlibrary(tidyverse)\n\n# Load data set\n\nanes &lt;- read_csv(\"anes_week10.csv\")\nanes$sex &lt;- as.factor(anes$sex)\n\n# Bivariate Regression\n############################\n\nmodel1 &lt;- lm(fttrump1 ~ sex, data = anes)\n\nsummary(model1)\n\n\n\nanes &lt;- anes %&gt;%\n  mutate(sex = relevel(sex, ref = \"Male\"))\ntable(anes$sex)\n\n\n\nmodel2 &lt;- lm(fttrump1 ~ sex, data = anes)\n\nsummary(model2)\n\n# Multiple Regression\n############################\n\nmodel3 &lt;- lm(fttrump1 ~ inc + sex, data = anes)\n\nsummary(model3)\n\n\n\n\nDummy Variables in R",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 10</span>"
    ]
  },
  {
    "objectID": "10-PO91Q.html#data-prep-1",
    "href": "10-PO91Q.html#data-prep-1",
    "title": "Week 10",
    "section": "Data Prep",
    "text": "Data Prep\nEach respondent was randomly assigned to a different module, indicated by split and only asked a subsection of the questions. The antisocx variable is part of module A and asks for a score from respondents on how much antisocial behaviour is in the neighbourhood.\n\nCreate a new data set for those who were chosen for the ‘A’ module. Call this crime.a.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 10</span>"
    ]
  },
  {
    "objectID": "10-PO91Q.html#working-with-regression-analysis",
    "href": "10-PO91Q.html#working-with-regression-analysis",
    "title": "Week 10",
    "section": "Working with Regression Analysis",
    "text": "Working with Regression Analysis\n\nPrevious research has suggested men perceive more antisocial behaviour in urban areas than rural areas.\n\nCreate a linear model using only male respondents from crime.a with the dependent variable antisocx and the independent variable rural2. Do your findings support previous research?\nTest the same model using only women. How do the two models differ?\nUsing interaction effects test whether being in a rural area has a larger effect on women’s perception of antisocial behaviour compared to men’s?\nHow would you test whether being female has an effect on the perception of higher antisocial behaviour in urban compared to rural areas?\n\nWe will now be using the full crime data frame. The wburgl variable asks respondents how worried they are about being burgled with answers ranging from “Very worried” to “Not at all worried” along with “Not applicable” and “Don’t know”.\n\nRecode those with “Not applicable” or “Don’t know” as NAs.\nUsing agegrp7 and wburgl as continuous variables, test the hypothesis that older people are more worried about being burgled.\nUsing a dummy variable test whether those over 65 have are more worried about burglary then those who are younger.\nUsing dummies test whether any of the age groups differ significantly from the youngest age group.\nWhat does the intercept in each of the three models represent?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 10</span>"
    ]
  },
  {
    "objectID": "10-PO91Q.html#transformation-of-variables",
    "href": "10-PO91Q.html#transformation-of-variables",
    "title": "Week 10",
    "section": "Transformation of Variables",
    "text": "Transformation of Variables\nThis Section uses the london_exercises.csv data set. You are already familiar with this data set from Week 7. The data are taken from London Data Store (2013).\n\nUnemployment rate, defined as the ratio of people in full time employment to population of working age is often said to be related to crime. Generate an unemployment rate variable for each of the wards.\nIt is theorised that unemployment is a driving factor behind crime rates.\n\nPlot a scatter graph with unemployment rate, crime rate, and the regression line that may be used to evaluate the theory. Describe the plot and the best fit line.\nPlot the graph again excluding wards with a crime rate greater than 500. Describe the plot and the best fit line.\nPlot another graph excluding wards with a crime rate of over 500 with the crime rate log transformed. Describe the plot and the best fit line.\nBuild both models. Interpret both including the effect size. Which model fits the data better?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 10</span>"
    ]
  },
  {
    "objectID": "10-PO91Q.html#going-further",
    "href": "10-PO91Q.html#going-further",
    "title": "Week 10",
    "section": "Going Further",
    "text": "Going Further\nIf you want to do a little more in R, then please return to the crime.csv data set again.\n\nThere are five variables which ask how worried the respondent is about being the victim of various crimes:\n\nCreate an additive variable called worry from these variables so that a score of 0 indicates the respondent answered all “Not at all worried” and a score of 15 indicates the respondent answered all “Very worried.” (Tip: Make sure to clean the variables for NAs).\nWhat are the mode, mean and median for worry?\nDescribe the variable educat3. How could it be modified to be used as a continuous variable?\nUsing educat3 as both a continuous and factor variable evaluate the statement “Worry about being a victim of crime is higher in those with lower education levels.”\nWhat are the \\(R^2\\) values for the two models calculated? Which is higher? Does this make that model better than the other?\nCalculate \\(97.5\\%\\) confidence intervals for the coefficients for those with GCSEs and those with Degrees. What does this tell you?",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 10</span>"
    ]
  },
  {
    "objectID": "11-Downloads.html",
    "href": "11-Downloads.html",
    "title": "Downloads",
    "section": "",
    "text": "Introduction\nYes, all solutions and all RScripts are available to you without silly time restrictions. The reason is that I have come to realise that I am operating a module in a university, and not in a kindergarten. Of course, you can download and look at the solutions before you have given the exercises a go yourself first. By all means, cheat. But I can’t promise you that you will learn very much. It’s your choice.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Downloads</span>"
    ]
  },
  {
    "objectID": "11-Downloads.html#documents",
    "href": "11-Downloads.html#documents",
    "title": "Downloads",
    "section": "Documents",
    "text": "Documents\n\nModule Handbook 2025/26\nPO91Q Bibliography\nStatistical Tables\nFormula Collection\nWorksheet Week 3 Solutions\nWorksheet Week 4 Solutions\nWorksheet Week 5 Solutions\nWorksheet Week 7 Solutions\nWorksheet Week 8 Solutions\nCase Study Week 8 Solutions",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Downloads</span>"
    ]
  },
  {
    "objectID": "11-Downloads.html#data-sets-in-alphabetical-order",
    "href": "11-Downloads.html#data-sets-in-alphabetical-order",
    "title": "Downloads",
    "section": "Data Sets – in alphabetical order",
    "text": "Data Sets – in alphabetical order\n\nEU.xlsx\nExample.xlsx\nks2\nlondon_exercises\nmortality\nWDI_PO91Q.csv",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Downloads</span>"
    ]
  },
  {
    "objectID": "11-Downloads.html#r-scripts",
    "href": "11-Downloads.html#r-scripts",
    "title": "Downloads",
    "section": "R Scripts",
    "text": "R Scripts\n\nWeek 2 Solutions\nWeek 3 Solutions\nWeek 4 Solutions\nWeek 7 Solutions\nWeek 8 Solutions",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Downloads</span>"
    ]
  },
  {
    "objectID": "12-Glossary.html",
    "href": "12-Glossary.html",
    "title": "Full Glossary",
    "section": "",
    "text": "Full Glossary\n\n\n\nUnless otherwise noted, the definitions are taken from Reiche (forthcoming).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nDescription\n\n\n\n\nAdjusted R-Squared\nThe coefficient of determination for multiple regression, and penalises the researcher for including more IVs\n\n\nalternative hypothesis\nThe alternative hypothesis, denoted as \\(H_1\\) or \\(H_a\\), on the other hand, proposes that there is an effect or a difference. Again, more formally, we can state that \\(\\mu \\ne \\mu_{0}\\)\n\n\nanalysis\nA detailed evaluation of data to discover their structure and relevant information to answer a research question\n\n\nasymmetry\nThe notion that while X causes Y, Y does not cause X. It is established with temporal priority, manipulated events, and/or the independence of causes (see Brady (2011))\n\n\nattribute\nA component or characteristic of a concept\n\n\nbackground concept\nThe broad constellation of meanings and understandings associated with the concept (Adcock & Collier, 2001, p. 531)\n\n\ncase\nUnit of analysis (rows in our dataset/table)\n\n\ncategorical\nDescribing the qualitative categories of a characteristic, for example different religions\n\n\ncausal\nIn order to establish a causal relationship, the following criteria must be met concurrently:  \n\n\ncausal loop\nY causes itself (e.g., health; happiness, wealth)\n\n\ncentral limit theorem\nIn random sampling with a large sample size – where n=30 is usually sufficient – the sampling distribution of the sample mean \\(\\bar{y}\\) will be approximately normally distributed, irrespective of the shape of the population distribution\n\n\ncoefficient\nA coefficient is a numerical expression which is multiplied with the value of a variable\n\n\ncoefficient of determination\nIndicates the proportion of the variation in the dependent variable which is explained through the independent variable. It is defined as \\(\\frac{\\text{Explained Sum of Squares}}{\\text{Total Sum of Squares}}\\)\n\n\nconcept\nAbstract ideas which form the building blocks of theories (Clark et al., 2021, p. 150)\n\n\nconceptualisation\nFormulating a systematized concept through reasoning about the background concept, in light of the goals of research (Adcock & Collier, 2001, p. 531)\n\n\nconditional distributions\nThe number of subjects observed at all combinations of possible outcomes for the two (or more) categorical variables in the crosstabulation\n\n\nconditional expectation function\nsee Population Regression Function (PRF)\n\n\nconfidence interval\nA confidence interval is an estimated range, based on a sample, that is likely to contain the true population parameter (such as the mean). If the sampling process were repeated many times, approximately \\((1 - \\alpha) \\cdot 100\\) % of the resulting intervals would contain the true parameter. The value of \\(\\alpha\\) determines the confidence level – for example, \\(\\alpha = 0.05\\) corresponds to a 95% confidence level, while \\(\\alpha = 0.01\\) corresponds to 99%\n\n\nconfidence level\nThe confidence level is the proportion of confidence intervals, constructed from repeated random samples of the same population using the same method, that are expected to contain the true population parameter. It is denoted by \\(1 - \\alpha\\), where \\(\\alpha\\) is the significance level. For example, a 95% confidence level implies that 95% of such intervals would contain the true parameter in the long run.\n\n\nconstant\nA variable which does not vary\n\n\ncontinuous\nCan assume any value within defined measurement boundaries\n\n\ncorrelation\nThe statistical dependence of two random variables which is determined by pairwise comparison of values\n\n\ncorrelation\nAssesses the statistical dependence of two random variables by a pairwise comparison of values\n\n\ncovariance\nA measure which assesses the degree to which the values of two variables (\\(X_i\\) and \\(X_j\\)) vary together.\n\n\ncritical value\nThe critical value is a threshold that determines the boundary for rejecting the null hypothesis (H\\(_0\\)) in a hypothesis test. It is a point on the probability distribution of the test statistic beyond which the null hypothesis is rejected. The critical value is chosen based on the significance level (\\(\\alpha\\)) of the test, which represents the probability of making a Type I error (i.e., rejecting a true null hypothesis)\n\n\ncross-sectional data\nLook at different units (or cross-sections) \\(i\\) at a single point in time\n\n\ncrosstabulation\nCrosstabulations, or contingency tables, display the number of subjects observed at all combinations of possible outcomes for two (or more) categorical variables in matrix format\n\n\ndata\nDerives from the Latin which means . For our purposes it is a collection of numbers (or quantities) for the purpose of analysis\n\n\ndata set\nA collection of numerical values for individual observations, separated into distinctive variables\n\n\ndegrees of freedom\nDegrees of freedom express constraints on our estimation process by specifying how many values in the calculation are free to vary\n\n\ndependent variable\nIs dependent through some statistical or stochastic process on the value of an independent variable\n\n\ndescriptive statistics\nSummarise information about the centre and variability of a variable\n\n\ndeviation\nThe deviation \\(d\\) of an observation \\(y_{i}\\) from the sample mean \\(\\bar{y}\\) is the difference between them: \\(d=y_{i}-\\bar{y}\\)\n\n\ndichotomous\nCan only assume two mutually exclusive, but internally homogeneous qualitative categories\n\n\ndiscrete\nThe result of a counting process\n\n\ndistribution\nRefers to the display of the values a variable can assume, together with their respective absolute or relative frequency\n\n\ndummy variable\nDummy variables are dichotomous, categorical variables which indicate the presence or the absence of a characteristic\n\n\neffect size\nThe magnitude of the difference or relationship. Larger effects are easier to spot and usually more practically important\n\n\nerror term\nThe error term quantifies the distance between each observation and the corresponding point on the regression line. The terms are denoted as \\(\\epsilon_{i}\\)\n\n\nExplained Sum of Squares\nThe variation that can be explained by the regression\n\n\ngeneralisability\nThe ability to apply the findings made on the basis of a representative sample to the population\n\n\nhistogram\nDisplays through rectangles the frequency with which the values of a continuous variable occur in specific ranges\n\n\nhypothesis\nIn statistics, a hypothesis is a formal statement about a population parameter or relationship between variables. Hypotheses guide statistical tests to determine whether data support or refute them. The hypothesis suggesting an effect or difference is called the alternative hypothesis. The alternative hypothesis is always paired with a null-hypothesis, suggesting no effect or difference\n\n\nindependent variable\nInfluences or helps us predict the level of a dependent variable. It is often treated as fixed, or “given” in statistical analysis, and is sometimes also called “explanatory variable”\n\n\ninference\nMaking predictions about the wider population or about the future\n\n\ninteraction effect\nAn interaction effect occurs when the effect of one independent variable, \\(x_1\\), on a dependent variable, \\(y\\), depends on the level of another independent variable, \\(x_2\\). In other words, the impact of \\(x_1\\) on \\(y\\) is not constant but varies depending on the value of \\(x_2\\), indicating that the two variables jointly influence the outcome in a non-additive way\n\n\nintercept\nThe intercept is the point at which the regression line intersects the y-axis. In this book we denote it as \\(\\beta_{0}\\)\n\n\ninterpretation\nThe explanation of results to answer the research question\n\n\ninterquartile range\nThe difference between the 3\\(^{\\text{rd}}\\) and the 1\\(^{\\text{st}}\\) quartiles\n\n\nintervening variable\nAn intervening variable is a variable that explains the relationship between two other variables, such that \\(x_1\\) affects \\(x_2\\), and in turn, \\(x_2\\) affects \\(y\\). In this case, the effect of \\(x_1\\) on \\(y\\) is indirect, operating through the intervening variable \\(x_2\\). This is also referred to as indirect causality\n\n\nliterature review\nAn analytical summary of the literature relating to a particular topic with the objective of identifying a gap and thus motivating a research question\n\n\nlogarithm\nLogarithm is defined as the exponent or power to which a base must be raised to yield a given number. Expressed mathematically, \\(x\\) is the logarithm of \\(n\\) to the base \\(b\\) if \\(b^x = n\\), in which case \\(x = log_b n\\)\n\n\nmarginal distributions\nMarginal distributions in a crosstabulation refer to the totals for each row and column, showing the overall distribution of each variable separately, regardless of the other. They are found in the margins of the table and help summarize how each variable is distributed across all categories\n\n\nmean\nIs equal to the sum of the observations divided by the number of observations\n\n\nmeasurement\nRefers to the selection of a measure or variable\n\n\nmedian\nSeparates the lower half from the upper half of observations\n\n\nmethod\nA tool for systematic investigation\n\n\nmode\nIs the most frequently occurring value\n\n\nmodel building\nRunning a number of regression models, each testing a different combination of variables\n\n\nmonotonic\nA relationship is monotonic if, as x increases, y never reverses direction: it either never decreases (monotone increasing) or never increases (monotone decreasing). The rate can change and the curve can be curved or flat in places—it just must not switch from increasing to decreasing (or vice versa).\n\n\nmulticausality\nMulticausality refers to a situation in which an outcome variable \\(y\\) is influenced by multiple independent variables, such that \\(x_1\\), \\(x_2\\), \\(x_3\\), all contribute to explaining variations in \\(y\\). In this context, no single variable fully accounts for changes in \\(y\\); instead, the outcome is the result of the combined effects of several causes. We distinguish a speacial case here in which the independent variables are interdependent\n\n\nnon-probability sampling\nIn non-probability sampling not every unit has the same probability of being sampled\n\n\nnormal distribution\nThe normal distribution is a bell-shaped probability distribution that is symmetrical around the mean. Approximately 68% of values fall within 1 standard deviation of the mean, 96% within 2 standard deviations, and 99.7% within 3 standard deviations. This is known as the empirical rule.\n\n\nnull hypothesis\nThe null hypothesis, denoted as \\(H_0\\), typically asserts that there is no effect or no difference – it serves as the point of comparison in the significance test. It thus assumes that the population value is equal to a specific value that indicates no effect. More formally, we can can note this as \\(H_{0}: \\mu = \\mu_{0}\\)\n\n\nOLS\nThe method of fitting a regression line by means of minimizing the sum of the squared distances between the observations and the estimated values\n\n\noutlier\nDefined as a value larger than the third quartile plus 1.5 times the interquartile range, or the first quartile minus 1.5 times the interquartile range\n\n\np-value\nThe p-value indicates the probability of obtaining a result equal to, or even more extreme than the observed value in the direction of the alternative hypothesis, assuming the null hypothesis is true. Common thresholds for significance are 0.05, 0.01, and 0.001. A smaller p-value suggests stronger evidence against the null hypothesis. The p-value is denoted as \\(p\\)\n\n\nparameter\nA parameter is the value a statistic would assume in the long run. It is also called the Expected Value\n\n\nparsimony\nRefers to the principle “as many as necessary, as few as possible”.\n\n\npartial slope coefficient\nA partial slope coefficient measures the influence of a variable in multiple regression, holding all other independent variables in the model constant\n\n\npercentile\nIn ordered data, the percentile refers to the value of a variable below which a certain proportion of observations falls\n\n\npopulation\nCollection of all cases which possess certain pre-defined characteristics\n\n\npopulation distribution\nThe probability distribution of the population\n\n\npopulation regression function\nThe Population Regression Function (PRF) describes the expected distribution of \\(y\\), given the values of the independent variable(s) \\(x\\). It is also called the conditional expectation function (CEF) and can be denoted as \\(E(y_{i}&#124;x_{i})\\)\n\n\npower\nAlso known as “sensitivity”. It is the probability that a test will detect a true effect (reject \\(H_0\\) when it is false). Power grows with larger effects and larger samples, and shrinks when you demand stronger evidence (smaller \\(\\alpha\\)) or when variability is high\n\n\nprimary data\nPrimary data are data you have collected yourself\n\n\nprobability\nRefers to how many times out of a total number of cases a particular event occurs. We can also see it as the chance of a particular event occurring\n\n\nprobability sampling\nIn probability sampling all units have the same probability of entering the sample. In addition, all possible combinations of n cases must have the same probability to be selected\n\n\nQM\nThe process of testing theoretical propositions through the analysis of numerical data in order to provide an answer to a research question\n\n\nquartile\nDivides ordered data into four equal parts and indicates the percentage of observations that falls into the respective quartile and below\n\n\nrange\nThe difference between the largest and the smallest observation\n\n\nreference category\nThe category of a dummy variable in respect to which the effect on the value of the dependent variable is displayed\n\n\nregression\nRegression analysis determines the direction and magnitude of influence of one or more independent variables on a dependent variable\n\n\nregression line\nThe regression line describes how the dependent variable is functionally related to the values of the independent variable. It it defined by the intercept \\(\\beta_{0}\\) and the slope \\(\\beta_{1}\\)\n\n\nreliability\nRefers to the extent to which repeated measurement produces the same results\n\n\nrepresentative sample\nA sample which contains all characteristic of the population in accurate proportions\n\n\nresearch question\nA specific enquiry relating to a particular topic or subject. It forms the starting point of the research cycle\n\n\nresidual\nAn estimation of the error term. The difference between an observation \\(y_{i}\\) and the estimated value \\(\\hat{y}_{i}\\). Denoted as \\(\\hat{\\epsilon}_{i}\\)\n\n\nResidual Sum of Squares\nThe variation that cannot be explained by the regression\n\n\nreverse causality\nCausality runs from x to y. If y could also cause x, then we deal with reverse causality\n\n\nsample\nA sub-group of the population\n\n\nsample distribution\nThe probability distribution of a sample\n\n\nSample Regression Function\nA regression line based on a randomly drawn sample\n\n\nsampling\nThe process of selecting sampling units from the population\n\n\nsampling\nThe process of selecting sampling units from the population\n\n\nsampling distribution\nThe probability distribution of a sample statistics, such as the mean. It can be derived from repeated sampling, or by estimation\n\n\nsampling error\nThe extent to which the mean of the population and the mean of the sample differ from one another\n\n\nsampling method\nThe way the sample is created\n\n\nsecondary data\nSecondary data are data which have been collected by somebody else\n\n\nsignificance level\nThe significance level, denoted by \\(\\alpha\\), is the threshold used in hypothesis testing to determine if a result is statistically significant. It represents the probability of rejecting the null hypothesis when it’s actually true (a Type I error). Common levels are 0.05 or 0.01, indicating 5% or 1% risk. We will cover this properly in Week 9.\n\n\nsignificance test\nA significance test is a statistical method used to determine whether observed data provide enough evidence to reject a null hypothesis. It calculates a probability of observing data as extreme as, or more extreme than, the actual sample results, assuming the null hypothesis is true\n\n\nslope\nA slope is defined as rise over run, and so it tells us how many units of y we need to climb (or descend if the slope is negative) for every additional unit of the independent variable \\(x\\)\n\n\nSocial Sciences\nAre concerned with the study of society and seek to scientifically describe and explain the behaviour of actors\n\n\nspurious\nA relationship between two variables, \\(x_1\\) and \\(y\\), is said to be spurious when it appears to exist in a simple analysis but disappears or weakens substantially once a third variable, \\(x_2\\), is introduced into the analysis. This suggests that the observed association between \\(x_1\\) and \\(y\\) was not causal, but rather the result of a confounding influence from \\(x_2\\)\n\n\nstandard deviation\nThe standard deviation s is defined as \\[\\begin{equation*}s=\\sqrt{\\frac{\\text{sum of squared deviations}}{\\text{sample size} -1}}=\\sqrt{\\frac{\\Sigma(y_{i} - \\bar{y})^2}{n-1}}\\end{equation*}\\]\n\n\nstandard error\nThe standard deviation of the sampling distribution. It is defined as:\\[\\begin{equation*}\\sigma_{\\bar{y}} = \\frac{\\sigma}{\\sqrt{n}}\\end{equation*}\\]\n\n\nstatistical analysis\nhe descriptive and inferential elements of Statistics\n\n\nstatistical dependence\nStatistical dependence in crosstabulations refers to a situation where the distribution of one categorical variable varies across the levels of another. If the variables are dependent, knowing the value of one gives information about the other; if independent, the distribution of one remains consistent across all categories of the other\n\n\nStatistics\nStatistics is the science of collecting, organising, analysing and interpretingnumerical data to assist in making a more informed decision\n\n\nsuppressor\nA suppressor variable is a variable, \\(x_2\\), that increases the predictive validity of another variable, \\(x_1\\), on an outcome \\(y\\) by accounting for irrelevant or misleading variance in \\(x_1\\). In other words, the relationship between \\(x_1\\) and \\(y\\) becomes stronger or more apparent when \\(x_2\\) is included in the analysis, even though \\(x_2\\) may have little or no direct relationship with \\(y\\)\n\n\nsymmetry\nIn the context of causation, symmetry is understood as the law-like regularity of events. There needs to be a recipe (causal mechanism) which regularly produces effects from causes (see Brady (2011))\n\n\nsystematized concept\nA specific formulation of a concept used by a given scholar or group of scholars; commonly involves an explicit definition (Adcock & Collier, 2001, p. 531)\n\n\nt-distribution\nThe t-Distribution is bell-shaped and symmetrical around a mean of zero. Its shape is dependent on the degrees of freedom in the estimation process.\n\n\ntest statistic\nA test statistic is a value calculated from the sample data that is used to decide whether to reject the null hypothesis (H\\(_0\\)) in a hypothesis test. It quantifies the degree to which the observed data diverges from what is expected under the null hypothesis. In a t-test, the test statistic is a t-value, which measures the distance between the sample mean and the (hypothesised) population mean, expressed in units of standard errors\n\n\ntheory\nA formal set of ideas that is intended to explain why something happens or exists (Oxford Learner’s Dictionaries, n.d.)\n\n\nType I Error\nA Type I Error occurs when a null hypothesis (H\\(_0\\)) that is actually true is incorrectly rejected. It is also known as a false positive errors, as it suggests that an effect of difference exists, when, in fact, it does not. The probability of committing a Type I Error is denoted by the significance level (\\(\\alpha\\)) of the test, which is typically set before conducting the test (e.g. \\(\\alpha\\) = 0.05). This means that there is a 5% chance of rejecting the true null hypothesis\n\n\nType II Error\nA Type II Error occurs when a null hypothesis (H\\(_0\\)) that is actually false is incorrectly accepted (or not rejected). It is also known as a false negative error, as it suggests that no effect or difference exists when, in fact, there is one\n\n\nvalidity\nThe extent to which the measure (variable) you choose genuinely represents the concept in question. The word comes from the Latin word “validus” which means “strong”\n\n\nvalue\nNumerical expressions taken by variables (close ended or open ended list)\n\n\nvariable\nAn element of a conceptual component which varies. We also call these “measures”\n\n\nvariance\nIs equal to the squared standard deviation\n\n\nz-score\nThe z-score, sometimes also referred to as z-value, expresses in units of standard deviation how far an observation of interest falls away from the mean. It is defined as \\[\\begin{equation*}z = \\frac{\\text{observation} - \\text{mean}}{\\text{standard deviation}}=\\frac{y-\\mu}{\\sigma}\\end{equation*}\\]\n\n\n\n\n\n\nTable 1: Glossary for PO91Q\n\n\n\n\n\n\n\n\n\nAdcock, R., & Collier, D. (2001). Measurement Validity: A Shared Standard for Qualitative and Quantitative Research. American Political Science Review, 95(3), 529–546. https://doi.org/10.1017/s0003055401003100\n\n\nBrady, H. E. (2011). Causation and Explanation in Social Science. In R. Goodin (Ed.), The Oxford Handbook of Political Science. Oxford University Press. https://doi.org/10.1093/oxfordhb/9780199604456.013.0049\n\n\nClark, T., Foster, L., Sloan, L., & Bryman, A. (2021). Bryman’s Social Research Methods (Sixth Edition). Oxford: Oxford University Press.\n\n\nOxford Learner’s Dictionaries. (n.d.). https://www.oxfordlearnersdictionaries.com/\n\n\nReiche, F. (forthcoming). Introduction to Quantitative Methods in the Social Sciences. Oxford: Oxford University Press.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Full Glossary</span>"
    ]
  },
  {
    "objectID": "13-bib.html",
    "href": "13-bib.html",
    "title": "List of References",
    "section": "",
    "text": "List of References\n\n\nAdcock, R., & Collier, D. (2001). Measurement\nValidity: A Shared Standard for Qualitative and Quantitative\nResearch. American Political Science\nReview, 95(3), 529–546. https://doi.org/10.1017/s0003055401003100\n\n\nArel-Bundock, V. (2022). modelsummary: Data and\nModel Summaries in R. Journal of Statistical Software,\n103(1), 1–23. https://doi.org/10.18637/jss.v103.i01\n\n\nBoix, C., & Stokes, S. (2003). Endogenous\nDemocratization. World Politics,\n55, 517–549. https://doi.org/https://doi.org/10.1353/wp.2003.0019\n\n\nBrady, H. E. (2011). Causation and Explanation in\nSocial Science. In R. Goodin (Ed.), The\nOxford Handbook of Political Science. Oxford University\nPress. https://doi.org/10.1093/oxfordhb/9780199604456.013.0049\n\n\nButton, K. S., Ioannidis, J. P. A., Mokrysz, C., Nosek, B. A., Flint,\nJ., Robinson, E. S. J., & Munafò, M. R. (2013). Power failure: Why\nsmall sample size undermines the reliability of neuroscience. Nature\nReviews Neuroscience, 14(5), 365–376. https://doi.org/10.1038/nrn3475\n\n\nClark, T., Foster, L., Sloan, L., & Bryman, A. (2021). Bryman’s Social Research Methods (Sixth\nEdition). Oxford: Oxford University Press.\n\n\nCohen, J. (2013). Statistical power analysis for the behavioral\nsciences. Routledge. https://doi.org/10.4324/9780203771587\n\n\nEpstein, D. L., Bates, R., Goldstone, J., Kristensen, I., &\nO’Halloran, S. (2006). Democratic Transitions.\nAmerican Journal of Political Science,\n50(3), 551–569. https://doi.org/10.1111/j.1540-5907.2006.00201.x\n\n\nEuropean Comission. (n.d.). Eurostat – Your Key\nto European Statistics. https://ec.europa.eu/eurostat/data/database\n\n\nGOV.UK. (2013). National Statistics: Income and\ntax by Parliamentary constituency. https://www.gov.uk/government/statistics/income-and-tax-by-parliamentary-constituency-2010-to-2011\n\n\nHouse of Commons Library. (n.d.). Data Dashboard.\nhttps://commonslibrary.parliament.uk/type/data-dashboard/\n\n\nKing, G. (1995). Replication, replication.\nPS: Political Science and Politics, 28(3), 541–559. https://doi.org/10.2307/420301\n\n\nLakens, D. (2013). Calculating and reporting effect sizes to facilitate\ncumulative science: A practical primer for t-tests and ANOVAs.\nFrontiers in Psychology, 4. https://doi.org/10.3389/fpsyg.2013.00863\n\n\nLondon Data Store. (2010). London Parliamentary Constituency\nProfiles 2010. https://data.london.gov.uk/dataset/london-parliamentary-constituency-profiles\n\n\nLondon Data Store. (2013). Ward Profiles and\nAtlas. https://data.london.gov.uk/dataset/ward-profiles-and-atlas\n\n\nMarshall, M. G., & Gurr, T. R. (2020). Polity V Project: Political Regime Characteristics and\nTransitions, 1800-2018. http://www.systemicpeace.org/inscrdata.html\n\n\nMiller, M., Boix, C., & Rosato, S. (2022). Boix-Miller-Rosato Dichotomous Coding of Democracy,\n1800-2020 (Version V1) [Data set]. Harvard Dataverse. https://doi.org/10.7910/DVN/FENWWR\n\n\nOxford Learner’s Dictionaries. (n.d.). https://www.oxfordlearnersdictionaries.com/\n\n\nPrzeworski, A., Alvarez, M. E., Cheibub, J. A., & Limongi, F.\n(2000). Democracy and Development - Political\nInstitutions and Well-Being in the World, 1950-1990.\nCambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511804946\n\n\nReiche, F. (forthcoming). Introduction to\nQuantitative Methods in the Social Sciences. Oxford: Oxford\nUniversity Press.\n\n\nStimson, J. A. (n.d.). Professional Writing in\nPolitical Science: A Highly opinionated Essay. http://stimson.web.unc.edu/files/2018/02/Writing.pdf\n\n\nTufte, E. R. (2001). The Visual Display of\nQuantitative Information (Second Edition). Cheshire, Conn:\nGraphics Press. https://doi.org/10.1198/tech.2002.s78\n\n\nUniversity of Manchester, Cathie Marsh Institute for Social Research\n(CMIST), UK Data Service, Office for National Statistics. (2019).\nCrime Survey for England and Wales, 2013-2014:\nUnrestricted Access Teaching Dataset. https://doi.org/10.5255/UKDA-SN-8011-1\n\n\nWorld Bank. (n.d.). World Development Indicators.\nhttps://datacatalog.worldbank.org/dataset/world-development-indicators",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>List of References</span>"
    ]
  }
]