[["index.html", "PO91Q: Seminar Companion Preface", " PO91Q: Seminar Companion Dr Flo Reiche Department of Politics and International Studies University of Warwick Last Updated 31 March, 2025 Preface !!! site still under construction !!! Welcome to “Fundamentals in Quantitative Research Methods”. This module has two main aims: introduce to secondary data acquisition, management and analysis in the social sciences; prepare to attend further statistical training and make use of statistics in future research works, academic (such as master’s or PhD dissertations) or not. The module does not require any prior knowledge of mathematics or statistics. You only need to understand the importance of statistics for empirical social sciences, and show willingness to learn about them. This is the online companion to the seminars on PO91Q. It hosts all of the material needed for the seminars and replaces any physical worksheets. This is environmentally friendlier, and has the great advantage that when working with R, it allows you to copy/paste code directly from the code chunks in this online companion. I hope you find this useful! "],["companion-features.html", "Companion Features", " Companion Features You will find embedded in the text four different types of boxes which serve different purposes: This box appears whenever I want you to stop at a particular point in the worksheet and to flag up to me that you are done. This appears when you need to be careful with your coding in R to avoid problems. Some explanations that will hopefully make your work with this webpage or learning the material itself easier. We will start working with R in week 5, and I have recorded some videos to ease you into working with the program. A brief question which tests your understanding of the previous material. A definition we encountered in the lecture. "],["accessibility.html", "Accessibility", " Accessibility For those of you who prefer a dark background, like me, you can select this option from the menu at the top of the page. Click the “A” symbol, and then you can choose between “white”, “sepia”, or “night”. The companion uses the font “Lexend”. Lexend fonts are intended to reduce visual stress and so improve reading performance. Initially they were designed with dyslexia and struggling readers in mind, but Bonnie Shaver-Troup, creator of the Lexend project, soon found out that these fonts are also great for everyone else. "],["introduction-to-r.html", "Introduction to R Installation R - Getting Started RScript First Steps in R The Working Directory R Packages Saving", " Introduction to R Installation Today we start working with R and the first step is to install the program. Please follow these instructions: Go to https://cran.r-project.org/mirrors.html and select a server from which you want to download R. It is convention to do this from the server which is nearest to you. Follow on-screen instructions and install the program. Go to https://rstudio.com/products/rstudio/download/ and download RStudio Desktop which is free. Install the program. Now open RStudio - you do not need to open R itself, as we will be operating it through RStudio. Whilst you need to install both R and RStudio, we will never be working with R directly. Instead, we will be operating it through RStudio. R - Getting Started In this worksheet and also in all other presentations and documents I use on this module, I am using two different fonts: Font for plain text A typewriter font for R functions, values, etc. I am also regularly including “screenshots” of operations in R with their output. Whenever you see these, please replicate them on your own computer. To start, let’s have a look at RStudio itself. When you open the programme, you are presented with the following screen: Figure 1: RStudio It has – for now – three components to it. On the left hand-side you see the so-called Console into which you can enter the commands, and in which also most of the results will be displayed. On the right hands side, you see the Workspace which consists of an upper and a lower window. The upper window has three tabs in it. The tab Environment will provide you with a list of all the data sets you have loaded into R, and also of the objects and values you create (more on that later). Under the History tab, you find a history (I know, who would have thought it) of all the commands you have used. This can be very useful to retrace your steps. In the Connections tab you can connect to online sources. We will not use this tab. In the lower window, you have five tabs. Under Files you find the file structure of your computer. Once you have set a working directory (more on that in a moment), you can also view the files in your working directory here which gives you a good overview of the files you need to refer to for a particular project. The Plots tab will display the graphs we will be producing. Packages form the heart and soul of R and they make the program as powerful as it is (again, more on that later). RStudio also has a Help function, which is rarely very illuminating. I usually search for stuff online on “stackexchange”, as there is a large community of R users out there who share their knowledge and solutions to problems. We won’t use the last tab Viewer. Introduction to R Studio If you can’t get enough of my delightful German accent, then I have some videos for you in which I go through the respective components of the worksheet on screen. Here is the first: RScript If you read the previous section carefully, you will have noticed that I wrote that you can enter the commands” in the Console. You can, but you shouldn’t. What you should be using instead is an RScript. An RScript is a list of commands you use for a project (an essay, your dissertation, an article) to calculate quantities of interest, such as descriptive statistics in the form of mean, median and mode, and produce graphs. One of the foundations of scientific research is “reproducibility”“, or”replicability”. This means that “sufficient information exists with which to understand, evaluate, and build upon a prior work if a third party could replicate the results without any additional information from the author.” King (1995, p. 444, emphasis removed) This principle applies in academia more generally, because only if you understand what a person has done before you, you can pick their work up whether they left it, and push the boundaries of knowledge further. But a bit closer to home, it is also relevant for conducting quantitative research in assessments. We require you to submit an RScript (or a “do file” if you use Stata) now together with your actual essay. This is not only to check what you have done; data preparation is often the most time-consuming part (as you will soon discover), and this is a way to gain recognition for this work. So it is actually to your advantage, and not a mere plagiarism check. The creation of an RScript will allow you to open the raw data, and by running the script, to bring it to exactly where you left off. This saves you saving data sets which can take up a lot of work. If you back the script up properly, you also have an insurance against losing all your work a day before the assessment is due. To create an RScript, click File \\(\\rightarrow\\) New File \\(\\rightarrow\\) RScript. A fourth window opens, and your screen will now look something like this: Figure 2: The RScript Window You can now write your commands in the RScript, where a new line (for now) means a new command. If you want to execute a command, put the cursor on the line the command is on and press “command” / “enter” simultaneoulsy on a Mac and “Ctrl” / “Enter” on Windows. If you precede a line with #, you can write annotations to yourself, for example explaining what you do with a particular command. More on this in the next sub-section. Figure 3 shows the start of the RScript for this worksheet. I prefer a dark background, it’s easier on the eyes, especially when you work with R for long periods. You can change the settings in: Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Appearance \\(\\rightarrow\\) Twilight. Figure 3: Example of an RScript More Themes If you copy and paste the following code chunks into your “Console” and run one at a time, you will have even more themes1 to choose from: install.packages( &quot;rsthemes&quot;, repos = c(gadenbuie = &#39;https://gadenbuie.r-universe.dev&#39;, getOption(&quot;repos&quot;)) ) rsthemes::install_rsthemes() You can also download Flo’s Dark Theme2 and then “add” it at the bottom of the “Appearance” menu. Appearance RScript Structure Well, I am German, and I like things neat and tidy, so I feel almost compelled to discuss how to properly organise an RScript. But apart from genetical dispositions, a well-organised RScript is also very much in the spirit of reproducibility. It simply makes sense to structure an RScript in such a way that another researcher is able to easily read and understand it. First of all, which commands to include? If you introduce me to your current girlfriend or boyfriend, I have no interest in learning about all your past relationships; they have not worked out. In a similar fashion, nobody wants to read through lines of code that are irrelvant. So you will only include in the RScript those commands which produce the output you actually include in the essay or article. I stated above that if you precede a line with #, you can write annotations to yourself. This is also a useful way to structure an RScript, for example into exercise numbers, sections of an essay /article, or different stages of data preparation (which we will be doing in due course). RScript Structure First Steps in R But enough of the preliminary talk, let’s get started in R. In principle, you can think of R as a massive and powerful calculator. So I will use it as such to start of with. If you want to know what the sum of 5 and 3 is, you type: 5+3 and execute the line as previously explained. In everything that is to follow, commands will be shown in boxes with the output underneath preceded by a number in square brackets. So, including result, the calculation would look like this: 5+3 [1] 8 where the [1] indicates that the 8 is the first component of the result. In this case, we only have one component, so it’s superfluous really, but we will soon encounter situations in which results can have a number of different items. You can copy the code from this page by hovering over the code chunk and clicking the icon in the top-right hand corner. You can then paste it into your RScript. A fundamental component of R is objects. You can define an object by way of a reversed arrow, and you can assign values, characters, or functions to them. If we want to assign the sum of 5 and 3 to an object called result, for example, we call3 result &lt;- 5+3 If we now call the object, R will return its value, 8. result [1] 8 Make a habit of adding a note underneath each code chunk in your RScript (preceded with a #) in which you translate the code into plain English. This is especially useful for the lengthy complex chunks. The Working Directory It is imperative that you create a suitable filing system to organise the materials for all of your modules. At the very least you should have a folder called “University” or similar, in which you have a sub-folder for each module you take. In those modules in which you are working with R, you need to extend this system a little. I have created a schematic of what I have in mind in Figure 4. Figure 4: Folder Structure You see that there is a sub-folder for each week of the module (I have only done three for illustrative purposes), and that each of these folders is divided into lecture and seminar in turn. Into these you can place the lecture and seminar materials, respectively. Create this system now for PO11Q. R works with so-called Working Directories. You can think of these as drawers from which R takes everything it needs to conduct the analysis (such as the data set), and into which it puts everything it produces (such as graph plots). As this will be an R-specific drawer within the seminar, create yet another sub-folder in your seminar folder, and call it something suitable, such as “PO11Q_Seminar_Week 1”. Do NOT call this “Working Directory”, as you will have many of those, rendering this name completely meaningless. Save the file EU.xlsx into this folder. Data are taken from European Comission (n.d.). Please set up this structure now. If I find you using a random folder on your desktop named “working directory” in the coming weeks, I am going to implode! I mean it. Now we need to tell R to use this folder. If you know the file structure of your computer you can simply use the setwd() command, and enter the path. Here is an example from my computer: setwd(&quot;~/Warwick/Modules/PO91Q/Seminars/Week 1/R Week 1&quot;) If you don’t know the file structure of your computer, then you can click Session \\(\\rightarrow\\) Set Working Directory \\(\\rightarrow\\) Choose Directory. Working Directory R Packages It would be difficult to overstate the importance of packages in R. The program has a number of “base” functions which enable the user to do many different basic things, but packages are extensions that allow you to do pretty much anything and everything with this software - this is one of the reasons why I love it so much. The first package we need to use will enable us to load an Excel sheet into R. It is called readxl. You can install any package with the command install.packages() where the package name goes, wrapped in quotation marks, into the brackets: install.packages(&quot;readxl&quot;) We can then load this package into our library with the library() command. library(readxl) Once you close R at the end of a session, the library will be reset. When you reopen R, you have to load the packages you require again. But you do not have to install them again. Saving Please now save this RScript into the same folder (working directory) as the raw data. When R asks before closing, there is no need to save the workspace or the data, as running the RScript on the raw data will bring you precisely to where you left off. Source: https://www.garrickadenbuie.com/project/rsthemes/↩︎ This is a variation of the Dracula Theme.↩︎ To “call” means to execute a command.↩︎ "],["exercises.html", "Exercises Core Exercises Going Further", " Exercises Core Exercises Execute the following lines of command. Each time try and explain what is happening, using statistical terms as in the slides and textbooks. If you know another statistical software, spot commonalities and differences with R. Experiment variations on your own to make sure your interpretations are right. a&lt;-1 class(a) a b&lt;-2 b c&lt;-a+b c d&lt;-2*c d e&lt;-c(10,20,30) class(e) e e*10 f&lt;-c(&quot;Small&quot;,&quot;Medium&quot;,&quot;Big&quot;) class(f) f e[1] f[3] mean(e) mean(f) sd(e) ?sd example(sd) g&lt;-c(e,f) g h&lt;-rbind(e,f) i&lt;-cbind(e,f) class(h) h[1,] i[,2] h[1,2] ls() ls.str() Download the “European Social Survey” dataset from https://www.europeansocialsurvey.org/data. You will need to register and receive a confirmation email. Select round 9 (2018), SPSS version (.sav). Download also: the questionnaire the variable list the “showcards” the Source project instructions Take a bit of time to make sure you understand what each of these documents is for. Can you tell: what is(are) the population(s) of reference? how was(were) the sample(s) composed? how do interviewers reach respondents? what is the method for questionnaire administration? Use SPSS to open the dataset. SPSS is pre-installed on Warwick PCs. Otherwise, download and install it for free (after logging in): https://www2.warwick.ac.uk/services/its/servicessupport/software/list/spss In the dataset open in SPSS, identify key statistical elements: Cases and their ID Variables and values Categorical, ordinal and numeric variables We can now pick up where we left on the worksheet. Make sure that you have created an appropriate folder structure and set your working directory. Also ensure you have loaded the foreign package. setwd(&quot;~/Warwick/Modules/PO91Q/Seminars/Week 1/Worksheet&quot;) # This is my WD library(foreign) # Loads the package for importing data ESS&lt;-read.spss(&quot;ESS9e03.sav&quot;,to.data.frame=TRUE,max.value.labels=10) # Imports the SPSS data with the right R format # (ignore Warning messages in red) # (&#39;to.data.frame&#39; converts the SPSS dataset into an R dataset) # (&#39;use.missings=TRUE&#39; means you do not classify the values # coded as missing in SPSS as missing in R) attach(ESS) View(ESS) # Compare with the SPSS view # Can you sort the data? # Can you filter the data? nrow(ESS) ncol(ESS) dim(ESS) names(ESS) str(ESS) head(ESS) # Can you imagine what the 6 functions above are for? # Afterwards, check your answers using &#39;?function&#39; # to get the help file for each function. Execute: table(cntry). What uses could you make of this variable in research? Going Further In ESS questionnaire, showcards and variable list, identify the questions about: languages spoken by respondent R R’s educational background R’s relationship to politics How many questions about each topic can you find? How suitable would this survey be to study these three topics? Select one of the three topics above according to your interest. Select a set of three variables of particular interest (called X, Y and Z below). Make sure you understand the meaning of X, Y and Z. X Y Z What do these return? table(X) plot(X) Download the “British Social Attitudes” Survey 2016 from the UK Data service website. You will need to register, validate your registration through email, indicate that this is within an academic course and agree with conditions. Choose .tab and .sav format. Open the .tab dataset with Excel and record as .xlsx file Open the .sav dataset with SPSS Import from the SPSS file into R (same as above with ESS) Try to understand the differences between the 3 softwares, their respective advantages and disadvantages. How does the BSA compare with the ESS regarding the three topics listed above? Download the World Values Study 7 from Gesis: https://dbk.gesis.org/dbksearch/sdesc2.asp?no=4804. Select all relevant files. How does the EVS compare with the ESS and BSA regarding the 3 topics above? "],["homework-for-week-2.html", "Homework for Week 2", " Homework for Week 2 Finish working through this worksheet. Add a note underneath each code chunk in your RScript (by starting the line with #), translating the code into plain English. This will help you learn the vocabulary and grammar of R quicker. If you are unsure what individual functions mean, you can find a .csv file with a full list for each week underneath the flashcards (see below, but here is an example). Read the required literature for week 2. Work thoroughly through chapters 7 and 8 of the Fogarty book to make sure you are familiar with all the relevant commands to produce descriptive statistics and graphs with R. Work through this week’s flashcards to familiarise yourself with the relevant R functions. Find an example for each NEW function and apply it in R to ensure it works "],["glossary.html", "Glossary", " Glossary Table 1: Glossary Week 1 Term Description analysis A detailed evaluation of data to discover their structure and relevant information to answer a research question data Derives from the Latin which means . For our purposes it is a collection of numbers (or quantities) for the purpose of analysis interpretation The explanation of results to answer the research question literature review An analytical summary of the literature relating to a particular topic with the objective of identifying a gap and thus motivating a research question method A tool for systematic investigation QM The process of testing theoretical propositions through the analysis of numerical data in order to provide an answer to a research question research question A specific enquiry relating to a particular topic or subject. It forms the starting point of the research cycle Social Sciences Are concerned with the study of society and seek to scientifically describe and explain the behaviour of actors theory A formal set of ideas that is intended to explain why something happens or exists (Oxford Learner’s Dictionaries, n.d.) "],["flashcards.html", "Flashcards", " Flashcards From this week onwards you will find here some flashcards which should help you learn the purpose of R functions on PO91Q. These are divided into: New Functions this week, containing only those functions that we encounter for the first time All Functions this week, containing all functions we have used in that particular week All Functions up to now, containing all functions used on the module up to that particular week. You can download a .csv file containing the functions and the descriptions for each of these underneath the flashcard window. Should you wish to create your own flashcards, for example, with more difficult-to-remember functions, I have written some instructions how to go about it. R Functions This Week The data are available as a .csv file. "],["data-manipulation-in-r.html", "Data Manipulation in R Self-Reflection Questions – Group Work Opening your Data Set Viewing the Data Variable Types in R Sub-Setting Data Ordering Data Grouping Data Combining Ordering and Grouping Data Descriptive Statistics Graphs Basic Graphs Advanced Graphs Even More Advanced Graphs Organising Code in the RScript Exercises Captions for Tables and Figures in Word", " Data Manipulation in R Self-Reflection Questions – Group Work Why is it good practice to always use an RScript, rather than simply entering your commands into the console in R? Give an example of an unordered factor variable and an ordered one. Why are descriptive statistics useful? Give an example of a situation where you would need to transform a character variable into a factor variable. Why should you always create a new variable, instead of overwriting an existing one? These questions are taken from Reiche (forthcoming). Please stop here and don’t go beyond this point until we have compared notes on your answers. Opening your Data Set We are now ready to open a data set in R - where it is called a “data frame”. For this, we create a new object EU, and ask R to read “Sheet 1”” of the Excel file “EU.xlsx” which we placed in the working directory earlier library(readxl) EU &lt;- read_excel(&quot;EU.xlsx&quot;, sheet=&quot;Sheet1&quot;) We can now use our data in R! Loading the Data Set Please do not use the “Import Dataset” button in the Environment, but do this properly, manually. We sometimes need to set options for importing data sets, and the “pointy, clicky” approach won’t be able to offer you what you need. Viewing the Data Unless you have been cheeky and opened the file in Excel to have a look, you have no idea yet, what the data look like. So it’s a good idea to view the data frame before doing anything with it. You can use the View() command to see the data frame: View(EU) If you only want to see the first 6 observations of each variable, use the head() command: head(EU) # A tibble: 6 × 5 country pop18 access area GDP_2015 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Belgium 11413058 1951 30280 4.66e11 2 Bulgaria 7050034 2007 108560 1.22e11 3 Czechia 10610055 2004 77230 3.19e11 4 Denmark 5781190 1973 42430 2.46e11 5 Germany 82850000 1951 348540 3.60e12 6 Estonia 1319133 2004 42390 3.51e10 If you simply want to know the variable names in the data frame, type: names(EU) [1] &quot;country&quot; &quot;pop18&quot; &quot;access&quot; &quot;area&quot; &quot;GDP_2015&quot; The next one is a very important command, because it reveals not only the variable names and their first few observations, but also the nature of each variable (numerical, character, etc.). It is the str() command, where “str” stands for structure: str(EU) tibble [28 × 5] (S3: tbl_df/tbl/data.frame) $ country : chr [1:28] &quot;Belgium&quot; &quot;Bulgaria&quot; &quot;Czechia&quot; &quot;Denmark&quot; ... $ pop18 : num [1:28] 11413058 7050034 10610055 5781190 82850000 ... $ access : num [1:28] 1951 2007 2004 1973 1951 ... $ area : num [1:28] 30280 108560 77230 42430 348540 ... $ GDP_2015: num [1:28] 4.66e+11 1.22e+11 3.19e+11 2.46e+11 3.60e+12 ... You can see that R has recognised most variables as numerical, one is displayed as a character variable. This is appropriate for some variables, such as pop18, but not for the ordinal variable access which is ordinal. We need to recode it, and all other variables we are unhappy with. Variable Types in R R distinguishes between a number of different variable types and here is a broad overview of them. This will help you in deciding which descriptive statistics to calculate, or into which variable type you need to recode (next step) to achieve what you want. There are two general types: numeric – numbers character (also called string) – letters Within numeric we can distinguish between the following: factor - nominal ordered factor - ordinal integer - numeric, but only “whole” numbers (discrete) numeric - any number (interval or ratio) Numerical variables are already in the data set, we have to attend to nominal and ordinal variables. Nominal Variables In terms of the variable types we encountered in the lecture this week, the country name is a nominal variable. So we need to tell R to turn this into a factor variable. We do this as follows: EU$country = factor(EU$country) Ordinal Variables As mentioned above, the variable access should be ordinal, and therefore has to be turned into an ordered factor. The command which follows is almost identical to producing a factor variable, only that we add the option ordered = TRUE at the end: EU$access_fac = factor(EU$access, ordered = TRUE) If you are familiar with European Studies, you will know that each accession wave has got a particular name. The 1973 enlargement, for example, is called the “First Enlargement”, the 1981 wave the Mediterranean Enlargement, and so forth. Let us create a new variable which uses these names instead of the years. This process is a little more involved, and requires a new package to be installed and loaded: dplyr. This package is part of the so-called tidyverse which is a suite of packages designed to make working with R simpler and commands shorter. You can install all of them by calling install.packages(\"tidyverse\"). We then load the tidyverse with: library(tidyverse) The command which follows takes a little explaining. We start by stating the dataframe we wish to assign the result to, EU. Then we name the data frame that contains the data we wish to manipulate, here also EU. The symbol which follows, \\%&gt;\\%, reads as “and then”, and is called a “pipe”. So we take the data frame EU “and then” carry out a function called mutate. It creates a new variable. This function in turn defines the new variable wave by recoding the variable access_fac. The command then specifies all categories of the “old” variable access_fac and what their respective values in the “new” variable wave are going to be. The categories in each are set in quotation marks, as they are factor / character categories. EU &lt;- EU %&gt;% mutate(wave = recode(access_fac, &#39;1951&#39;=&quot;Founding&quot;, &#39;1973&#39;= &quot;First&quot;, &#39;1981&#39;= &quot;Mediterranean&quot;, &#39;1986&#39; = &quot;Mediterranean&quot;, &#39;1995&#39; = &quot;Cold War&quot;, &#39;2004&#39; = &quot;Eastern&quot;, &#39;2007&#39; = &quot;Eastern&quot;, &#39;2013&#39; = &quot;Balkans&quot;)) Please note that some colleagues in the department have decided to take a random dislike to the tidyverse as one of currently 22,250 packages4 and might therefore require you to use base R in their modules. I am still using the tidyverse however, as: I think it is nonsense to exclude one package in particular my textbook, which is going to be the main textbook for this module once it is published, uses the tidyverse GGPLOT2 which is part of the tidyverse simplifies code for generating figures significantly and will do for all but the most specific requirements a lot of support on stackexchange is geared toward the tidyverse as a lot of US-based data scientists work with this package, and so you will find it easier to solve problems But to keep everybody happy, I am providing the base R code whenever possible in a collapsible section like this one: Base R Solution EU$wave &lt;- NA EU$wave[EU$access_fac==&#39;1951&#39;] &lt;- &quot;Founding&quot; EU$wave[EU$access_fac==&#39;1973&#39;] &lt;- &quot;First&quot; EU$wave[EU$access_fac==&#39;1981&#39;] &lt;- &quot;Mediterranean&quot; EU$wave[EU$access_fac==&#39;1986&#39;] &lt;- &quot;Mediterranean&quot; EU$wave[EU$access_fac==&#39;1995&#39;] &lt;- &quot;Cold War&quot; EU$wave[EU$access_fac==&#39;2004&#39;] &lt;- &quot;Eastern&quot; EU$wave[EU$access_fac==&#39;2007&#39;] &lt;- &quot;Eastern&quot; EU$wave[EU$access_fac==&#39;2013&#39;] &lt;- &quot;Balkans&quot; EU$wave &lt;- factor(EU$wave, ordered = TRUE) Here, we first create a new, empty variable called wave in the EU data set. We then create new values , for example Founding in the variable EU$wave for the condition (this is what the square brackets [ ] do) that the variable access_fac in the EU data set, equals a specific value. For Founding this is is 1951. The last step is to turn the wave variable into an ordered factor. But back to the recoding exercise itself. As the original variable access_fac was already an ordered factor, R (or the mutate function to be precise) also returns wave as an ordered factor. Had we not done this, wave would have been an unorderd factor (aka nominal variable). You can specify in an option to the mutate function whether you want the factor to be ordered or not: EU &lt;- EU %&gt;% mutate(wave = recode(access_fac, &#39;1951&#39;=&quot;Founding&quot;, &#39;1973&#39;= &quot;First&quot;, &#39;1981&#39;= &quot;Mediterranean&quot;, &#39;1986&#39; = &quot;Mediterranean&quot;, &#39;1995&#39; = &quot;Cold War&quot;, &#39;2004&#39; = &quot;Eastern&quot;, &#39;2007&#39; = &quot;Eastern&quot;, &#39;2013&#39; = &quot;Balkans&quot;), ordered=TRUE) An alternative procedure, producing exactly the same result is to use the cut() function on the access variable which literally cuts up a variable into chunks at the points we specify. This only works on numerical variables! We are fine to use it here, as we didn’t change access, and it is still numerical. Incidentally, this shows you the benefit of always creating a new variable instead of overwriting the original: there is no “back” button in R, if you mess up, you will have the pleasure to start from the beginning. Again, we use the mutate function, this time naming our new variable wave1 (so as not to overwrite the wave variable we created with the recode() function). This time, we cut up the original variable at the accession years, and specify the levels, this time as labels. Labels denominate the output, whilst level are input. A factor only knows levels which is set by the label function. Here we have already created the levels with the cut() function, and assign labels to these in the second step. EU &lt;- EU %&gt;% mutate(wave1=cut(access, breaks=c(1950, 1951, 1973, 1986, 1995, 2007, 2013), labels=c(&quot;Founding&quot;,&quot;First&quot;, &quot;Mediterranean&quot;, &quot;Cold War&quot;, &quot;Eastern&quot;, &quot;Balkans&quot;))) levels(EU$wave) [1] &quot;Founding&quot; &quot;First&quot; &quot;Mediterranean&quot; &quot;Cold War&quot; &quot;Eastern&quot; [6] &quot;Balkans&quot; Base R Solution EU$wave &lt;- cut(EU$access, breaks=c(1950, 1951, 1973, 1986, 1995, 2007, 2013), labels=c(&quot;Founding&quot;,&quot;First&quot;, &quot;Mediterranean&quot;, &quot;Cold War&quot;, &quot;Eastern&quot;, &quot;Balkans&quot;)) Recoding a Factor Variable Recoding Ordered Factor Variables Binary Dummy Very often in political science we have yes/no scenarios, such as democracy yes or no, civil war, yes or no, etc. To analyse these scenarios, we can create so-called “dummy variables”. In the present example, let’s specify for each country whether it has been a founding member of the EU. It is a factor variable and so we do this exactly the same way as our initial recoding of the wave variable above: EU &lt;- EU %&gt;% mutate(founding = recode(access_fac, &#39;1951&#39;=&quot;Yes&quot;, &#39;1973&#39; = &quot;No&quot;, &#39;1981&#39; = &quot;No&quot;, &#39;1986&#39; = &quot;No&quot;, &#39;1995&#39; = &quot;No&quot;, &#39;2004&#39; = &quot;No&quot;, &#39;2007&#39; = &quot;No&quot;, &#39;2013&#39; = &quot;No&quot;)) # OR, much shorter EU &lt;- EU %&gt;% mutate(founding = factor(ifelse(access_fac==&quot;1951&quot;, &quot;Yes&quot;, &quot;No&quot;), levels =c(&quot;Yes&quot;, &quot;No&quot;))) # the result is the same str(EU$founding) Factor w/ 2 levels &quot;Yes&quot;,&quot;No&quot;: 1 2 2 2 1 2 2 2 2 1 ... Base R Solution Using ifelse, this is very similar, only the pipe disappears: EU$founding &lt;- factor(ifelse(EU$access_fac==&quot;1951&quot;, &quot;Yes&quot;, &quot;No&quot;), levels =c(&quot;Yes&quot;, &quot;No&quot;)) The ifelse function is very handy, and is therefore worth explaining in a little more detail. It reads as ifelse('condition', 'if condition met, then', 'otherwise'). So, the above code reads as “if the value of the variable access_fac is equal to 1951, then code the observation as ‘Yes’, otherwise as ‘No’”. Sub-Setting Data When we start analysing data, we rarely need all data at the same time. We might not need some variables, at all, for example, or we only want to work with certain observations, such as those countries in the “founding” wave. In these cases, we can subset the data. I will show you some examples of subsetting now. By Variable If you are sure you won’t need a variable (remember, there is no back button), you can simply drop (i.e. delete) it. Let’s do this with the area variable: EU$area &lt;- NULL If we are dropping multiple variables, we can either perform this operation each time, or use another command which allows us to operate with multiple variables at the same time. The select() command comes from the tidyverse package and specifies which variables we wish to keep: EU_pop &lt;- select(EU, country, pop18, access_fac, founding) This creates a new data frame called EU_pop containing only the variables country, pop18, access_fac, and founding. Base R Solution The package documentation offers some basic instructions how to convert the tidyverse (or dyplyr, to be precise) code into base R. But here is the solution for the previous code chunk: EU_pop &lt;- subset(EU, country, pop18, access_fac, founding) We can, however, use the same command and tell R which variables to drop by adding a minus sign in from of the variables we want to delete. The following command produces exactly the same result as the one before: EU_pop1 &lt;- select(EU, -access, -GDP_2015) By Observation Instead of dropping and keeping variables, we can do the same thing to individual observations. Here, we use the slice() command (like a cake) and specify which slices we want to drop or keep. For example to drop the Benelux countries we would delete observations 1, 16 and 19: EU_nobenelux &lt;- slice(EU, -1, -16, -19) Base R Solution EU_pop &lt;- EU[c(-1, -16, -19),] Alternatively, if we were only interested in Benelux countries we would subset to only those observations: EU_benelux &lt;- slice(EU, 1, 16, 19) Base R Solution EU_pop &lt;- EU[c(1, 16, 19),] Keep if a variable has a certain value One of the most useful commands is filter(), as it allows us to keep all observations for which the value of a variable is of a particular number. For example if we wanted to conduct an analysis with all countries which have a population in excess of 10 million we could subset by: EU_pop_large &lt;- filter(EU, pop18 &gt; 10000000) Base R Solution EU_pop_large &lt;- subset(EU, pop18 &gt; 10000000) Here is a list of some operators you can use for this purpose: Operator Description &lt; less than &lt;= less than or equal to &gt; greater than &gt;= greater than or equal to == exactly equal to != not equal to !x Not x x | y x OR y x &amp; y x AND y Subsetting Data Ordering Data The data set in its original state is purposely not ordered by any criterion, such as alphabetical order of countries, etc. But we can use R to do exactly that. Let us work with a subset containing only three variables: EU_subset &lt;- select(EU, country, pop18, access) It would be lovely if the command for ordering data would be called order(), but it is called arrange()5. Let’s order countries by ascending population in a new data frame called eu_order: eu_order &lt;- arrange(EU_subset, pop18) Base R Solution eu_order &lt;- EU_subset[order(EU_subset$pop18),] We can now display the first 10 rows with the following command: eu_order[1:10,] # A tibble: 10 × 3 country pop18 access &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Malta 475701 2004 2 Luxembourg 602005 1951 3 Cyprus 864236 2004 4 Estonia 1319133 2004 5 Latvia 1934379 2004 6 Slovenia 2066880 2004 7 Lithuania 2808901 2004 8 Croatia 4105493 2013 9 Ireland 4838259 1973 10 Slovakia 5443120 2004 The content in the brackets refers to the rows (before the comma), and to the columns (after the comma). As we only want certain rows and displaying all variables, I have left the space after the comma blank. We can do the same thing in descending order by calling: eu_order &lt;- arrange(EU_subset, desc(pop18)) eu_order[1:10,] # A tibble: 10 × 3 country pop18 access &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Germany 82850000 1951 2 France 67221943 1951 3 United Kingdom 66238007 1973 4 Italy 60483973 1951 5 Spain 46659302 1986 6 Poland 37976687 2004 7 Romania 19523621 2007 8 Netherlands 17181084 1951 9 Belgium 11413058 1951 10 Greece 10738868 1981 Base R Solution eu_order &lt;- EU_subset[order(desc(EU_subset$pop18)),] A neat feature of R is that it allows us to order observations by more than one variable. So for example, we could order them by ascending accession wave first, and then by ascending population in 2018 as follows: eu_order &lt;- arrange(EU_subset, access, pop18) eu_order[1:10,] # A tibble: 10 × 3 country pop18 access &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Luxembourg 602005 1951 2 Belgium 11413058 1951 3 Netherlands 17181084 1951 4 Italy 60483973 1951 5 France 67221943 1951 6 Germany 82850000 1951 7 Ireland 4838259 1973 8 Denmark 5781190 1973 9 United Kingdom 66238007 1973 10 Greece 10738868 1981 Base R Solution eu_order &lt;- EU_subset[order(EU_subset$access,EU_subset$pop18),] # or slightly shorter eu_order &lt;- EU_subset[order(with(EU_subset, access,pop18)),] Grouping Data Looking at the last example, a question that might spring up is in which accession wave the joining countries brought the largest population increase on average to the EU. We can calculate summary statistics for a particular group by, well, grouping them. The first step is to group data into rows with the same value: eu_access &lt;- group_by(EU_subset, access) By the way: whenever you have grouped anything, and finished analysing data in this grouped version it is essential that you ungroup the data afterwards, so that you don’t unintentionally keep using the groups: ungroup(EU_subset) # A tibble: 28 × 3 country pop18 access &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Belgium 11413058 1951 2 Bulgaria 7050034 2007 3 Czechia 10610055 2004 4 Denmark 5781190 1973 5 Germany 82850000 1951 6 Estonia 1319133 2004 7 Ireland 4838259 1973 8 Greece 10738868 1981 9 Spain 46659302 1986 10 France 67221943 1951 # ℹ 18 more rows But let’s calculate the average population size per accession wave in an elegant command which combines multiple steps by using pipes: eu_popaccess &lt;- EU_subset %&gt;% group_by(access) %&gt;% summarise(avg = mean(pop18)) eu_popaccess # A tibble: 8 × 2 access avg &lt;dbl&gt; &lt;dbl&gt; 1 1951 39958677. 2 1973 25619152 3 1981 10738868 4 1986 28475164. 5 1995 8151880. 6 2004 7327746. 7 2007 13286828. 8 2013 4105493 Base R Solution eu_popaccess1 &lt;- aggregate(pop18 ~ access, data = EU_subset, FUN = mean ) You now see a new variable called avg which contains the average population increase for each wave. In which wave did the joining countries have the largest population on average? Combining Ordering and Grouping Data The question was easy to answer here, as we only have a few accession waves. It starts to get unwieldy though, the more groups we have, but we can let R do the job by combining first grouping, and then ordering. So we take the grouped data frame eu_popaccess and order it by descending avg: eu_popaccess_order &lt;- arrange(eu_popaccess, desc(avg)) eu_popaccess_order # A tibble: 8 × 2 access avg &lt;dbl&gt; &lt;dbl&gt; 1 1951 39958677. 2 1986 28475164. 3 1973 25619152 4 2007 13286828. 5 1981 10738868 6 1995 8151880. 7 2004 7327746. 8 2013 4105493 Base R Solution eu_popaccess_order &lt;- eu_popaccess[order(desc(eu_popaccess$avg)),] Descriptive Statistics We have covered quite a large number of descriptive statistics, so far. These are: Mean Median Mode Standard Deviation Variance Quartiles and Percentiles Range Interquartile Range They are a lot of effort to calculate by hand, especially for larger data sets, but R can do these with a few intuitive commands. First up is the mean. mean(EU$pop18) [1] 18311106 Then the median: median(EU$pop18) [1] 9300319 You can get information on the quartiles (remember that the median is the second quartile), the mean, as well as the minimum and maximum through one, simple command: summary(EU$pop18) Min. 1st Qu. Median Mean 3rd Qu. Max. 475701 3781345 9300319 18311106 17766718 82850000 Let’s now move to measures of variability. First up is the range; you can either calculate this with two commands by finding out the minimum and maximum separately, or just ask R to give you both values straight away: min(EU$pop18) [1] 475701 max(EU$pop18) [1] 82850000 range(EU$pop18) [1] 475701 82850000 The stadard deviation is rather long-winded to calculate by hand, but the R command is short and sweet: sd(EU$pop18) [1] 23787945 As you know, the variance is the squared standard deviation, but you can calculate it with its own command in R, too: var(EU$pop18) [1] 5.658663e+14 Descriptive Statistics Now that you have all the relevant tools at hand, complete the following tasks: Generate descriptive statistics for 3 of our variables. Recode the variable ‘GDP_2015’ into a ordered factor called ‘gdp_level’ with three levels called “low”, “medium”, and “high” with cut-off points of your own choosing. Produce a tabulation for ‘gdp_level’. Graphs R is probably the most powerful statistics programme for creating graphs. As this is an introductory level module, and we only have so much time available in the seminars, I will only be able to introduce you to the most commonly used ones; in the first instance histograms and boxplots. I will then introduce you to the package ggplot2 which is simply the best invention since sliced bread, as it gives you pretty much endless optionality in customising graphs to show exactly what you want. Whenever you produce a graph and you use it in an essay, your dissertation, or article, it is crucial that the graph is able to communicate its message independently from the text. So, a reader should be able to understand the graph and be able to appreciate fully its message without having to read the text. In a similar fashion, the text should always be written in such a way that a reader is able to understand it without having to look at the graph. This is a principle which equally applies to tables (more on this on PO12Q). If you do not follow this principle in the assessments on my modules, you will be marked down. Chapter 4 in “The Visual Display of Quantitative Information” by Tufte (2001) is on the reading list as an essential item, but there are some more principles he sets out at the start of the book (p. 13) which are worthwhile repeating here: Excellence in statistical graphics consists of complex ideas communicated with clarity, precision, and efficiency. Graphical displays should show the data induce the viewer to think about the substance rather than about methodology, graphic design, the technology of graphic production, or something else avoid distorting what the data have to say present many numbers in a small space make large data sets coherent encourage the eye to compare different pieces of data reveal the data at several levels of detail, from a broad overview to the fine structure serve a reasonably clear purpose: description, exploration, tabulation, or decoration be closely integrated with the statistical and verbal descriptions of a data set. Graphics reveal data. Indeed graphics can be more precise and revealing than conventional statistical computations. Basic Graphs Let’s start with a histogram of the variable pop18. The range of the pop18 variable is about 82 million - this is rather unwieldy to imagine and also to put onto axes of graphs, as they would mostly consist of zeros. So let’s express the population of each countries in million instead: EU$popmio &lt;- EU$pop18/1000000 We can now produce a histogram. Before doing this, it is sensible to think about the number of bars we want in the histogram. The smallest country has just shy of 500,000 inhabitants, whereas the largest has over 82 million. So, I would like the x-axis to run from zero to 100 (million) and divide this into 5 bars. Accordingly, we are introducing 4 breaks on the x-axis with the following command: hist(EU$popmio, breaks=4) This is certainly a histogram, but it does not conform to the principle of graphs that they should be able to communicate their message independently, yet. Take the label of the x-axis, for example, what does EU$popmio mean? You and I know, but somebody who doesn’t know R language wouldn’t. We can tell R to adjust the axis label, as well as the main title of the histogram as follows: hist(EU$popmio, breaks=4, xlab=&quot;Population in million&quot;, main=&quot;Histogram of EU Population (2018)&quot;) Histograms This is fine now. Ugly, but fine. I will show you how to do a boxplot next, and then I will take you through the process of making all this look a bit more jazzy. The command for the boxplot is very intuitive. The default in R is to arrange the boxplot vertically. I prefer them horizontally, and you can set this in an equally intuitive option. boxplot(EU$popmio, horizontal = TRUE) You will recognise the descriptives we calculated earlier with the summary() function: summary(EU$popmio) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.4757 3.7814 9.3003 18.3111 17.7667 82.8500 Explain the outliers on the right mathematically. Advanced Graphs The graphs we have produced so far are functional, but let’s be honest, they wouldn’t win any beauty contests. There is, as mentioned earlier, an amazing package called ggplot2 which changes this dramatically. You have already installed it as a part of the tidyverse. Otherwise, the function would be: install.packages(&quot;ggplot2&quot;) We can just load it: library(ggplot2) The “gg” in ggplot2 stands for “grammar of graphs”. You will be familiar with the term “grammar”” from learning a language already. In this context, we use grammar to build sentences by choosing and arranging a variety different components, such as subjects, verbs and objects. If you know how to do this properly, you can express exactly what you want to say. The grammar of graphs adopts this logic and specifies a number of different components which allow you to create a graph which is able to communicate exactly what you wish to show. ggplot2 has eight basic grammatical arguments: Table 2: GGPLOT Components Data Frame The data you wish to visualize. Aesthetic Mappings Here you specify how the data are assigned to colour, size, etc. For now, this is the variable for which we want to create a graphical distribution. Geom Short for “geometry”. Use a geom function to represent data points through geometric objects, such as points, lines, etc. Each function returns a layer. Stat You can include statistical summaries through this, such as smoothing, or regression lines. Position Position adjustments determine how to arrange geoms that would otherwise occupy the same space. Facets Facets divide a plot into sub-plots based on the values of one or more discrete variables. Scale Maps data values to the visual values of an aesthetic. For example female=pink, male=blue. Coordinates How do the numbers get translated onto the plot? We are not going to look at this on this module. I like to think of using these arguments like dressing myself in the morning. The minimum that common decency requires me to wear if I wish to leave the house is some underwear, some trousers, and a top. Depending on how I feel and what the weather is like, I can add more layers, like socks, a jumper, or a scarf. It is exactly the same with ggplot2. As a minimum to produce a plot you need a data frame, the aesthetic mapping and a geom. Once you have produced this minimalistic graph, you can modify it, by adding more components / arguments. As you can imagine the possibilities are almost endless, and we only have time to deal with the minimum here. This is not a problem, however, as most of the other grammatical arguments (Stats, Position, Facets and Scales) generally have sensible defaults. So how does this work in practice? Let us reproduce the histogram of the age variable. We start by calling ggplot2 and advise the function which data frame we wish to use (EU). In a second step, we add a geometry – in our case geom_histogram. Within the geometry, we need to specify for which variable we wish to create a distribution, or in the language of ggplot2 which variable we wish to map to the geom as our Aesthetic. To produce 5 bars again, we specify a bandwith of 20 million (this refers to popmio). ggplot(data=EU) + geom_histogram(mapping=aes(popmio), binwidth = 20) Annoyingly, ggplot places the axis ticks in the middle of each bar which is WRONG for histograms. They need to align with the boundaries of the bars. We do this by telling R the boundary of the plot: ggplot(data=EU) + geom_histogram(mapping=aes(popmio), binwidth = 20, boundary = 0) This has shifted the ticks to the left, but now R has decided to label the x-axis in steps of 25, whereas our bars have a bandwidth of 20. Once again, we have the variable name on the x-axis, instead of a label which anybody could understand. I also prefer “Frequency” on the y-axis, instead of “Count”. To address both of these concerns, we simply add a layer for each. First up are the axis ticks. Our variable is continuous, so we choose the scale_x_continuous option, and tell R to break the axis up into a sequence which starts with zero, ends at 100 and has steps of 20 in between. In the labs argument we adjust the labelling as intended: ggplot(data=EU) + geom_histogram(mapping=aes(popmio), binwidth = 20, boundary = 0) + scale_x_continuous(breaks = seq(0, 100, 20)) + labs(x=&quot;Population (in million)&quot;, y=&quot;Frequency&quot;) + theme_classic() I have also removed the background in line with the principles set out by Tufte (2001, p. 96) by adding theme_classic(). This is it. A graph which can communicate its message independently, and which looks aesthetically pleasing. In the present case we have the population, so displaying the frequency on the y-axis is sort of sensible, but usually we would be dealing with a sample. Here the count is not very telling and we would be using percentages, instead. Let’s do it! Even More Advanced Graphs Unfortunately, there is no easy, default way to do this in R, but necessitates a calculation within the ggplot command. Once more we call ggplot and use the EU data set, and select the geom geom_histogram. Again we specify the binwidth as 20 with a boundary of zero, and put popmio on the x-axis. Now comes the point where we need to do something new, because y is not equivalent to the frequency any more, but should be percentage. To achieve this we advise R to put the density there (which is the relative frequency from the the lecture in week 5), and multiply this density by 100 to get percentage. Nothing has changed on the scaling of the x-axis from the previous plot, so we can copy and paste the scale_x_continuous section, as well as the labelling of the x-axis. In this last step, we now also need to adjust the label of the y-axis, because this has now percentage on it, and not frequency. The result is this: ggplot(data = EU) + geom_histogram(binwidth = 20, boundary = 0, aes(x= popmio, y = (..count..)/sum(..count..)*100)) + scale_x_continuous(breaks = seq(0, 100, 20)) + labs(x=&quot;Population (in million)&quot;, y=&quot;percent&quot;) + theme_classic() Jazzy Graphs with GGPLOT Organising Code in the RScript Now is probably a good time to make you aware of how I have been organising code which runs over several lines. I could also have written the code of the last graph as ggplot(data = EU) + geom_histogram(binwidth = 20, boundary = 0, aes(x= popmio, y = (..count..)/sum(..count..)*100)) + scale_x_continuous(breaks = seq(0, 100, 20)) + labs(x=&quot;Population&quot;, y=&quot;percent&quot;) + theme_classic() but this would have made it rather difficult to disentangle and to spot the structure of the graph straight away. So it is also a good idea to structure the code in a logical way which allows a reader to understand it as easily as possible. R is very smart in the way it indents the next line after pressing “enter” in an RScript automatically to the appropriate position. You see for example that in ggplot(data = EU) + geom_histogram(binwidth = 20, boundary = 0, aes(x= popmio, y = (..count..)/sum(..count..)*100)) + scale_x_continuous(breaks = seq(0, 100, 20)) + labs(x=&quot;Population (in million)&quot;, y=&quot;percent&quot;) + theme_classic() the aes which belongs to the geom_histogram layer is indented just so it starts flush with the first argument (binwidth) within this layer. Exercises Using these commands, and moving beyond with the help of today’s reading, complete the following tasks: Produce two base-R graphs of different types (e.g. histogram, bar chart, box-and-whisker plot) for separate variables in the EU data set. Produce two ggplot graphs of different types (e.g. histogram, bar chart, box-and-whisker plot) for separate variables in the EU data set. Google to find more geoms. Captions for Tables and Figures in Word In essays, your dissertation and in articles, you will have to refer to tables and figures in the text. Now, you can do this by writing “the figure below”. But this is not very elegant. Also, what happens if you change the layout and all of a sudden “the figure below” becomes “the figure above”. This not only causes additional work because you have to edit the text and check all references to tables and figures once you are done (which is tedious beyond description), but there is also the risk that you miss one or a few in the process. MS Word has a nifty function that allows you to insert captions for figures and tables, and then to insert cross-references into the text which get updated automatically before you send the document to the printer. Here is how to do it: Say, you have a figure inserted into Word. You now click on it, then hover over the bottom right-hand square, and right-click with your mouse. From the resulting context menu you select “Insert Caption”. This results in the following window: Select whether the item you want to describe is a figure, or a table. Then make sure you place the caption “below” the item (this is default). Then type your caption into the box at the top, such as “Figure 1: Skewness of Distributions”. Make sure the caption is telling. The reader needs to know from the caption what the figure or table is about. When you click OK, the document looks like this: Now you start writing the text and come to the point where you refer to the figure in question. Here, all you have to do is to select “Insert” and “Cross-Reference”” and select the following options in the pop-up window: Your text will then look like this: You don’t have to worry now about the sequence of numbering any more. If you insert another figure above this one, and insert a cross-reference in the text again, the sequence is automatically updated and our former “Figure 1” becomes “Figure 2”. Tables and figures have separate sequences of numbering. One last word on the display of data in tables: DO NOT screenshot tables from R and insert them into your presentations. They look ugly and unprofessional. Make the effort and create a proper table, either in Word or Excel and populate it manually with the data from R. The insertion of captions and cross-references is the same as described above. https://cran.r-project.org/web/packages/↩︎ There is a command called order(), but it is not part of the tidyverse, and as this package is steadily on the rise in coding, I am only showing you this here.↩︎ "],["exercises-2.html", "Exercises Exploring the ESS – Core Exercises Exploring the ESS – Going Further", " Exercises Exploring the ESS – Core Exercises Open the ESS9 dataset in R and attach it. Identify the variable about “Left-right placement”. Check its formatting in the questionnaire: what do 77, 88 and 99 mean? What is the level of this variable? Summarise it with functions class, str, and head What do these functions return? Apply the same steps to the variable about the “European unification:”should go further or already gone too far”. Them display the first 20 values of the two variables above Check the structure of the ‘table’ function and tabulate the two variables Can you see a major difference between them regarding non positive answers (ie outside proposed scale)? Consider the function rm(list=ls()) What function rm stand for? How useful can this function be for future exercises? Univariate Statistics and Recoding Calculate the two means, using only valid values (check the mean function beforehand) In Left-right, regroup values, using three different methods: recode convert to factor format using as.factor assign value labels using levels 0-1 into “far left” 2-3 into “left”, 4-6 into “centre”, 7-8 into “right” 9-10 into “far right” Exploring the ESS – Going Further Using the new, transformed variables: Calculate means in the UK only. You may use the [variable == \"value\"] subscript How does this mean compare with the average in Europe? and with France? Present the frequencies and means of these three samples in one unique table Install and load the questionr package. Use its function ‘freq’ to calculate the same as above, but with percentages. Compare deviations from the mean in the UK and Germany using the function abs (absolute value, which means any value with all negative signs deleted) Compare mean, median, mode, variance and standard deviation in the UK and Germany Try 3 or more kinds or graphs with these two variables separately, using the most appropriate of original or transformed values. Assess and compare the relevance of each graph Tabulate the two variables in original format against each other using table(X,Y). Interpret the output. Repeat this with the transformed format: Interpret the output. Try the same using questionr::cprop Graph the two variables against each other using ‘plot’ (make sure you choose the right versions of the variables). What can you conclude from this graph? "],["homework-for-week-3.html", "Homework for Week 3", " Homework for Week 3 Finish working through this worksheet. Add a note underneath each code chunk in your RScript (by starting the line with #), translating the code into plain English. Read the required literature for week 3. Work through this week’s flashcards to familiarise yourself with the relevant R functions. Find an example for each NEW function and apply it in R to ensure it works. "],["glossary-1.html", "Glossary", " Glossary Table 3: Glossary Week 2 Term Description attribute A component or characteristic of a concept background concept The broad constellation of meanings and understandings associated with the concept (Adcock &amp; Collier, 2001, p. 531) concept Abstract ideas which form the building blocks of theories (Clark et al., 2021, p. 150) conceptualization Formulating a systematized concept through reasoning about the background concept, in light of the goals of research (Adcock &amp; Collier, 2001, p. 531) measurement Refers to the selection of a measure or variable reliability Refers to the extent to which repeated measurement produces the same results systematized concept A specific formulation of a concept used by a given scholar or group of scholars; commonly involves an explicit definition (Adcock &amp; Collier, 2001, p. 531) validity The extent to which the measure (variable) you choose genuinely represents the concept in question. The word comes from the Latin word “validus” which means “strong” variable An element of a conceptual component which varies. We also call these “measures” "],["flashcards-1.html", "Flashcards", " Flashcards New Functions This Week The data are available as a .csv file. R Functions This Week The data are available as a .csv file. All R Functions So Far The data are available as a .csv file. "],["foundations-of-statistical-inference.html", "Foundations of Statistical Inference Self-Assessment Questions6 How to read a z-table", " Foundations of Statistical Inference Self-Assessment Questions6 What is a sample distribution? How does the sample distribution differ from the sampling distribution? Why do we need the sampling distribution? What does the central limit theorem postulate, and why do we need it? What is the difference between the normal distribution and the t-distribution? Please stop here and don’t go beyond this point until we have compared notes on your answers. How to read a z-table In the lecture you have learned about the Normal Distribution. Under the Normal Distribution, the area under the curve is determined by the number of standard deviations around our mean \\(\\mu\\). This number is expressed in the form of the z-score which is defined as: \\[\\begin{equation} z=\\frac{\\text{Observation} - \\text{Mean}}{\\text{Standard Deviation}}=\\frac{y-\\mu}{\\sigma} \\end{equation}\\] To put this in words, z takes the difference between a particular value we are interested in and the mean. It then divides this distance by the standard deviation, in order to express the distance in units of standard deviations. Why do we do this? We know that under the Normal Distribution the area of the interval mean \\(\\pm\\) one standard deviation is equal to 68%. This is equivalent to the blue area in Figure 5. This also means that the remaining white area is equal to 32%, or the white section on each side 16%. Figure 5: Area under the Normal Distribution Imagine now, we took the point of minus one standard deviation as a starting point, and turn right, as in Figure 6. The white area is still 16%, so that the blue area needs to be 84%. Figure 6: Right-Tail Probability So the probability of finding a value larger than what is equivalent to minus one standard deviation is 84%. We call this a right-tail probability. The beauty is that we can do this for any point on the x-axis. Once we know how many standard deviations a value is removed from the mean, we can use the right-tail probability to assess how likely a value higher (or lower) than this value is to occur. The number of standard deviations is the z-score. Every z-score has a right-tail probability associated with it. These probabilities are listed in the Normal Table (see Moodle). How do we read this Table? Let me take you through the example used in the lecture once more. We assumed that the voter turnout rates of the 2019 European Elections for 28 countries was normally distributed, with \\(\\mu=50.66\\) and \\(\\sigma=16.56\\). You can see this distribution visualised in Figure 7. Figure 7: Voter Turnout in the 2017 European Elections (n=28) The question then was how likely it was for the EU to achieve a voter turnout larger than the voter turnout in the last General Election in the UK (68.8). If we wanted to visualise this, we would need the area to the right of 68.8 on the x-axis. This would look like this: Figure 8: Probability of Voter Turnout &gt; 68.8 In order to calculate the size of this area, we first took the difference between 68.8 and 50.66 which is 18.14. We then divided 18.14 by the standard deviation of 16.56, to express the distance in units of the standard deviation. The result is 1.095411. We know, therefore, that the point of 68.8 percent voter turnout on the x-axis is located 1.095411 standard deviations to the right of the mean. We now need to find the right-tail probability that belongs to this value. In the left-most column of the Normal Table you find the z-values with the first decimal place. Move down to 1.1. From here you turn right, until you hit the second decimal place. As our value is 1.10 you will only have to go one column to the right. If the value was 1.11, you would have to go two columns to the right. For a z-score of 1.1 the area is 0.1357, or 13.57%. We can therefore say that with a voter turnout of 50.66 and a standard deviation of 16.56, the probability of achieving a voter turnout higher than 68.6 is 13.57%. Before moving on, I need to note that z can be negative. If we were assessing the probability of achieving a voter turnout of less than 32.52% (50.66-18.14), we would get a z-score of -1.1. Because the Normal Distribution is symmetrical, we can use the same process, but need to reverse the logic. Because the right tail probability gives us the area to the right of the z-score, a negative z-score would give us the area to the left of the z-score. So the probability of a voter turnout smaller than 32.52 is also 13.57%. With this knowledge at hand, let’s do some exercises. Some of the content of this worksheet is taken from Reiche (forthcoming).↩︎ "],["exercises-3.html", "Exercises7 Conceptual Exercises R Exercises - Core R Exercises - Going Further Solutions", " Exercises7 Conceptual Exercises The mean weight of a bag of apples is 1 kg. The weight of bags is normally distributed around this mean with a standard deviation of 50g. Billy is looking for the heaviest bag possible and finds one that is 1082 g. What is the probability of finding a heavier bag? What is the probability that Billy will find a bag lighter than 870g? How would the results of a. and b. change if the standard deviation was only 40g? Why? Which z-value gives you a right-tail probability of 2.5%? With a z-value of 1.13 what is the right-tail probability in %? The average mark on a fictitious module was 62.3 with a standard deviation of 8.5. What was the probability to fail the module (pass-mark=50)? R Exercises - Core Open the European Social Survey data, but this time use Wave 7 (2014) which you need to download from the ESS website. Name it conveniently and attach it. Univariate Statistics Identify height and weight variables Use an appropriate function to summarise them in a few lines Plot the two variables separately as boxplots Plot the two variables separately as histograms using function ‘hist’ Adjust the bars to 5 cm and 5 kg (or as close to these numbers as you can) Discuss the advantage of various break values What are weight’s and height’s modes? Plot the same using the ‘density’ of each variable (density is a function that smoothes a distribution) Calculate relevant central values for both variables. Do they match your intuition? Sampling Execute the following three times, then explain what function ‘sample’ is: sample(1:20,5) Execute the following three times, then explain what function ‘set.seed’ is: set.seed(1) sample(1:20,5) Select a random sample of 100 from the ESS data frame, using “set.seed(1)”. Sampling Distributions Generate a vector “w” containing 10000 normally distributed numbers with mean 0 and standard deviation 1. Convert the vector into a data frame “dfw”. Using ggplot, plot a histogram of variable w (hint: use “after_stat(density)”), and overlay the histogram with a red density curve. Label the graph appropriately. Create an empty vector, called “n30”. Write a function which draws 9000 samples of size 30 from variable w and stores the mean of each sample in vector “n30”. Convert the vector “n30” into a data frame. Using ggplot, plot a histogram of variable n30 (hint: use “after_stat(density)”), and overlay the histogram with a red density curve. Label the graph appropriately. Plot the two graphs we just created in a grid with two columns and 1 row. (Hint: use package ‘gridExtra’) Interpret the juxtaposition of these graphs. R Exercises - Going Further Complete Section 4 of the Core Exercises. Repeat steps a-d for the following distributions: A uniform distribution with min = 0, max = 1. A beta distribution, Beta(2,5). What does the beta distribution represent? Create a graph in which each distribution type has a new row, and population and sampling distributions are placed next to each other. Solutions You can find the Solutions in the Downloads Section. Some of these are taken from Reiche (forthcoming).↩︎ "],["homework-for-week-4.html", "Homework for Week 4", " Homework for Week 4 Finish working through this worksheet. Add a note underneath each code chunk in your RScript (by starting the line with #), translating the code into plain English. Read the required literature for week 4. Work through this week’s flashcards to familiarise yourself with the relevant R functions. Find an example for each NEW function and apply it in R to ensure it works. "],["glossary-2.html", "Glossary", " Glossary Table 4: Glossary Week 3 Term Description conflation A variable does not belong to the attribute in question, but to a different one (Munck &amp; Verkuilen, 2002, pp. 13–14) dependent variable Is dependent through some statistical or stochastic process on the value of an independent variable democracy A system in which the population chooses and holds accountable elected representatives through fair, free, and contested, multi-party elections. The human rights, civil rights, and civil liberties of individuals are protected by law independent variable Influences or helps us predict the level of a dependent variable. It is often treated as fixed, or “given” in statistical analysis, and is sometimes also called “explanatory variable” redundancy Two or more variables measure the same sub-attribute (Munck &amp; Verkuilen, 2002, p. 13) "],["flashcards-2.html", "Flashcards", " Flashcards New Functions This Week The data are available as a .csv file. R Functions This Week The data are available as a .csv file. All R Functions So Far The data are available as a .csv file. "],["confidence-intervals.html", "Confidence Intervals Self-Assessment Questions8 Calculating Confidence Intervals", " Confidence Intervals Self-Assessment Questions8 In your own words: What does a significance test do? Give an example of a significance test. Give an example of a hypothesis that is not testable. Explain the difference between a significance test and a confidence interval. Explain the relationship between the size of the p-value and the Type I and II Errors. Please stop here and don’t go beyond this point until we have compared notes on your answers. Calculating Confidence Intervals Just as a reminder, what is a confidence interval? Confidence Interval A confidence Interval for a parameter is an interval of numbers within which the parameter is believed to fall. The probability that this method produces an interval that contains the parameter is called the confidence level. This is a number chosen to be close to 1, such as 0.95 or 0.99. (Agresti, 2018, p. 110) There are two scenarios under which we are calculating confidence intervals: (1) we know \\(\\sigma\\) (the standard deviation of the population distribution), and (2) we don’t know \\(\\sigma\\). Let me take you through these two scenarios in turn: 1. We know \\(\\sigma\\) If we know the population distribution, then we can use z. Assume we have sample data on the age of students in the university with n=81, and \\(\\bar{y}=26\\). The standard deviation of the population (\\(\\sigma\\)) is 9. We now want an interval within which we find with 99% certainty the true average age of students (we did this example with 95% in the lecture). We start with calculating the standard error: \\[\\begin{equation*} \\sigma_{\\bar{y}} = \\frac{\\sigma}{\\sqrt{n}} \\end{equation*}\\] \\[\\begin{equation*} \\sigma_{\\bar{y}} = \\frac{9}{\\sqrt{81}} = 1 \\end{equation*}\\] What is important to bear in mind for the construction of confidence intervals, is that we do not just need the right-tail probability, because the area we are trying to cover under the distribution is symmetrical around the mean. This is visualised in Figure 9 where the area is defined between \\(\\pm\\) 1.96 standard deviations around the mean. Figure 9: 95 Percent Confidence Interval around the Mean As the grey area needs to be 95%, the white areas to the left and right need to be equal to 5% jointly. So each of them is 2.5%. When we look into our Table with right-tail probabilities, we therefore need to look for the z-score that corresponds to 0.025 (or 2.5%). When you look in the Normal Table (see Statistical Tables), then you will find this at z=1.96. Now, the question arises, how many standard deviations we need for a 99% confidence interval. The area to the left and right needs to be jointly 1%, or 0.05% each side. We consult the Normal Table again, and try to find the z-score for 0.005. The exact value is not available, only either 0.0051, or 0.0049. 0.0051 would lead to a confidence interval of 98.98%, so not quite large enough. We therefore need to go for 0.0049, and the corresponding z-score of z=2.58. \\[\\begin{equation*} \\bar{y} \\pm 2.58 \\times \\sigma_{\\bar{Y}} \\end{equation*}\\] Popping the values in we receive \\[\\begin{equation*} 26 \\pm 2.58 \\times 1 \\end{equation*}\\] As a result, we can say that with 99% certainty the true average age of students at Warwick lies between 23.42 and 28.58. Note that this confidence interval is wider than the 95% one where the boundaries were defined as 24.04 and 27.96. As we went for higher certainty here, the confidence interval became wider. If we want our interval to contain the true average in 99 out of 100 samples, we need to cast our net wider than if we were content with 95. 2. We don’t know \\(\\sigma\\) If we don’t know the population distribution and its standard deviation, we need to use the t-distribution. As you know from the lecture, the t-distribution is a shape shifter. Its width depends on the degrees of freedom: the more degrees of freedom we have, the more narrow, or the closer to the normal distribution it comes. With df=30 the shapes of the t-distribution and the normal distribution are almost identical. You can see this in the following Figure, comparing the yellow t-distribution for \\(df=30\\) and the black (normal) distribution. Figure 10: Comparison of t-Distributions In reversed logic, this also means that the t-distribution is rather wide for small sample sizes (and small df). Consequently, a confidence interval of a given level (e.g. 99%) would be much wider for a small sample size than under the normal distribution (or t-distributions with higher sample sizes). Let me illustrate this using the same sample average as for the normal distribution example above, \\(\\bar{y}=26\\). We have a sample standard deviation (\\(s\\)) of 2. Our sample size is very small, we only have \\(n=4\\). Again, we want an interval within which we find with 99% certainty the true average age of students. We start by estimating the standard error: \\[\\begin{equation*} se=\\frac{s}{\\sqrt{n}} \\end{equation*}\\] \\[\\begin{equation*} se=\\frac{2}{\\sqrt{4}}=1 \\end{equation*}\\] This time we have to search in the t-Table, taking into account the degrees of freedom. As \\(n=4\\), this means that \\(df=3\\). Conveniently, the Table lists at the top the desired confidence interval. For \\(df=3\\) and a 99% confidence interval, the corresponding t-value is 5.841. Once again, we calculate: \\[\\begin{equation*} \\bar{y} \\pm 5.841 \\times se \\end{equation*}\\] and \\[\\begin{equation*} 26 \\pm 5.841 \\times 1 \\end{equation*}\\] The resulting boundaries are 20.159 as the lower boundary, and 31.841 as the upper boundary. This is far wider than the 99% confidence interval we had for the normal distribution, where the lower and upper boundaries were 23.42 and 28.58, respectively. This is a reflection of the fact that we only had a very small sample (\\(n=4\\)) to base our inference on and therefore have a lot of uncertainty in our inference. To conclude, it is fair to say that the procedure is essentially the same as with the normal procedure, only that have to go the extra step of taking into account the degrees of freedom. You are now ready to do some exercises. Some of the content of this worksheet is taken from Reiche (forthcoming).↩︎ "],["exercises-4.html", "Exercises Conceptual Exercises9 R Exercises - Core R Exercises - Going Further Solutions", " Exercises Conceptual Exercises9 A researcher is analysing individuals’ relative fear of being a victim of burglary on a 1-100 scale. A random sample of 9 individuals found a mean score of 47 on the scale with a sample variance of 158.76 for fear of being burgled. What distribution would be used to calculate an 80% confidence interval around this mean? Construct that interval. We are investigating the height of men in the UK. For this we have obtained a random sample of 100 UK men and found they had a mean height of 180cm with a standard deviation of 10cm. Construct a 95% confidence interval for the mean height of UK males. Select all true statements concerning the constructed confidence interval and justify your choice for each statement. The probability of the population mean being within the upper and lower bounds is 95%. 95% of men’s heights fall between the upper and lower bound. 95% of the cases in the sample fall between the upper and lower bound. On average 95% of confidence intervals constructed would contain the population mean. On average 95% of the means of samples with 100 respondents will fall within the upper and lower bands. R Exercises - Core These exercises will use the ks2 dataset. This data comprises fictitious10 average grades of Key Stage 2 (KS2) students in the UK, with 1,980 KS2 students’ test scores being included for reading (reading), mathematics (maths), and grammar, punctuation, and spelling (gps), as well as the mean of these three test scores (avg_all). The test scores have been standardised, with 80 representing the lowest possible mark, 120 representing the highest possible mark, 100 representing the minimum passing mark, and -1 representing an ungraded test. Load the ks2 dataset into R. Remove any observations that have any ungraded test scores. Calculate the means and standard deviations of each of the three subjects. Write a brief description of the insights that can be drawn from these values. Conduct a t-test to see if the means of each of the three subjects’ marks are statistically different from 100 at the 95% confidence level. Identify three ways that suggest statistically significant or insignificant differences. Conduct a t-test to see if the mean of the average score of the three tests is statistically less than 105 at the 99% confidence level. Interpret the results. The following questions will look at students’ English abilities generally. Create a new variable called english, which consists of the average of the reading and grammar, punctuation, and spelling variables. Conduct a t-test to see if the mean of the average score of the new English variable is statistically less than 105 at the 99.9% confidence level. Interpret the results. Conduct a t-test to see if the mean of the average score of the new English variable is statistically different from 105 at the 99.9% confidence level. Interpret the results. Is the any difference in the interpretation between the two above tests? Are there any differences in the results? Why? You are tasked with investing the performance of students who passed in mathematics those who did not. For the following tests, use a 95% confidence level. Create a binary variable that has two categories: those who passed mathematics (100 \\(\\leq\\) mark) and those who failed mathematics (mark \\(&lt;\\) 100). Conduct a proportion test to see if the proportion of students that fail mathematics is 10% or greater. Interpret the results. Conduct a t-test on the group who fail mathematics to see if they, on average, have marks for English statistically less than 100. Interpret the results. Conduct a t-test on the group who pass mathematics to see if they, on average, have marks for grammar, spelling, and punctuation statistically greater than 105. Interpret the results. Now it is worth investigating how students who fail at least one subject perform. Create a binary variable that has two categories: those who passed all three subjects (all marks greater than or equal to 100) and those who failed at least one subject (one or more marks less than 100). It can be hypothesised that the group of students who failed will have a mean of all of the test marks significantly below the pass mark of 100. Test this and interpret the findings with respect to the statistical and practical significance of the test. R Exercises - Going Further Imagine you are part of a team work working within the Department for Education, tasked with investigating this sample to produce recommendations for policymakers. Normalise the variable that contains the average of all three marks by setting the lowest mark (80) to 0, the highest mark (120) to 100, and the minimum pass mark (100) to 50. Justify why it might be useful to normalise these marks to this scale for non-specialist policymakers. Construct a categorical variable that consists of five categories: 0-49.99 (Fail), 50-59.99 (Pass), 60-60.99 (Merit), 70-79.99 (Distinction), and 80+ (Distinction+). Answer the following questions but write your answers as if intended for a non-specialist policy making with no knowledge of statistics: Are the averages of the Pass, Merit, and Distinction groups different from their middle marks (55, 65, and 75, respectively)? If so, which direction? Is the average mark of the Distinction+ group lower than the maximum mark of the group? Is the mean mark of the Fail group higher than the median mark of the group? Which skew does this indicate in the distribution? Solutions You can find the Solutions in the Downloads Section. These are taken from Reiche (forthcoming).↩︎ The means are based on KS2 scaled score averages which have been averaged over 2016-2019.↩︎ "],["homework-for-week-5.html", "Homework for Week 5", " Homework for Week 5 Finish working through this worksheet. Add a note underneath each code chunk in your RScript (by starting the line with #), translating the code into plain English. Read the required literature for week 5. Work through this week’s flashcards to familiarise yourself with the relevant R functions. Find an example for each NEW function and apply it in R to ensure it works. "],["glossary-3.html", "Glossary", " Glossary Table 5: Glossary Week 4 Term Description cross-sectional data Look at different units (or cross-sections) \\(i\\) at a single point in time data set A collection of numerical values for individual observations, separated into distinctive variables descriptive statistics Summarise information about the centre and variability of a variable deviation The deviation \\(d\\) of an observation \\(y_{i}\\) from the sample mean \\(\\bar{y}\\) is the difference between them: \\(d=y_{i}-\\bar{y}\\) interquartile range The difference between the 3\\(^{\\text{rd}}\\) and the 1\\(^{\\text{st}}\\) quartiles mean Is equal to the sum of the observations divided by the number of observations median Separates the lower half from the upper half of observations mode Is the most frequently occurring value percentile In ordered data, the percentile refers to the value of a variable below which a certain proportion of observations falls primary data Primary data are data you have collected yourself quartile Divides ordered data into four equal parts and indicates the percentage of observations that falls into the respective quartile and below range The difference between the largest and the smallest observation secondary data Secondary data are data which have been collected by somebody else standard deviation The standard deviation s is defined as \\[\\begin{equation*}s=\\sqrt{\\frac{\\text{sum of squared deviations}}{\\text{sample size} -1}}=\\sqrt{\\frac{\\Sigma(y_{i} - \\bar{y})^2}{n-1}}\\end{equation*}\\] variance Is equal to the squared standard deviation "],["flashcards-3.html", "Flashcards", " Flashcards New Functions This Week The data are available as a .csv file. R Functions This Week The data are available as a .csv file. All R Functions So Far The data are available as a .csv file. "],["causality-and-consolidation.html", "Causality and Consolidation Self-Assessment Questions11 Case Study Codebook Data Exploration Descriptive Statistics Visualisation Hypothesis Sampling Inferential Statistics Causality Solutions", " Causality and Consolidation Self-Assessment Questions11 Why is causality an important topic? Explain the role of symmetry and asymmetry in causality. Choosing one of the literature strands presented in the literature, explain its approach to causality. How does the issue of causality affect the language we use in research? Please stop here and don’t go beyond this point until we have compared notes on your answers. Case Study You will be working with the “Scottish Index of Multiple Deprivation”. The data are taken from https://www.gov.scot/collections/ The Scottish Index of Multiple Deprivation is a relative measure of deprivation across 6,976 small areas (called data zones). If an area is identified as ‘deprived’, this can relate to people having a low income but it can also mean fewer resources or opportunities. SIMD looks at the extent to which an area is deprived across seven domains: income, employment, education, health, access to services, crime and housing. SIMD is the Scottish Government’s standard approach to identify areas of multiple deprivation in Scotland. It can help improve understanding about the outcomes and circumstances of people living in the most deprived areas in Scotland. It can also allow effective targeting of policies and funding where the aim is to wholly or partly tackle or take account of area concentrations of multiple deprivation. The data set provided combines two waves of the SIMD, namely 2016: containing data collected between 2011 and 2014 2020: containing data collected between 2014 and 2018 Rather than presenting you with the data pertaining to individual data zones (n=6,976), I have chosen to aggregate these data zones to the corresponding councils, reducing the number of observations to 32. Codebook Table 6: SIMD Codebook variable label Council_area Council area name alc16 Hospital stays related to alcohol use: standardised ratio mortality16 Standardised mortality ratio (2011-2014) mortality20 Standardised mortality ratio (2014-2018) Data Exploration Before starting, we need to load libraries and install packages if not already installed. In these exercises we will be using the tidyverse package. Set your working directory, place the data set in it, and load it into R. Create a new RScript for this case study and annotate it as you go through the exercises presented here. Load the tidyverse package. Descriptive Statistics Produce descriptive statistics for all three numerical variables. Visualisation Let’s visualise the distribution of the variable alc16. Figure 11: Distribution of Alcohol-Related Hospital Admissions (2011-2014) Reproduce Figure 11. What does the distribution tell us about alcohol-related admissions to hospital? How does the shape of the distribution in Figure 11 relate to the descriptive statistics calculated in the previous Section? What would happen to the shape of the distribution if the median was smaller than the mean? Hypothesis We are interested in how alcohol-related admissions to hospital have affected mortality rates in Scotland. The following scatter plot uses the variables alc16 and mortality20. Figure 12: Alcohol-Related Hospital Admissions and Mortality Reproduce Figure 12. Based on this scatter plot, formulate the alternative and the null hypotheses:    H\\(\\pmb{_0}\\):    H\\(\\pmb{_\\text{A}}\\): Sampling The data frame which we have been using so far represents the population. Let us now draw a random sample of 15 councils as follows: set.seed(6) sample &lt;- sample_n(simd, 15) Explain the purpose of the set.seed function. Inferential Statistics Let us now see if the mortality ratio has changed between the two waves of 2016 and 2020. This is the worked example from the lecture, but I am repeating it here deliberately, so that you can carry out the example yourself. As a first step, create a new variable measuring the difference between mortality20 and mortality16. Make sure that increases are positive and decreases negative. What is the sample mean of the differences in mortality rates, variable diff? The sample size of 15 is small. Will it be appropriate to conduct a t-test? Why? Why not? Find out whether the difference in mortality rates is significantly different from zero. Draw a graph which depicts the direction of the alternative hypothesis and the p-value. Try not to look at the lecture slides. Suppose the Scottish Government claims that mortality rates have decreased. Test this claim. Again, draw a graph which depicts the direction of the alternative hypothesis and the p-value. Try not to look at the lecture slides. Drawing on the results from Exercises 5 and 7, reason about the p-value you would obtain if you tested the hypothesis that mortality rates have increased between the waves of 2016 and 2020. Causality Identify the elements of symmetry and asymmetry in the setup of this case study. Consider again Figure 13 from the lecture. Which aspects of establishing causality has the case study addressed? What is missing? Figure 13: Causality Framework Solutions You can find the Solutions in the Downloads Section. Some of the content of this worksheet is taken from Reiche (forthcoming).↩︎ "],["homework-for-week-7.html", "Homework for Week 7", " Homework for Week 7 Finish working through this worksheet. Add a note underneath each code chunk in your RScript (by starting the line with #), translating the code into plain English. Read the required literature for week 7. Work through this week’s flashcards to familiarise yourself with the relevant R functions. Find an example for each NEW function and apply it in R to ensure it works. Work through the Reading Week Taks. Before coming to the seminar, I encourage you to work through the Methods, Methods, Methods Section of Week 7. This will take you through the application of cross-tabulations in R. "],["glossary-4.html", "Glossary", " Glossary Table 7: Glossary Week 5 Term Description categorical Describing the qualitative categories of a characteristic, for example different religions constant A variable which does not vary continuous Can assume any value within defined measurement boundaries dichotomous Can only assume two mutually exclusive, but internally homogeneous qualitative categories discrete The result of a counting process distribution Refers to the display of the values a variable can assume, together with their respective absolute or relative frequency histogram Displays through rectangles the frequency with which the values of a continuous variable occur in specific ranges outlier Defined as a value larger than the third quartile plus 1.5 times the interquartile range, or the first quartile minus 1.5 times the interquartile range "],["flashcards-4.html", "Flashcards", " Flashcards New Functions This Week The data are available as a .csv file. R Functions This Week The data are available as a .csv file. All R Functions So Far The data are available as a .csv file. "],["reading-week.html", "Reading Week", " Reading Week Reading week is not an institutionalised holiday, but I do expect you to put in about 10 hours of work for this module over the course of this week. Here are a few suggestions for activities to fill these 10 hours: Catch up on the reading Revise the material of Weeks 1-5, as we will switch to bivariate and multivariate analysis in Week 7. Go through the worksheets of Weeks 1-5 to make sure you are on top of things with R. Revise all R functions up to this point. I have created a Reading Week Consolidation Flashcard Section for this. "],["methods-methods-methods.html", "Methods, Methods, Methods Data Prep Video and RScript", " Methods, Methods, Methods In the coming weeks, you will find this section on the Companion to help you apply the material we cover in the lecture in R. It will contain an RScript for some preliminary data preparation. This is not an actual part of introducing the method, but you are certainly encouraged to read it and to try and understand it. These sections will work with the American National Election Studies (ANES), and to be more precise with the pilot survey conducted before the 2020 presidential election. You can download the required data set by following this link. The actual data set itself is available here. Data Prep Place the ANES data in a folder which you will be using as a working directory for this session. Open the “Code for Data Preparation” below, and copy this into an RScript. Remember to adjust the working directory with the setwd() command at the beginning. Then run the RScript and you will be ready to proceed to the video. Code for Data Preparation ###################################### # MMM - Week 1 - Data Preparation ###################################### # Set WD setwd() # Load packages library(tidyverse) # Load data set anes &lt;- read_csv(&quot;anes.csv&quot;) # Get rid of missing values for variables used in analysis today ## 999 is equivalent to NA, so needs to be recoded anes$fttrump1 &lt;- with(anes, replace(fttrump1, fttrump1 == 999, NA)) anes &lt;- filter(anes, !is.na(fttrump1), !is.na(sex)) # turn support for Trump `fttrump1` into ordered factor with three levels anes &lt;- anes %&gt;% mutate(trump= ordered( cut(fttrump1, breaks=c(0, 33, 66, 100), labels=c(&quot;low&quot;,&quot;medium&quot;,&quot;high&quot;)))) # Label variable `sex` anes$sex &lt;- factor(anes$sex) anes &lt;- anes %&gt;% mutate(sex= recode(sex,&quot;1&quot;=&quot;Male&quot;, &quot;2&quot;=&quot;Female&quot;)) # save data set for use in video write.csv(anes, &quot;anes_week1.csv&quot;) You can copy the code from this page by hovering over the code chunk and clicking the icon in the top-right hand corner. You can then paste it into your RScript. Video and RScript You can find the video introducing you to this week’s method by way of a worked example below. You can also access the code I am typing up in the video in the “Code for Data Analysis” section. I would encourage you to type it yourself, though, as then code tends to better sink into the depths of your brain 😉 Code for Data Analysis ###################################### # MMM - Week 1 - Crosstabulations ###################################### # Set WD setwd() # Load packages library(tidyverse) # Load data set anes &lt;- read_csv(&quot;anes_week1.csv&quot;) anes$sex &lt;- as.factor(anes$sex) anes$trump &lt;- as.factor(anes$trump) # Tabulate relationship between sex and support for Trump table(anes$sex,anes$trump) prop.table(table(anes$sex,anes$trump)) prop.table(table(anes$sex,anes$trump), margin = 1) # save table for analysis trumpsex &lt;- table(anes$sex,anes$trump) trumpsex xsq &lt;- chisq.test(trumpsex, correct=FALSE) xsq # display observed and expected values xsq$expected xsq$observed Cross Tabulations in R "],["bivariate-methods.html", "Bivariate Methods Self-Assessment Questions12 Calculations by Hand Applied Exercises in R (Core) Applied Exercises in R (Going Further) Solutions", " Bivariate Methods Self-Assessment Questions12 Why do we need a test of statistical significance? Given an example of a one-sided, and of a two-sided test of significance. Can we use continuous variables for cross-tabulations? Why is there no dependent and independent variable in correlation analysis? Consider the causality framework of Week 5. To what extent are (a) correlation and (b) cross-tabulations able to establish causality? Please stop here and don’t go beyond this point until we have compared notes on your answers. Calculations by Hand I have given you an example of a cross-tabulation in the lecture. Consider the following Table: Calculate the Expected Values and fill in the following table: Calculate the \\(\\chi^{2}\\)-value How many degrees of freedom does this table have? Why? Using the \\(\\chi^2\\) Table, what is the p-value? Are mode of transport and year of study independent in the population? Applied Exercises in R (Core) Open the European Social Survey data, wave 9. Name it conveniently and attach it Univariate Analysis (revising weeks 1-2) Identify and explore briefly variables about respondents’ happiness, subjective health, age and household’s income (we are going to look at their relationships later) What do the values of the income variable mean? What is the mode of ‘age’? Try to write a function which automatically determines the mode of a variable (more advanced, don’t worry if this beats you. We will discuss this in the seminar.) Check the purpose of function round (?round) and, if relevant, apply it appropriately to your results from now onward Calculate means, variances and standard deviations when relevant Find a suitable plotting function for all four variables Crosstabulations We suspect there is a relationship between health and happiness. State the null and alternative hypothesis. Take care to correctly specify the dependent and independent variable. Cross-tabulate health and happiness, using the table function (search with ‘?’ if needed) Apply the ‘addmargins’ function to the previous formula Can you read the results? Can you interpret them? Display a bivariate plot with the same (search with ‘?’ if needed). What kind of plot do you get? Does this help reading and interpreting? Convert the table into percentages. You can use function ‘cprop’ or ‘lprop’ from package ‘questionr’ (you can install the new package through the menu “Tools” in RStudio). Interpret the result. Improve the table as much as you can. Conduct a Chi2 test of significance and interpret the result. Reorder the levels of Health using the ‘factor’ function with levels argument. Then display the cross-tabulation again. Is the relationship still statistically significant? Correlation Calculate the Pearson correlation between health and happiness (you need to work only with the cases that have positive values for both variables, search the help file for this) Does the result confirm your previous conclusion? Is the correlation coefficient statistically significant? What does this mean? Further Application Form the null und alternative hypotheses for the relationship between happiness and household income happiness and age Repeat the crosstabulation and correlation exercises for these two pairs of variables to test your hypotheses. Applied Exercises in R (Going Further) There is a lot to choose from this week, take your pick. There is no need to do all of these. Aim for two sections. Section 1 Use the ‘prop.table’ function to look at happiness against health in a comparison between genders.Compare genders as precisely as possible. Does health drive happiness more for men or women? Identify the two countries with lowest and highest happiness Compare happiness distributions between these two countries What benefit do distributions have compared to just looking at means? Section 2 For this section use Wave 7 (2014). Name it conveniently and attach it. Calculate the Body Mass Index variable with BMI=weight/height. Compare mean and standard deviation of BMI between the UK and Spain Apply ‘t.test’ function to test the difference between the two means (note this is now a two-sample t-test) Have the inhabitants of one country a significantly better BMI? Do you know why? Section 3 For this section use Wave 7 (2014). Name it conveniently and attach it. Plot the age at which respondents completed full-time education against the same for their fathers (note that these variables are only valid for the UK sample). There might be outlying values – can you see them? Plot the same, excluding outlying values Is there a significant link between fathers and children regarding this variable? Is there a significant link between mothers and children? Are these links, if any, stronger for sons or for daughters? Can you interpret? Solutions You can find the Solutions in the Downloads Section. Some of the content of this worksheet is taken from Reiche (forthcoming).↩︎ "],["homework-for-week-8.html", "Homework for Week 8", " Homework for Week 8 Finish working through this worksheet. Add a note underneath each code chunk in your RScript (by starting the line with #), translating the code into plain English. Read the required literature for week 8. Work through this week’s flashcards to familiarise yourself with the relevant R functions. Find an example for each NEW function and apply it in R to ensure it works. Before coming to the seminar, I encourage you to work through the Methods, Methods, Methods Sections of Week 8, starting with Bivariate Regression (there are three different ones). This will take you through the relevant components of regression analysis in R. "],["glossary-5.html", "Glossary", " Glossary Table 8: Glossary Week 7 Term Description generalisability The ability to apply the findings made on the basis of a representative sample to the population non-probability sampling In non-probability sampling not every unit has the same probability of being sampled normal distribution The Normal Distribution is a bell-shaped probability distribution which is symmetrical around the mean parameter A parameter is the value a statistic would assume in the long run. It is also called the Expected Value population Collection of all cases which possess certain pre-defined characteristics population distribution The probability distribution of the population probability Refers to how many times out of a total number of cases a particular event occurs. We can also see it as the chance of a particular event occurring probability sampling In probability sampling all units have the same probability of entering the sample. In addition, all possible combinations of n cases must have the same probability to be selected representative sample A sample which contains all characteristic of the population in accurate proportions sample A sub-group of the population sample distribution The probability distribution of a sample sampling The process of selecting sampling units from the population sampling distribution The probability distribution of a sample statistics, such as the mean. It can be derived from repeated sampling, or by estimation sampling error The extent to which the mean of the population and the mean of the sample differ from one another sampling method The way the sample is created standard error The standard deviation of the sampling distribution. It is defined as:\\[\\begin{equation*}\\sigma_{\\bar{y}} = \\frac{\\sigma}{\\sqrt{n}}\\end{equation*}\\] z-score The z-score, sometimes also referred to as z-value, expresses in units of standard deviation how far an observation of interest falls away from the mean. It is defined as \\[\\begin{equation*}z = \\frac{\\text{observation} - \\text{mean}}{\\text{standard deviation}}=\\frac{y-\\mu}{\\sigma}\\end{equation*}\\] "],["flashcards-5.html", "Flashcards", " Flashcards New Functions This Week The data are available as a .csv file. R Functions This Week The data are available as a .csv file. All R Functions So Far The data are available as a .csv file. "],["mmm-bivariate-regression.html", "MMM – Bivariate Regression Data Prep Video and RScript", " MMM – Bivariate Regression This week we will be starting to conduct linear regression analysis in R. Just as in week 7 we will be working with the American National Election Studies (ANES), and to be more precise with the pilot survey conducted before the 2020 presidential election. If you haven’t already done so, you will have to register with ANES in order to download the data set. To do so, please follow this link. Data Prep Place the ANES data in a folder which you will be using as a working directory for this session. Open the “Code for Data Preparation” below, and copy this into an RScript. Remember to adjust the working directory with the setwd() command at the beginning. Then run the RScript and you will be ready to proceed to the video. Code for Data Preparation ###################################### # MMM - Week 4 - Data Preparation ###################################### # Set WD setwd() # Load packages library(tidyverse) # Load data set anes &lt;- read_csv(&quot;anes.csv&quot;) # Get rid of missing values for variables used in analysis today ## 999 is equivalent to NA, so needs to be recoded anes$fttrump1 &lt;- with(anes, replace(fttrump1, fttrump1 == 999, NA)) anes$income &lt;- with(anes, replace(income, income == 99, NA)) anes &lt;- filter(anes, !is.na(fttrump1), !is.na(age), !is.na(income)) # Turn income variable into a numerical variable with mid-points of each level anes$income &lt;- factor(anes$income) table(anes$income) anes &lt;- anes %&gt;% mutate(income_fac = recode(income, &#39;1&#39;= &quot;2500&quot;, &#39;2&#39;= &quot;7499.5&quot;, &#39;3&#39;= &quot;12499.5&quot;, &#39;4&#39;= &quot;17499.5&quot;, &#39;5&#39;= &quot;22499.5&quot;, &#39;6&#39;= &quot;27499.5&quot;, &#39;7&#39;= &quot;32499.5&quot;, &#39;8&#39;= &quot;37499.5&quot;, &#39;9&#39;= &quot;42499.5&quot;, &#39;10&#39;= &quot;47499.5&quot;, &#39;11&#39;= &quot;52499.5&quot;, &#39;12&#39;= &quot;57499.5&quot;, &#39;13&#39;= &quot;62499.5&quot;, &#39;14&#39;= &quot;67499.5&quot;, &#39;15&#39;= &quot;72499.5&quot;, &#39;16&#39;= &quot;77499.5&quot;, &#39;17&#39;= &quot;82499.5&quot;, &#39;18&#39;= &quot;87499.5&quot;, &#39;19&#39;= &quot;92499.5&quot;, &#39;20&#39;= &quot;97499.5&quot;, &#39;21&#39;= &quot;112499.5&quot;, &#39;22&#39;= &quot;137499.5&quot;, &#39;23&#39;= &quot;162499.5&quot;, &#39;24&#39;= &quot;187499.5&quot;, &#39;25&#39;= &quot;224999.5&quot;, &#39;26&#39;= &quot;500000&quot;)) anes$inc &lt;- as.numeric(as.character(anes$income_fac)) # save data set for use in video write.csv(anes, &quot;anes_week4.csv&quot;) Video and RScript You can find the video introducing you to this week’s method by way of a worked example below. You can also access the code I am typing up in the video in the “Code for Data Analysis” section. I would encourage you to type it yourself, though, as then code tends to better sink into the depths of your brain 😉 Code for Data Analysis ###################################### # MMM - Week 4 - Bivariate Regression ###################################### # Set WD setwd() # Load packages library(tidyverse) # Load data set anes &lt;- read_csv(&quot;anes_week4.csv&quot;) # Visualisation ############################ ggplot(anes, aes(x = inc, y = fttrump1)) + geom_point() + geom_smooth(method = lm) # Regression ############################ model1 &lt;- lm(fttrump1 ~ inc, data = anes) model1 Bivariate Regression in R "],["mmm-model-fit.html", "MMM – Model Fit Data Prep Video and RScript", " MMM – Model Fit In this Section we will explore a measure for model fit, the so-called R-Squared. Discover how to obtain it in R in this section. Just as in week 4 we will be working with the American National Election Studies (ANES), and to be more precise with the pilot survey conducted before the 2020 presidential election. If you haven’t already done so, you will have to register with ANES in order to download the data set. To do so, please follow this link. Data Prep Place the ANES data in a folder which you will be using as a working directory for this session. Open the “Code for Data Preparation” below, and copy this into an RScript. Remember to adjust the working directory with the setwd() command at the beginning. Then run the RScript and you will be ready to proceed to the video. Code for Data Preparation ###################################### # MMM - Week 5 - Data Preparation ###################################### # Set WD setwd() # Load packages library(tidyverse) # Load data set anes &lt;- read_csv(&quot;anes.csv&quot;) # Get rid of missing values for variables used in analysis today ## 999 is equivalent to NA, so needs to be recoded anes$fttrump1 &lt;- with(anes, replace(fttrump1, fttrump1 == 999, NA)) anes$income &lt;- with(anes, replace(income, income == 99, NA)) anes &lt;- filter(anes, !is.na(fttrump1), !is.na(age), !is.na(income)) # Turn income variable into a numerical variable with mid-points of each level anes$income &lt;- factor(anes$income) table(anes$income) anes &lt;- anes %&gt;% mutate(income_fac = recode(income, &#39;1&#39;= &quot;2500&quot;, &#39;2&#39;= &quot;7499.5&quot;, &#39;3&#39;= &quot;12499.5&quot;, &#39;4&#39;= &quot;17499.5&quot;, &#39;5&#39;= &quot;22499.5&quot;, &#39;6&#39;= &quot;27499.5&quot;, &#39;7&#39;= &quot;32499.5&quot;, &#39;8&#39;= &quot;37499.5&quot;, &#39;9&#39;= &quot;42499.5&quot;, &#39;10&#39;= &quot;47499.5&quot;, &#39;11&#39;= &quot;52499.5&quot;, &#39;12&#39;= &quot;57499.5&quot;, &#39;13&#39;= &quot;62499.5&quot;, &#39;14&#39;= &quot;67499.5&quot;, &#39;15&#39;= &quot;72499.5&quot;, &#39;16&#39;= &quot;77499.5&quot;, &#39;17&#39;= &quot;82499.5&quot;, &#39;18&#39;= &quot;87499.5&quot;, &#39;19&#39;= &quot;92499.5&quot;, &#39;20&#39;= &quot;97499.5&quot;, &#39;21&#39;= &quot;112499.5&quot;, &#39;22&#39;= &quot;137499.5&quot;, &#39;23&#39;= &quot;162499.5&quot;, &#39;24&#39;= &quot;187499.5&quot;, &#39;25&#39;= &quot;224999.5&quot;, &#39;26&#39;= &quot;500000&quot;)) anes$inc &lt;- as.numeric(as.character(anes$income_fac)) # save data set for use in video write.csv(anes, &quot;anes_week5.csv&quot;) Video and RScript You can find the video introducing you to this week’s method by way of a worked example below. You can also access the code I am typing up in the video in the “Code for Data Analysis” section. I would encourage you to type it yourself, though, as then code tends to better sink into the depths of your brain 😉 Code for Data Analysis ###################################### # MMM - Week 5 - Model Fit ###################################### # Set WD setwd() # Load packages library(tidyverse) # Load data set anes &lt;- read_csv(&quot;anes_week5.csv&quot;) # Visualisation ############################ ggplot(anes, aes(x = inc, y = fttrump1)) + geom_point() + geom_smooth(method = lm) # Regression ############################ model1 &lt;- lm(fttrump1 ~ inc, data = anes) model1 summary(model1) Model Fit for Regression in R "],["mmm-significance-testing.html", "MMM – Significance Testing Data Prep Video and RScript", " MMM – Significance Testing Now we will delve a little deeper into the output for linear regression analysis in R to ascertain whether our coefficients are actually statistically different from zero, or put less technically whether they have an influence. Just as last week we will be working with the American National Election Studies (ANES), and to be more precise with the pilot survey conducted before the 2020 presidential election. If you haven’t already done so, you will have to register with ANES in order to download the data set. To do so, please follow this link. Data Prep Place the ANES data in a folder which you will be using as a working directory for this session. Open the “Code for Data Preparation” below, and copy this into an RScript. Remember to adjust the working directory with the setwd() command at the beginning. Then run the RScript and you will be ready to proceed to the video. Code for Data Preparation ###################################### # MMM - Week 7 - Data Preparation ###################################### # Set WD setwd() # Load packages library(tidyverse) # Load data set anes &lt;- read_csv(&quot;anes.csv&quot;) # Get rid of missing values for variables used in analysis today ## 999 is equivalent to NA, so needs to be recoded anes$fttrump1 &lt;- with(anes, replace(fttrump1, fttrump1 == 999, NA)) anes$income &lt;- with(anes, replace(income, income == 99, NA)) anes &lt;- filter(anes, !is.na(fttrump1), !is.na(age), !is.na(income)) # Turn income variable into a numerical variable with mid-points of each level anes$income &lt;- factor(anes$income) table(anes$income) anes &lt;- anes %&gt;% mutate(income_fac = recode(income, &#39;1&#39;= &quot;2500&quot;, &#39;2&#39;= &quot;7499.5&quot;, &#39;3&#39;= &quot;12499.5&quot;, &#39;4&#39;= &quot;17499.5&quot;, &#39;5&#39;= &quot;22499.5&quot;, &#39;6&#39;= &quot;27499.5&quot;, &#39;7&#39;= &quot;32499.5&quot;, &#39;8&#39;= &quot;37499.5&quot;, &#39;9&#39;= &quot;42499.5&quot;, &#39;10&#39;= &quot;47499.5&quot;, &#39;11&#39;= &quot;52499.5&quot;, &#39;12&#39;= &quot;57499.5&quot;, &#39;13&#39;= &quot;62499.5&quot;, &#39;14&#39;= &quot;67499.5&quot;, &#39;15&#39;= &quot;72499.5&quot;, &#39;16&#39;= &quot;77499.5&quot;, &#39;17&#39;= &quot;82499.5&quot;, &#39;18&#39;= &quot;87499.5&quot;, &#39;19&#39;= &quot;92499.5&quot;, &#39;20&#39;= &quot;97499.5&quot;, &#39;21&#39;= &quot;112499.5&quot;, &#39;22&#39;= &quot;137499.5&quot;, &#39;23&#39;= &quot;162499.5&quot;, &#39;24&#39;= &quot;187499.5&quot;, &#39;25&#39;= &quot;224999.5&quot;, &#39;26&#39;= &quot;500000&quot;)) anes$inc &lt;- as.numeric(as.character(anes$income_fac)) # save data set for use in video write.csv(anes, &quot;anes_week7.csv&quot;) Video and RScript You can find the video introducing you to this week’s method by way of a worked example below. You can also access the code I am typing up in the video in the “Code for Data Analysis” section. I would encourage you to type it yourself, though, as then code tends to better sink into the depths of your brain 😉 Code for Data Analysis ###################################### # MMM - Week 7 - Hypothesis Testing ###################################### # Set WD setwd() # Load packages library(tidyverse) # Load data set anes &lt;- read_csv(&quot;anes_week8.csv&quot;) # Visualisation ############################ ggplot(anes, aes(x = inc, y = fttrump1)) + geom_point() + geom_smooth(method = lm) # Regression ############################ model1 &lt;- lm(fttrump1 ~ inc, data = anes) model1 summary(model1) Significance Test for Coefficients in R "],["bivariate-regression.html", "Bivariate Regression Self-Assessment Questions13 Exercises – Core Going Further in R Going Further - Conceptual", " Bivariate Regression Self-Assessment Questions13 What does regression do? Why is there an error term in regression? How does OLS fit the regression line? Why do we need a measure to assess goodness of fit? Consider the causality framework of Week 5. To what extent is bivariate regression helpful to establish causality? Please stop here and don’t go beyond this point until we have compared notes on your answers. Exercises – Core For the exercises you will be using the data set london_exercises. We will analyse patterns of crime in London14. The dataset contains several variables for each of the London wards, describing, amongst other things, demographics of their population and the number of crimes committed in each of them in 2015. A full codebook is provided in the following Table: Table 9: Codebook for london_exercises Data Set variable label year ward Ward name n/a inner Binary classification of whether the ward is inner or outer London (based on ONS classification) n/a area Land area (km2) n/a population Population 2015 2015 children Number of people aged 0-15 2015 adults Number of people aged 16-64 2015 elderly Number of people aged 65+ 2015 age Mean age of population 2013 education Percentage of population with level 4 qualifications and above 2011 crime Crimes committed per 1,000 residents 2015 employed Number of people aged 16-64 in employment 2015 benefits Percentage of population claiming work-related benefits 2011 migration Net rate of worker-aged migration 2012 income Median household income (GBP) 2013 houseprice Median house price (GBP) 2014 cars Average number of cars per household 2011 turnout Turnout at the 2012 mayoral election (%) 2012 The data are taken from London Data Store (2013). Examine the London data. What hypothesis could be tested using the variables contained in the dataset? Crime rate is defined as the number of crimes committed per 1,000 people in a given area. Evaluate the variable’s distribution using a histogram and summary statistics. Which wards had the most crimes committed in them? Find the names of wards in which the crime rate was higher than the 99th percentile of the variable’s distribution. London is divided into 32 Boroughs and the City of London. Examine the variation in crime on Borough level. Calculate crime rate for each of the Boroughs. Present the Borough-level crime rates using a table and a bar chart, with Borough order descending by crime rate. Unemployment rate, defined as the ratio of people in full time employment to population of working age is often said to be related to crime. Generate an unemployment rate variable for each of the wards. Examine the distribution of the unemployment rate in a similar manner as in Exercise 2. Create a scatter plot of the relationship between unemployment and crime. Make sure to choose the right axis for each of the variables and to label the axes correctly. Interpret the results. Calculate the correlation coefficient between these two variables and interpret its value. Estimate a regression model for the relationship between unemployment rate and crime rate. Interpret the following: The intercept and its significance. The slope and its significance. The distribution of residuals. (Use a histogram). The \\(R^2\\) statistic. Add the regression line to the scatter plot from Exercise 4. Calculate the 95% CI for the coefficient and the slope of the model. Using the model, predict the value of crime rate for an hypothetical ward with unemployment rate of \\(0.4\\). It can be hypothesised that a ward with higher median household income will have a lower rate of crime compared to a ward with lower median household income. Run a regression model testing this hypothesis. Interpret the coefficients, their significance levels, and the \\(R^2\\) of the model. Produce a scatter plot and add the regression line. What conclusions can you draw from the model and plot with regards to the hypothesis? The mean age of a ward can be said to influence the rate of crime in that ward. Come up with a hypothesis for the relationship between the mean age of a ward and the rate of crime in that ward. Briefly justify why you chose this hypothesis. Run a regression model testing this hypothesis and interpret the results of the model. What does this interpretation suggest about your hypothesis? It can be hypothesised that a ward with a larger number of benefit recipients will have a higher rate of crime compared to a ward with fewer benefit recipients.%8 Explain some reasons why this might be the case. Run a regression model testing this hypothesis and interpret the results of the model. What does this interpretation suggest about your hypothesis? Produce a scatter plot and add the regression line. Compare the regression model with the models from Exercises 6 and 7. Which is better able to explain the crime rate of a ward and why? Going Further in R London Boroughs are classified as either ‘Inner’ or ‘Outer’ boroughs. Recreate the scatter plot from Question 2 in the Core Exercises, but with different colours for the inner and outer boroughs subsetting the data to wards with a crime rate of below 500. Show the observations of the inner boroughs in red, and th outer boroughs in blue. Add a line of best fit to the plot, retaining red and blue for inner and outer boroughs, respectively. Estimate a regression model for the relationship between the unemployment rate and crime rate in inner and outer boroughs. Compare the coefficients for the two models, how can you interpret these? Compare and interpret the \\(R^2\\) statistics. Does unemployment better explain crime in inner or outer boroughs? Using the models estimated predict what the crime rate would be in an inner and outer borough given a ward with and unemployment rate of \\(0.45\\). Going Further - Conceptual Recall that: \\[\\begin{equation}\\label{eq:beta1est} \\hat{\\beta_{0}} = \\bar{Y} - \\hat{\\beta_{1}}\\bar{X} \\end{equation}\\]     \\[\\begin{equation} \\hat{\\beta_{1}} = \\dfrac{\\Sigma_{i=1}^{N} (X_{i}-\\bar{X})(Y_{i}-\\bar{Y})}{\\Sigma_{i=1}^{N} (X_{i} - \\bar{X})^{2}} \\end{equation}\\] Now consider the following data set: Table 10: Regression Data Set i age (x) income (y) 1 22 700 2 19 650 3 56 2300 4 45 1900 5 37 2000 6 23 900 7 32 1000 8 65 2500 9 43 1800 10 48 1200 Assuming a regression model of the type \\(Y_{i}=\\beta_{0}+ \\beta_{1}X_{i}+\\epsilon_{i}\\), calculate the estimators for \\(\\beta_{0}\\) and \\(\\beta_{1}\\). Use the above Table as a guide to the required intermediary calculations. Specify the SRF and interpret the estimators of \\(\\beta_{0}\\) and \\(\\beta_{1}\\). Calculate the coefficient of determination, \\(r^{2}\\), with \\(\\hat{Y_{i}}= \\widehat{-53.1} + \\widehat{39.7} X_{i}\\). Table 11: Regression Data Set i age (x) income (y) \\(y-\\bar{y}\\) \\((y-\\bar{y})^2\\) 1 22 700 700 490000 2 19 650 650 422500 3 56 2300 2300 5290000 4 45 1900 1900 3610000 5 37 2000 2000 4000000 6 23 900 900 810000 7 32 1000 1000 1000000 8 65 2500 2500 6250000 9 43 1800 1800 3240000 10 48 1200 1200 1440000 Some of the content of this worksheet is taken from Reiche (forthcoming).↩︎ These exercises are taken from Chapter 9 in Reiche (forthcoming).↩︎ "],["case-study-1.html", "Case Study15 Load Packages and Data Inspect your data Preliminary Analysis Visualisation Visualisation 2.0 Saving the Scatterplot Regression Analysis (yes, finally) Interpretation Exporting the Results Comparing Models Causality", " Case Study15 Load Packages and Data Before starting, we need to load libraries and install packages if not already installed. In these exercises we will be using the following packages: haven ggplot2 modelsummary We will be using the london.csv data set from the lecture, but with a different independent variable this time. These will be of particular interest: Table 12: london Codebook variable label year const Parliamentary constituency n/a gcse An average score based on a pupil’s best eight grades in a group of GCSEs. The maximum a pupil can achieve is 90 points. 2019 eth_min Percent of population composed by ethnic minorities 2011 idaci The Income Deprivation Affecting Children Index rank - how it compares to other constituencies 2015/16 income Mean income by constituency 2017/18 Data are taken from House of Commons Library (n.d.), GOV.UK (2013), and London Data Store (2010). Set your working directory and load the data. Inspect your data Here you can use several basic functions. The dataset does not contain too many variables, so you can start by using names(), str(), etc. Preliminary Analysis Let’s say we want to look at the relationship between income deprivation affecting children and GCSE scores. The two variables are, respectively, idaci and gcse. Now, formulate the working (alternative) and the null hypothesis. Write them down. H\\(\\pmb{_0}\\): H\\(\\pmb{_1}\\): Which is your dependent variable? Run a frequency table on the idaci variable. Does this distribution make sense? Why/why not? Do the same for the other variable. And guess what is the level of measurement. Visualisation Let’s start with the visualisation of the relationship between the two variables. What is the best way to visualise the relationship considering the level of measurement of our variables? Hint: Probably a scatterplot, right? So, use a scatterplot to visualise the relationship and add the regression line. You can use ggplot, but also the base R plot() function. Improve the graph by: Adding a regression line. Adding up a relevant title, also possibly a subtitle. Adding axes labels and making them readable. Visualisation 2.0 Now, draw a vertical and horizontal line corresponding to the mean of your variables using geom_hline and geom_vline. You can thus check if the regression line passes through the mean of X and Y. (see: https://www.rdocumentation.org/packages/ggplot2/versions/0.9.1/topics/geom_hline). You can improve the scatterplot using a series of arguments (e.g., alpha) in the geom_point() function in ggplot. Try to improve the Aesthetics of the scatterplot playing with alpha, for instance. (see: https: //www.rdocumentation.org/packages/ggplot2/versions/3.4.0/topics/geom_point). Saving the Scatterplot You can save a graph as .png, .JPG (even .pdf) that you can then import in a word document. Although there are many way to use your R output, saving a graph might be sometimes useful. Use the function ggsave() to save your scatterplot. Again, there are tons of examples online, google it. Hint: You first need to store the graph in an object. Hint 2: The file will end up in your working directory. Regression Analysis (yes, finally) Now we can finally run a linear regression with gcse as the outcome variable and idaci as the predictor using the lm() function. Store the results in an object called model and visualise the regression output using summary(). # Store the results in an object called model # model&lt;-lm(gcse ~ idaci, london) # Visualise the regression output using summary() # summary(model) Call: lm(formula = gcse ~ idaci, data = london) Residuals: Min 1Q Median 3Q Max -6.4921 -2.5321 -0.1722 2.7077 7.7815 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 45.852958 0.657251 69.765 &lt; 2e-16 *** idaci 0.019765 0.002894 6.831 2.4e-09 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 3.317 on 71 degrees of freedom Multiple R-squared: 0.3965, Adjusted R-squared: 0.388 F-statistic: 46.66 on 1 and 71 DF, p-value: 2.398e-09 You can also extract specific blocks of the output table. One way of doing it is to use the brackets [] after the summary() function. For example summary()[8]. Try to extract the block of Coefficients from the table, like this: $coefficients Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 45.8529580 0.65725150 69.764707 3.767825e-67 idaci 0.0197651 0.00289364 6.830532 2.397995e-09 Interpretation Interpret the results, starting with model evaluation. Is the p-value of the F-statistics statistically significant? We will be discussing this in our following lectures. How much variation in the outcome variable does the model explain? What does this tell us about the model? What’s the value of the slope? What does it mean? What’s the value of the intercept? How do we interpret it? Is it statistically significant? What does it mean in practice? Interpret the results (in plain language) referring to the hypothesis you formulated above. Exporting the Results As for the graphs, you can also export and save the results of the regression model in a Word table. To do that you can use the ‘modelsummary’ package. Try to export the table. Just like writing a shopping list, we start by creating a list of models for which we want modelsummary to produce a table. You can merely list the names of the objects in which the models are stored, or you can give them specific names which will appear as column titles in the table. This addition is by no means a must, but sometimes you might wish to give models a particular name, for example if you have used different methods of estimation, different components of a theory, etc. In the table we are creating here, I wish to distinguish between bivariate and multivariate models. By default, modelsummary just numbers models in ascending order. models &lt;- list( &quot;Bivariate&quot; = model ) This simple step is already sufficient to produce – an admittedly somewhat crude – results table. All we have to do is to load the modelsummary package and to use the previously defined list of models as the argument of the modelsummary() function: library(modelsummary) modelsummary(models) Execute these code chunks as we go through this Section, so you can see the alterations we make to the table in real-time. This table is a vast improvement on the raw R output you would receive when calling summary(model1). But it is far from finished. One characteristic which is conspicuously absent is an assessment of statistical significance. In model summaries of this kind this information is usually provided in the form of asterisks or other symbols next to the respective coefficient. We can add these simply by adding the option stars=TRUE to the code. modelsummary(models, stars = TRUE) Next up is the modification of the stub. The stub is the leftmost column in which you name the indicators for which coefficients will be presented.In the stub, “[a]bbreviate nothing. And never ever ever use computer variable names to stand for concepts. These are personal code words that convey no meaning to readers.” (Stimson, n.d., p. 10) Following this advice, let us add proper labels to the independent variables. To let modelsummary know how to replace each variable name, we create a so-called coefficient map. This is really just a character vector which follows a “before-after” logic in each of its rows. For example 'age'='Age' replaces the variable name age with the label Age. We need to do this for all the variables we used in our models. I am storing this in a vecor called cm which stands for coefficient map. cm &lt;- c(&#39;idaci&#39; = &#39;Income Deprivation Affecting Children Index&#39;, &#39;(Intercept)&#39; = &#39;Intercept&#39;) The next step is easy, as we only need to feed this coefficient map into the modelsummary code with the option coef_map=cm: modelsummary(models, stars = TRUE, coef_map = cm) This is a personal thing, but I also like to show the label of the dependent variable in results tables. They just feel incomplete to me without this information. There is no default to achieve this in modelsummary and so we need to use a little trick. What I want to add in the first row, but only in the second columm, the text . This requires us to alter the style of the table slightly. modelsummary uses another package called tinytable to style the output. We load the package with library(tinytable) and instruct R to place the text where we want it. library(tinytable) modelsummary(models, stars = TRUE, coef_map = cm)|&gt; group_tt(j = list(&quot;Dependent Variable: GCSE Score&quot; = 2)) Lastly, let us tackle the model fit statistics. By default, modelsummary prints a whole festival of these into the bottom section of the table, but I want to concentrate only on R\\(^2\\). modelsummary(models, stars = TRUE, coef_map = cm, gof_omit = &#39;DF|Deviance|Log.Lik|F|AIC|BIC|RMSE&#39;)|&gt; group_tt(j = list(&quot;Dependent Variable: GCSE Score&quot; = 2)) This should produce this table: /* tinytable css entries after */ .table td.tinytable_css_z27mpah6ubp1eivub3np, .table th.tinytable_css_z27mpah6ubp1eivub3np { text-align: center; } .table td.tinytable_css_xce8ql9b88e3p8usijy8, .table th.tinytable_css_xce8ql9b88e3p8usijy8 { text-align: left; border-bottom: solid black 0.05em; } .table td.tinytable_css_vu1eqzu0ara6yd5hjcqt, .table th.tinytable_css_vu1eqzu0ara6yd5hjcqt { text-align: center; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_vog99f8hskxj2fz5vrxc, .table th.tinytable_css_vog99f8hskxj2fz5vrxc { text-align: left; } .table td.tinytable_css_n2x1l40lbalm922ls3zh, .table th.tinytable_css_n2x1l40lbalm922ls3zh { text-align: left; border-top: solid #d3d8dc 0.1em; text-align: center; } .table td.tinytable_css_l07z8vdwt98vgc16gt73, .table th.tinytable_css_l07z8vdwt98vgc16gt73 { text-align: left; border-bottom: solid #d3d8dc 0.1em; } .table td.tinytable_css_c53n75ebl4a5p8uf37l2, .table th.tinytable_css_c53n75ebl4a5p8uf37l2 { text-align: center; border-bottom: solid #d3d8dc 0.1em; } .table td.tinytable_css_a7v3ljb69assh87by2dj, .table th.tinytable_css_a7v3ljb69assh87by2dj { text-align: left; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_6dlj2dor6mh1bu5wjb2z, .table th.tinytable_css_6dlj2dor6mh1bu5wjb2z { text-align: center; border-top: solid #d3d8dc 0.1em; text-align: center; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_3lbtok5bnv9ckj9ye8hw, .table th.tinytable_css_3lbtok5bnv9ckj9ye8hw { text-align: center; border-bottom: solid black 0.05em; } Dependent Variable:GCSE Score Table 13: Regression Models Bivariate + p Income Deprivation Affecting Children Index 0.020*** (0.003) Intercept 45.853*** (0.657) Num.Obs. 73 R2 0.397 R2 Adj. 0.388 You can export your tables for use in MS Word, or Apple Pages. But you will not be able to export the full table, only up to the point at which you are specifying the options for tinytable with |&gt;. This is probably as good a reason as any to start working with Markdown or LaTeXBut if you are happy with this caveat, the following code will render the table in an MS Word document (output=\"table.docx\") that is placed in your working directory. modelsummary(models, stars = TRUE, coef_map = cm, gof_omit = &#39;AIC|BIC|Log.Lik|F|RMSE&#39;, add_rows = rows, notes = list(&#39;This is a long and rather pointless note to demonstrate formatting.&#39;), output = &quot;table.docx&quot;) Comparing Models You can now run another regression model with . Compare your original model with the new one. How do you know which independent variable is doing a better job in explaining your dependent variable? Causality Consider the causality framework of Week 5. To what extent has this case study established causality? What is missing from the framework? The structure of this study was originally designed by Oleksiy Bondarenko. I have slightly altered it in subsequent years.↩︎ "],["homework-for-week-9.html", "Homework for Week 9 Solutions", " Homework for Week 9 Finish working through this worksheet. Complete the Case Study. Add a note underneath each code chunk in your RScript (by starting the line with #), translating the code into plain English. Read the required literature for week 9. Work through this week’s flashcards to familiarise yourself with the relevant R functions. Find an example for each NEW function and apply it in R to ensure it works. Before coming to the seminar, I encourage you to work through the Methods, Methods, Methods Section of Week 9. This will take you through the relevant components of regression analysis in R. Solutions You can find the Solutions in the Downloads Section. "],["glossary-6.html", "Glossary", " Glossary Table 14: Glossary Week 8 Term Description central limit theorem In random sampling with a large sample size – where n=30 is usually sufficient – the sampling distribution of the sample mean \\(\\bar{y}\\) will be approximately normally distributed, irrespective of the shape of the population distribution confidence interval A confidence interval constructs an interval of numbers which will contain the true parameter of the population (e.g. the mean) in \\((1-\\alpha)\\) times of cases. \\(\\alpha\\) is usually chosen to be small, so that our confidence interval has a probability of 95% or 99%. confidence level The confidence level is the probability with which the confidence interval is believed to contain the true parameter of the population and is defined as \\((1-\\alpha)\\) degrees of freedom Degrees of freedom express constraints on our estimation process by specifying how many values in the calculation are free to vary significance level The significance level, denoted by \\(\\alpha\\), is the threshold used in hypothesis testing to determine if a result is statistically significant. It represents the probability of rejecting the null hypothesis when it’s actually true (a Type I error). Common levels are 0.05 or 0.01, indicating 5% or 1% risk. We will cover this properly in Week 9. t-distribution The t-Distribution is bell-shaped and symmetrical around a mean of zero. Its shape is dependent on the degrees of freedom in the estimation process. "],["flashcards-6.html", "Flashcards", " Flashcards New Functions This Week The data are available as a .csv file. R Functions This Week The data are available as a .csv file. All R Functions So Far The data are available as a .csv file. "],["methods-methods-methods-1.html", "Methods, Methods, Methods Data Prep Video and RScript", " Methods, Methods, Methods This week we are extending our bivariate regression model to approximate the real world in which nothing is mono-causal. We call this multiple regression. Just as last week we will be working with the American National Election Studies (ANES), and to be more precise with the pilot survey conducted before the 2020 presidential election. If you haven’t already done so, you will have to register with ANES in order to download the data set. To do so, please follow this link. Data Prep Place the ANES data in a folder which you will be using as a working directory for this session. Open the “Code for Data Preparation” below, and copy this into an RScript. Remember to adjust the working directory with the setwd() command at the beginning. Then run the RScript and you will be ready to proceed to the video. Code for Data Preparation ###################################### # MMM - Week 8 - Data Preparation ###################################### # Set WD setwd() # Load packages library(tidyverse) # Load data set anes &lt;- read_csv(&quot;anes.csv&quot;) # Get rid of missing values for variables used in analysis today ## 999 is equivalent to NA, so needs to be recoded anes$fttrump1 &lt;- with(anes, replace(fttrump1, fttrump1 == 999, NA)) anes$income &lt;- with(anes, replace(income, income == 99, NA)) anes &lt;- filter(anes, !is.na(fttrump1), !is.na(age), !is.na(income)) # Turn income variable into a numerical variable with mid-points of each level anes$income &lt;- factor(anes$income) table(anes$income) anes &lt;- anes %&gt;% mutate(income_fac = recode(income, &#39;1&#39;= &quot;2500&quot;, &#39;2&#39;= &quot;7499.5&quot;, &#39;3&#39;= &quot;12499.5&quot;, &#39;4&#39;= &quot;17499.5&quot;, &#39;5&#39;= &quot;22499.5&quot;, &#39;6&#39;= &quot;27499.5&quot;, &#39;7&#39;= &quot;32499.5&quot;, &#39;8&#39;= &quot;37499.5&quot;, &#39;9&#39;= &quot;42499.5&quot;, &#39;10&#39;= &quot;47499.5&quot;, &#39;11&#39;= &quot;52499.5&quot;, &#39;12&#39;= &quot;57499.5&quot;, &#39;13&#39;= &quot;62499.5&quot;, &#39;14&#39;= &quot;67499.5&quot;, &#39;15&#39;= &quot;72499.5&quot;, &#39;16&#39;= &quot;77499.5&quot;, &#39;17&#39;= &quot;82499.5&quot;, &#39;18&#39;= &quot;87499.5&quot;, &#39;19&#39;= &quot;92499.5&quot;, &#39;20&#39;= &quot;97499.5&quot;, &#39;21&#39;= &quot;112499.5&quot;, &#39;22&#39;= &quot;137499.5&quot;, &#39;23&#39;= &quot;162499.5&quot;, &#39;24&#39;= &quot;187499.5&quot;, &#39;25&#39;= &quot;224999.5&quot;, &#39;26&#39;= &quot;500000&quot;)) anes$inc &lt;- as.numeric(as.character(anes$income_fac)) # save data set for use in video write.csv(anes, &quot;anes_week8.csv&quot;) Video and RScript You can find the video introducing you to this week’s method by way of a worked example below. You can also access the code I am typing up in the video in the “Code for Data Analysis” section. I would encourage you to type it yourself, though, as then code tends to better sink into the depths of your brain 😉 Code for Data Analysis ###################################### # MMM - Week 8 - Multiple Regression ###################################### # Set WD setwd() # Load packages library(tidyverse) # Load data set anes &lt;- read_csv(&quot;anes_week9.csv&quot;) # Bivariate Regression ############################ model1 &lt;- lm(fttrump1 ~ inc, data = anes) summary(model1) model2 &lt;- lm(fttrump1 ~ age, data = anes) summary(model2) # Multiple Regression ############################ model3 &lt;- lm(fttrump1 ~ inc + age, data = anes) summary(model3) Multiple Regression in R "],["multiple-regression.html", "Multiple Regression Group Work – Self-Reflection16 Multiple Regression in R – Guided Example Multiple Regression in R – Independent Analysis 0.1 Going Further", " Multiple Regression Group Work – Self-Reflection16 Compare bivariate regression and multiple regression. Give an example of the relationship in which you could apply multiple linear regression. How do you interpret partial slope coefficients? How do you interpret adjusted R-Squared in regression analysis? Consider the causality framework of Week 5. To what extent is multiple regression helpful to establish causality? Prepare to interpret the following regression output substantively (what do the coefficients mean, how good is the model fit, etc.) income represents the median household income in a London ward (in £), and turnout represents the turnout at the 2012 mayoral election (in %). Call: lm(formula = turnout ~ income, data = london) Residuals: Min 1Q Median 3Q Max -24.3099 -2.3808 0.5004 3.2679 11.2834 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.955e+01 9.861e-01 19.83 &lt;2e-16 *** income 3.720e-04 2.467e-05 15.08 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 4.594 on 623 degrees of freedom Multiple R-squared: 0.2673, Adjusted R-squared: 0.2661 F-statistic: 227.3 on 1 and 623 DF, p-value: &lt; 2.2e-16 Please stop here and don’t go beyond this point until we have compared notes on your answers. Multiple Regression in R – Guided Example Download the WDI_PO91Q.csv data set. Data are taken from World Bank (2024), Boix et al. (2018), and Marshall &amp; Gurr (2020). Put it into an appropriate working directory for this seminar and create a dedicated RScript and save it into the same working directory. setwd(&quot;~/Warwick/Modules/PO91Q/Seminars/Week 8&quot;) Import the data set into R: wdi &lt;- read.csv(&quot;WDI_PO91Q.csv&quot;) Once again, here is the overview of the variables available and their respective label in Table 15: Table 15: WDI Codebook variable label Country Name Country Name Country Code Country Code year year democracy 0 = Autocracy, 1 = Dictatorship (Boix et al., 2018) gdppc GDP per capita (constant 2010 US$) gdpgrowth Absolute growth of per capita GDP to previous year (constant 2010 US Dollars) enrl_gross School enrollment, primary (% gross) enrl_net School enrollment, primary (% net) agri Employment in agriculture (% of total employment) (modeled ILO estimate) slums Population living in slums (% of urban population) telephone Fixed telephone subscriptions (per 100 people) internet Individuals using the Internet (% of population) tax Tax revenue (% of GDP) electricity Access to electricity (% of population) mobile Mobile cellular subscriptions (per 100 people) service Services, value added (% of GDP) oil Oil rents (% of GDP) natural Total natural resources rents (% of GDP) literacy Literacy rate, adult total (% of people ages 15 and above) prim_compl Primary completion rate, total (% of relevant age group) infant Mortality rate, infant (per 1,000 live births) hosp Hospital beds (per 1,000 people) tub Incidence of tuberculosis (per 100,000 people) health_ex Current health expenditure (% of GDP) ineq Income share held by lowest 10% unemploy Unemployment, total (% of total labor force) (modeled ILO estimate) lifeexp Life expectancy at birth, total (years) urban Urban population (% of total population) polity5 Combined Polity V score Let’s take the example from Week 7 back up. First re-run the regression I used back then. wdi_life &lt;- lm(gdppc ~ lifeexp, data = wdi) summary(wdi_life) Call: lm(formula = gdppc ~ lifeexp, data = wdi) Residuals: Min 1Q Median 3Q Max -18457 -9877 -4187 4963 78242 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -94306.8 10406.7 -9.062 3.33e-16 *** lifeexp 1503.2 144.4 10.412 &lt; 2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 14690 on 167 degrees of freedom (26 observations deleted due to missingness) Multiple R-squared: 0.3936, Adjusted R-squared: 0.39 F-statistic: 108.4 on 1 and 167 DF, p-value: &lt; 2.2e-16 Interpret the coefficients. Interpreting a coefficient The order in which to interpret a coefficient is as follows: Is it significant? If not, all you can say is that there is no influence. You have falsified the alternative hypothesis. If it is significant, you can interpret its size and direction according to the statistical model (for example, slope coefficient vs. partial slope coefficient). What does the coefficient mean for the hypothesis? Look at the direction. Is the direction as predicted by the hypothesis? Then you have evidence to support the hypothesis. If the direction is inverse, then you have falsified the hypothesis, even though you have a significant coefficient. Now we use a different regressor, say “Urban population (% of total)”. Specify the null and the alternative hypotheses. wdi_urban &lt;- lm(gdppc ~ urban, data = wdi) summary(wdi_urban) Call: lm(formula = gdppc ~ urban, data = wdi) Residuals: Min 1Q Median 3Q Max -28588 -10681 -3110 6863 152303 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -16983.43 3872.34 -4.386 1.98e-05 *** urban 542.03 61.74 8.779 1.42e-15 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 19120 on 176 degrees of freedom (17 observations deleted due to missingness) Multiple R-squared: 0.3045, Adjusted R-squared: 0.3006 F-statistic: 77.07 on 1 and 176 DF, p-value: 1.417e-15 Interpret the coefficients. What does this mean for the hypotheses? If we want to assess the influence of both independent variables together, we type: wdi_joint &lt;- lm(gdppc ~ lifeexp + urban, data = wdi) summary(wdi_joint) Call: lm(formula = gdppc ~ lifeexp + urban, data = wdi) Residuals: Min 1Q Median 3Q Max -22570 -8825 -2143 5594 74445 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -74786.14 10690.13 -6.996 6.17e-11 *** lifeexp 1012.17 172.70 5.861 2.41e-08 *** urban 273.74 59.13 4.629 7.37e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 13860 on 166 degrees of freedom (26 observations deleted due to missingness) Multiple R-squared: 0.463, Adjusted R-squared: 0.4565 F-statistic: 71.55 on 2 and 166 DF, p-value: &lt; 2.2e-16 Interpret the coefficients. Bear in mind for your interpretation that these are partial slope coefficients! Because I am nice, I am producing an overview17 of all three regressions in Table 16:     /* tinytable css entries after */ .table td.tinytable_css_xi209ttquk7ighknippe, .table th.tinytable_css_xi209ttquk7ighknippe { text-align: center; border-top: solid #d3d8dc 0.1em; text-align: center; } .table td.tinytable_css_v9p67rlvusrx76886n3p, .table th.tinytable_css_v9p67rlvusrx76886n3p { text-align: center; border-top: solid #d3d8dc 0.1em; text-align: center; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_u9icsg2w2hd79tf58x71, .table th.tinytable_css_u9icsg2w2hd79tf58x71 { text-align: left; } .table td.tinytable_css_tt6nw06zmp3qdvn2rbta, .table th.tinytable_css_tt6nw06zmp3qdvn2rbta { text-align: center; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_pyzjbz0gfup195rl0rr5, .table th.tinytable_css_pyzjbz0gfup195rl0rr5 { text-align: left; border-bottom: solid black 0.05em; } .table td.tinytable_css_e754dxk9kisafq9y187h, .table th.tinytable_css_e754dxk9kisafq9y187h { text-align: center; border-bottom: solid #d3d8dc 0.1em; } .table td.tinytable_css_dr3h4o231xysf5lh4crh, .table th.tinytable_css_dr3h4o231xysf5lh4crh { text-align: center; border-bottom: solid black 0.05em; } .table td.tinytable_css_aizgxrc1agcehe0g7a4n, .table th.tinytable_css_aizgxrc1agcehe0g7a4n { text-align: left; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_874edlx3kjdg6tfon5ju, .table th.tinytable_css_874edlx3kjdg6tfon5ju { text-align: left; border-bottom: solid #d3d8dc 0.1em; } .table td.tinytable_css_4v5j4z8mrxdmcroyjzm8, .table th.tinytable_css_4v5j4z8mrxdmcroyjzm8 { text-align: center; } .table td.tinytable_css_2ocoxv454jbd7vy7b7xi, .table th.tinytable_css_2ocoxv454jbd7vy7b7xi { text-align: left; border-top: solid #d3d8dc 0.1em; text-align: center; } Dependent Variable:per capita GDP Table 16: Regression Models 1. Bivariate(1) Bivariate(2) Multiple(3) + p Life Expectancy (years) 1503.211*** 1012.168*** (144.371) (172.696) Urbanisation 542.033*** 273.735*** (61.743) (59.134) Constant -94306.813*** -16983.431*** -74786.145*** (10406.727) (3872.336) (10690.128) Num.Obs. 169 178 169 R2 0.394 0.305 0.463 R2 Adj. 0.390 0.301 0.456     Drawing on what you have learned about the stargazer package in the additional exercises last week, replicate this table. How have the slope coefficients changed? Why? Which model explains the level of GDP best? Why? Specify the SRF for Model 3, paying special attention to notation. Now assume, we want to know whether education has a bearing on the level of GDP. We call: wdi_lit &lt;- lm(gdppc ~ literacy, data = wdi) and also add it to the joint model: wdi_joint1 &lt;- lm(gdppc ~ lifeexp + urban + literacy, data = wdi) This should lead to these results:     /* tinytable css entries after */ .table td.tinytable_css_ympg1jkl9cutrxi0jdzg, .table th.tinytable_css_ympg1jkl9cutrxi0jdzg { text-align: left; } .table td.tinytable_css_uqeqrz0czy3ig72f0azf, .table th.tinytable_css_uqeqrz0czy3ig72f0azf { text-align: center; border-top: solid #d3d8dc 0.1em; text-align: center; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_twwbjmw6kd39f0kooiqh, .table th.tinytable_css_twwbjmw6kd39f0kooiqh { text-align: center; border-bottom: solid black 0.05em; } .table td.tinytable_css_tp4rouft5htnms7ggbz5, .table th.tinytable_css_tp4rouft5htnms7ggbz5 { text-align: left; border-bottom: solid #d3d8dc 0.1em; } .table td.tinytable_css_jt0f3iujnjjngu85rclk, .table th.tinytable_css_jt0f3iujnjjngu85rclk { text-align: center; } .table td.tinytable_css_jg3awbvpmnqskiizs9i2, .table th.tinytable_css_jg3awbvpmnqskiizs9i2 { text-align: center; border-bottom: solid #d3d8dc 0.1em; } .table td.tinytable_css_ididkncqd09tqs4vn3op, .table th.tinytable_css_ididkncqd09tqs4vn3op { text-align: center; border-top: solid #d3d8dc 0.1em; text-align: center; } .table td.tinytable_css_d8o8z75pkq14pumnmlic, .table th.tinytable_css_d8o8z75pkq14pumnmlic { text-align: left; border-top: solid #d3d8dc 0.1em; text-align: center; } .table td.tinytable_css_777k7jhncpfgyh0v0scq, .table th.tinytable_css_777k7jhncpfgyh0v0scq { text-align: left; border-bottom: solid black 0.05em; } .table td.tinytable_css_0ts07vmyqwdszgcw5h8g, .table th.tinytable_css_0ts07vmyqwdszgcw5h8g { text-align: left; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_0se8gux6p3pyp3bmje2r, .table th.tinytable_css_0se8gux6p3pyp3bmje2r { text-align: center; border-bottom: solid #d3d8dc 0.05em; } Dependent Variable:per capita GDP Table 17: Regression Models 2. (1) (2) (3) (4) (5) + p Life Expectancy (years) 1503.211*** 1012.168*** 973.985* (144.371) (172.696) (411.375) Urbanisation 542.033*** 273.735*** 227.126* (61.743) (59.134) (83.427) Literacy 258.407* -221.731 (97.381) (142.455) Constant -94306.813*** -16983.431*** -74786.145*** -12967.003 -55246.837** (10406.727) (3872.336) (10690.128) (8529.810) (19311.126) Num.Obs. 169 178 169 40 39 R2 0.394 0.305 0.463 0.156 0.497 R2 Adj. 0.390 0.301 0.456 0.134 0.454     In Model 5 the coefficient for literacy has turned insignificant. Reproduce the results in Table 4 to find out which variable takes away the significance.     /* tinytable css entries after */ .table td.tinytable_css_zp3fuzb4fs59744gllrz, .table th.tinytable_css_zp3fuzb4fs59744gllrz { text-align: left; border-bottom: solid black 0.05em; } .table td.tinytable_css_uwxbwy7ohgxyfq1i53u1, .table th.tinytable_css_uwxbwy7ohgxyfq1i53u1 { text-align: center; border-bottom: solid #d3d8dc 0.1em; } .table td.tinytable_css_p3r4451lj9cc4qz3sw7j, .table th.tinytable_css_p3r4451lj9cc4qz3sw7j { text-align: left; border-top: solid #d3d8dc 0.1em; text-align: center; } .table td.tinytable_css_ncaz4xl40cncbts1ojwe, .table th.tinytable_css_ncaz4xl40cncbts1ojwe { text-align: center; border-top: solid #d3d8dc 0.1em; text-align: center; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_mbey07irxne9v470ac8k, .table th.tinytable_css_mbey07irxne9v470ac8k { text-align: center; } .table td.tinytable_css_jqgf9cyh5jnuf1h6xaua, .table th.tinytable_css_jqgf9cyh5jnuf1h6xaua { text-align: left; border-bottom: solid #d3d8dc 0.1em; } .table td.tinytable_css_cx3q5wxkibrl7cpohpg8, .table th.tinytable_css_cx3q5wxkibrl7cpohpg8 { text-align: center; border-bottom: solid black 0.05em; } .table td.tinytable_css_ad2o75r45up1ou6b9l94, .table th.tinytable_css_ad2o75r45up1ou6b9l94 { text-align: left; } .table td.tinytable_css_2bxpdjay01il88e9o7ki, .table th.tinytable_css_2bxpdjay01il88e9o7ki { text-align: left; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_1rxg1c9ue1nzkpg8okdj, .table th.tinytable_css_1rxg1c9ue1nzkpg8okdj { text-align: center; border-top: solid #d3d8dc 0.1em; text-align: center; } .table td.tinytable_css_0x2wilm0h9i8dfghm9tk, .table th.tinytable_css_0x2wilm0h9i8dfghm9tk { text-align: center; border-bottom: solid #d3d8dc 0.05em; } Dependent Variable:per capita GDP Table 18: Regression Models 3. (1) (2) (3) + p Life Expectancy (years) 1483.785*** (397.567) Urbanisation 315.375*** (77.633) Literacy 258.407* -223.305 28.390 (97.381) (154.620) (99.706) Constant -12967.003 -77851.339*** -12399.608+ (8529.810) (18924.058) (7189.937) Num.Obs. 40 39 40 R2 0.156 0.390 0.417 R2 Adj. 0.134 0.357 0.385     What can we conclude from this investigation? Does the variable infant have the same effect? What do you conclude from this? Which measurement explains GDP better, life or infant?     /* tinytable css entries after */ .table td.tinytable_css_xss40jg92o3qw6diihpj, .table th.tinytable_css_xss40jg92o3qw6diihpj { text-align: left; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_t3h698zzyw5kh61y5z0k, .table th.tinytable_css_t3h698zzyw5kh61y5z0k { text-align: center; border-bottom: solid black 0.05em; } .table td.tinytable_css_luy37155x4x8euzk7u54, .table th.tinytable_css_luy37155x4x8euzk7u54 { text-align: left; border-bottom: solid #d3d8dc 0.1em; } .table td.tinytable_css_l34q29dl8t9jt5p927r9, .table th.tinytable_css_l34q29dl8t9jt5p927r9 { text-align: center; border-bottom: solid #d3d8dc 0.1em; } .table td.tinytable_css_jn41u61h2u9ozbjnbumg, .table th.tinytable_css_jn41u61h2u9ozbjnbumg { text-align: left; } .table td.tinytable_css_9ffmhq164lopw08zpicb, .table th.tinytable_css_9ffmhq164lopw08zpicb { text-align: center; border-top: solid #d3d8dc 0.1em; text-align: center; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_84tpu0f9hdh1feajrp5o, .table th.tinytable_css_84tpu0f9hdh1feajrp5o { text-align: center; border-top: solid #d3d8dc 0.1em; text-align: center; } .table td.tinytable_css_7b4q96lwrofx3y9x2fqn, .table th.tinytable_css_7b4q96lwrofx3y9x2fqn { text-align: center; } .table td.tinytable_css_2iz7ipbg495a1kx03ppq, .table th.tinytable_css_2iz7ipbg495a1kx03ppq { text-align: left; border-top: solid #d3d8dc 0.1em; text-align: center; } .table td.tinytable_css_1e16z77er7ykgmeesr6l, .table th.tinytable_css_1e16z77er7ykgmeesr6l { text-align: left; border-bottom: solid black 0.05em; } .table td.tinytable_css_18yzecaumkwt37ay8bwi, .table th.tinytable_css_18yzecaumkwt37ay8bwi { text-align: center; border-bottom: solid #d3d8dc 0.05em; } Dependent Variable:per capita GDP Table 19: Regression Models 4. life infant + p Life Expectancy (years) 1503.211*** (144.371) Infant Mortality (per 1,000 live births) -524.111*** (73.885) Constant -94306.813*** 26467.135*** (10406.727) (2257.385) Num.Obs. 169 178 R2 0.394 0.222 R2 Adj. 0.390 0.218     Multiple Regression in R – Independent Analysis Before you start with these, please pause and let Flo know that you are done. We will compare notes on your answers up to this point, to make sure that you are on the right track for the independent exercises. Use the wdi data frame. Set polity5 as the dependent variable, and choose three sensible variables which you believe could influence democracy. Note that the Polity V Score codes regimes from -10 (indicating perfect autocracy) to +10 (indicating perfect democracy). State the null- and alternative hypotheses for each of the independent variables chosen. Plot two of the bivariate models in a scatter plot (black points) with fitted regression line (in red). Use base R, or ggplot. Does the direction of influence agree with your hypotheses? Run all possible regression models, using a bottom-up strategy. Construct a stargazer table to present the results. Specify the Sample Regression Function (SRF) for a bivariate model (Model A), a multivariate model with two independent variables (Model B), and for the model using all three independent variables (Model C). Interpret the intercept and one of the slope coefficients in Models A, B, and C. Interpret the model fit measure for Models A, B, and C. Which model explains democracy best? Why?. What do we conclude with respect to the hypotheses stated in Exercise 2 from this analysis? Collate a PowerPoint (Keynote) presentation with one slide for each of the preceding nine points. We will discuss this in Week 9. 0.1 Going Further Plot the scatter plots for all models up to two independent variables. Add a line of best fit, or hyperplane of best fit as appropriate. Use the rockchalk package, and function plotPlane() for the 3D plots. All exercises are a reproduction from Reiche (forthcoming).↩︎ The following regression tables have been produced with the package modelsummary (Arel-Bundock, 2022).↩︎ "],["homework-for-week-10.html", "Homework for Week 10 Solutions", " Homework for Week 10 Finish working through this worksheet. Add a note underneath each code chunk in your RScript (by starting the line with #), translating the code into plain English. Read the required literature for week 10. Work through this week’s flashcards to familiarise yourself with the relevant R functions. Find an example for each NEW function and apply it in R to ensure it works. Complete the Mock Exam on Moodle. We will go through the solutions together next week. Before coming to the seminar, I encourage you to work through the Methods, Methods, Methods Section of Week 10. Solutions You can find the Solutions in the Downloads Section. "],["glossary-7.html", "Glossary", " Glossary Table 20: Glossary Week 9 Term Description critical value The critical value is a threshold that determines the boundary for rejecting the null hypothesis (H\\(_0\\)) in a hypothesis test. It is a point on the probability distribution of the test statistic beyond which the null hypothesis is rejected. The critical value is chosen based on the significance level (\\(\\alpha\\)) of the test, which represents the probability of making a Type I error (i.e., rejecting a true null hypothesis). hypothesis In statistics, a hypothesis is a formal statement about a population parameter or relationship between variables. Hypotheses guide statistical tests to determine whether data support or refute them. The hypothesis suggesting an effect or difference is called the alternative hypothesis. The alternative hypothesis is always paired with a null-hypothesis, suggesting no effect or difference. p-value The p-value indicates the probability of obtaining a result equal to, or even more extreme than the observed value in the direction of the alternative hypothesis, assuming the null hypothesis is true. Common thresholds for significance are 0.05, 0.01, and 0.001. A smaller p-value suggests stronger evidence against the null hypothesis. The p-value is denoted as \\(p\\). significance test A significance test is a statistical method used to determine whether observed data provide enough evidence to reject a null hypothesis. It calculates a probability of observing data as extreme as, or more extreme than, the actual sample results, assuming the null hypothesis is true test statistic A test statistic is a value calculated from the sample data that is used to decide whether to reject the null hypothesis (H\\(_0\\)) in a hypothesis test. It quantifies the degree to which the observed data diverges from what is expected under the null hypothesis. In a t-test, the test statistic is a t-value, which measures the distance between the sample mean and the (hypothesised) population mean, expressed in units of standard errors. Type I Error A Type I Error occurs when a null hypothesis (H\\(_0\\)) that is actually true is incorrectly rejected. It is also known as a false positive errors, as it suggests that an effect of difference exists, when, in fact, it does not. The probability of committing a Type I Error is denoted by the significance level (\\(\\alpha\\)) of the test, which is typically set before conducting the test (e.g. \\(\\alpha\\) = 0.05). This means that there is a 5% chance of rejecting the true null hypothesis. Type II Error A Type II Error occurs when a null hypothesis (H\\(_0\\)) that is actually false is incorrectly accepted (or not rejected). It is also known as a false negative error, as it suggests that no effect or difference exists when, in fact, there is one. "],["flashcards-7.html", "Flashcards", " Flashcards New Functions This Week The data are available as a .csv file. R Functions This Week The data are available as a .csv file. All R Functions So Far The data are available as a .csv file. "],["methods-methods-methods-2.html", "Methods, Methods, Methods Data Prep Video and RScript", " Methods, Methods, Methods We have sort of covered how to create dummy variables already in Week 2, but for completeness’ sake here is an exploration on how to create them, include them in regression models, and how to interpret them (that all-important reference category…). Just as last week we will be working with the American National Election Studies (ANES), and to be more precise with the pilot survey conducted before the 2020 presidential election. If you haven’t already done so, you will have to register with ANES in order to download the data set. To do so, please follow this link. Data Prep Place the ANES data in a folder which you will be using as a working directory for this session. Open the “Code for Data Preparation” below, and copy this into an RScript. Remember to adjust the working directory with the setwd() command at the beginning. Then run the RScript and you will be ready to proceed to the video. Code for Data Preparation ###################################### # MMM - Week 9 - Dummy Variables ###################################### # Set WD setwd() # Load packages library(tidyverse) # Load data set anes &lt;- read_csv(&quot;anes.csv&quot;) # Get rid of missing values for variables used in analysis today ## 999 is equivalent to NA, so needs to be recoded anes$fttrump1 &lt;- with(anes, replace(fttrump1, fttrump1 == 999, NA)) anes$income &lt;- with(anes, replace(income, income == 99, NA)) anes &lt;- filter(anes, !is.na(fttrump1), !is.na(income)) # Label variable `sex` anes$sex &lt;- factor(anes$sex) anes &lt;- anes %&gt;% mutate(sex= recode(sex,&quot;1&quot;=&quot;Male&quot;, &quot;2&quot;=&quot;Female&quot;)) anes$sex &lt;- as.factor(anes$sex) # Turn income variable into a numerical variable with mid-points of each level anes$income &lt;- factor(anes$income) anes &lt;- anes %&gt;% mutate(income_fac = recode(income, &#39;1&#39;= &quot;2500&quot;, &#39;2&#39;= &quot;7499.5&quot;, &#39;3&#39;= &quot;12499.5&quot;, &#39;4&#39;= &quot;17499.5&quot;, &#39;5&#39;= &quot;22499.5&quot;, &#39;6&#39;= &quot;27499.5&quot;, &#39;7&#39;= &quot;32499.5&quot;, &#39;8&#39;= &quot;37499.5&quot;, &#39;9&#39;= &quot;42499.5&quot;, &#39;10&#39;= &quot;47499.5&quot;, &#39;11&#39;= &quot;52499.5&quot;, &#39;12&#39;= &quot;57499.5&quot;, &#39;13&#39;= &quot;62499.5&quot;, &#39;14&#39;= &quot;67499.5&quot;, &#39;15&#39;= &quot;72499.5&quot;, &#39;16&#39;= &quot;77499.5&quot;, &#39;17&#39;= &quot;82499.5&quot;, &#39;18&#39;= &quot;87499.5&quot;, &#39;19&#39;= &quot;92499.5&quot;, &#39;20&#39;= &quot;97499.5&quot;, &#39;21&#39;= &quot;112499.5&quot;, &#39;22&#39;= &quot;137499.5&quot;, &#39;23&#39;= &quot;162499.5&quot;, &#39;24&#39;= &quot;187499.5&quot;, &#39;25&#39;= &quot;224999.5&quot;, &#39;26&#39;= &quot;500000&quot;)) anes$inc &lt;- as.numeric(as.character(anes$income_fac)) # save data set for use in video write.csv(anes, &quot;anes_week9.csv&quot;) Video and RScript You can find the video introducing you to this week’s method by way of a worked example below. You can also access the code I am typing up in the video in the “Code for Data Analysis” section. I would encourage you to type it yourself, though, as then code tends to better sink into the depths of your brain 😉 Code for Data Analysis ###################################### # MMM - Week 9 - Dummy Variables ###################################### # Set WD setwd() # Load packages library(tidyverse) # Load data set anes &lt;- read_csv(&quot;anes_week10.csv&quot;) anes$sex &lt;- as.factor(anes$sex) # Bivariate Regression ############################ model1 &lt;- lm(fttrump1 ~ sex, data = anes) summary(model1) anes &lt;- anes %&gt;% mutate(sex = relevel(sex, ref = &quot;Male&quot;)) table(anes$sex) model2 &lt;- lm(fttrump1 ~ sex, data = anes) summary(model2) # Multiple Regression ############################ model3 &lt;- lm(fttrump1 ~ inc + sex, data = anes) summary(model3) Dummy Variables in R "],["variable-transformations-and-exam-preparation.html", "Variable Transformations and Exam Preparation Self-Assessment Questions Exam Preparation Exercises", " Variable Transformations and Exam Preparation Self-Assessment Questions How does the concept of the conditional expected value relate to the interpretation of coefficients in multiple regression? How do you select variables for a multiple regression model? Why do we apply non-linear transformations to variables? How do you interpret the effect of a logarithmized variable? Using data from London wards (London Data Store, 2013) the regression models in Table 13 explain voter turnout in the 2012 mayoral elections. interpret the intercept in Model 1 interpret the slope coefficient in Model 1 interpret the slope coefficients in Model 3 explain why the size effect of the slope coefficients in Models 1 and 2 is so different interpret the model fit measure in Model 3   /* tinytable css entries after */ .table td.tinytable_css_znwnnu0ol2iw0q48o4sn, .table th.tinytable_css_znwnnu0ol2iw0q48o4sn { text-align: left; border-bottom: solid #d3d8dc 0.1em; } .table td.tinytable_css_xaz1a7dm6jsu0ha09v0y, .table th.tinytable_css_xaz1a7dm6jsu0ha09v0y { text-align: left; border-top: solid #d3d8dc 0.1em; text-align: center; } .table td.tinytable_css_vtef3nto0bl0bc77pz7d, .table th.tinytable_css_vtef3nto0bl0bc77pz7d { text-align: center; } .table td.tinytable_css_siyf74h4xdjlg9rrzdqm, .table th.tinytable_css_siyf74h4xdjlg9rrzdqm { text-align: left; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_rdhywhppqxb2dcurgcgs, .table th.tinytable_css_rdhywhppqxb2dcurgcgs { text-align: center; border-bottom: solid #d3d8dc 0.1em; } .table td.tinytable_css_pgdql5352cpo1d6k31ay, .table th.tinytable_css_pgdql5352cpo1d6k31ay { text-align: left; border-bottom: solid black 0.05em; } .table td.tinytable_css_jte0f7a1ux9eu4m66s64, .table th.tinytable_css_jte0f7a1ux9eu4m66s64 { text-align: center; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_ickuhbqh8oalh8pegvcq, .table th.tinytable_css_ickuhbqh8oalh8pegvcq { text-align: center; border-top: solid #d3d8dc 0.1em; text-align: center; border-bottom: solid #d3d8dc 0.05em; } .table td.tinytable_css_hlvgykf39uofnz3aja39, .table th.tinytable_css_hlvgykf39uofnz3aja39 { text-align: left; } .table td.tinytable_css_86syo4tjdmn8qh4zr5ab, .table th.tinytable_css_86syo4tjdmn8qh4zr5ab { text-align: center; border-top: solid #d3d8dc 0.1em; text-align: center; } .table td.tinytable_css_0rjxpp7aurdz53f3f4mk, .table th.tinytable_css_0rjxpp7aurdz53f3f4mk { text-align: center; border-bottom: solid black 0.05em; } Dependent Variable:Turnout in 2012 Mayoral Elections Table 13: Regression Models for Self-Reflection Exercises. Bivariate(1) Bivariate(2) Multiple(3) + p Age (median) 0.740*** 0.724*** (0.063) (0.064) Crime Rate (per 1,000) -0.009** -0.005+ (0.003) (0.003) Constant 7.555*** 34.900*** 8.536*** (2.284) (0.319) (2.340) Num.Obs. 625 625 625 R2 0.180 0.015 0.184 R2 Adj. 0.179 0.014 0.182 Please stop here and don’t go beyond this point until we have compared notes on your answers. Exam Preparation We will go through the solutions to the mock exam today. Please prepare this exam and note questions about the module content which we will discuss. Exercises We will be using the crime.csv data set to analyse attitudes and experiences of crime in England and Wales. The dataset contains several variables relating to questions asked about experience and perceptions of crime to a representative sample along with demographic details on the respondents. Data are taken from University of Manchester, Cathie Marsh Institute for Social Research (CMIST), UK Data Service, Office for National Statistics (2019). Data Prep Each respondent was randomly assigned to a different module, indicated by split and only asked a subsection of the questions. The antisocx variable is part of module A and asks for a score from respondents on how much antisocial behaviour is in the neighbourhood. Create a new data set for those who were chosen for the ‘A’ module. Call this crime.a. Working with Regression Analysis Previous research has suggested men perceive more antisocial behaviour in urban areas than rural areas. Create a linear model using only male respondents from crime.a with the dependent variable antisocx and the independent variable rural2. Do your findings support previous research? Test the same model using only women. How do the two models differ? Using interaction effects test whether being in a rural area has a larger effect on women’s perception of antisocial behaviour compared to men’s? How would you test whether being female has an effect on the perception of higher antisocial behaviour in urban compared to rural areas? We will now be using the full crime data frame. The wburgl variable asks respondents how worried they are about being burgled with answers ranging from “Very worried” to “Not at all worried” along with “Not applicable” and “Don’t know”. Recode those with “Not applicable” or “Don’t know” as NAs. Using agegrp7 and wburgl as continuous variables, test the hypothesis that older people are more worried about being burgled. Using a dummy variable test whether those over 65 have are more worried about burglary then those who are younger. Using dummies test whether any of the age groups differ significantly from the youngest age group. What does the intercept in each of the three models represent? Transformation of Variables This Section uses the london_exercises.csv data set. You are already familiar with this data set from Week 7. The data are taken from London Data Store (2013). Unemployment rate, defined as the ratio of people in full time employment to population of working age is often said to be related to crime. Generate an unemployment rate variable for each of the wards. It is theorised that unemployment is a driving factor behind crime rates. Plot a scatter graph with unemployment rate, crime rate, and the regression line that may be used to evaluate the theory. Describe the plot and the best fit line. Plot the graph again excluding wards with a crime rate greater than 500. Describe the plot and the best fit line. Plot another graph excluding wards with a crime rate of over 500 with the crime rate log transformed. Describe the plot and the best fit line. Build both models. Interpret both including the effect size. Which model fits the data better? Going Further If you want to do a little more in R, then please return to the crime.csv data set again. There are five variables which ask how worried the respondent is about being the victim of various crimes: Create an additive variable called worry from these variables so that a score of 0 indicates the respondent answered all “Not at all worried” and a score of 15 indicates the respondent answered all “Very worried.” (Tip: Make sure to clean the variables for NAs). What are the mode, mean and median for worry? Describe the variable educat3. How could it be modified to be used as a continuous variable? Using educat3 as both a continuous and factor variable evaluate the statement “Worry about being a victim of crime is higher in those with lower education levels.” What are the \\(R^2\\) values for the two models calculated? Which is higher? Does this make that model better than the other? Calculate \\(97.5\\%\\) confidence intervals for the coefficients for those with GCSEs and those with Degrees. What does this tell you? "],["homework.html", "Homework", " Homework Work through this week’s flashcards to familiarise yourself with the relevant R functions. Find an example for each NEW function and apply it in R to ensure it works Prepare yourself for the exam in January "],["glossary-8.html", "Glossary", " Glossary Table 21: Glossary Week 10 Term Description asymmetry The notion that while X causes Y, Y does not cause X. It is established with temporal priority, manipulated events, and/or the independence of causes (see Brady (2011)) causal In order to establish a causal relationship, the following criteria must be met concurrently: Relevance of the variables within the broader theoretical and empirical context of the research Clear Theoretical Framework Clear conceptualization Exclusion of alternative explanations Asymmetry Significant (and sufficiently strong) statistical association symmetry In the context of causation, symmetry is understood as the law-like regularity of events. There needs to be a recipe (causal mechanism) which regularly produces effects from causes (see Brady (2011)) "],["flashcards-8.html", "Flashcards", " Flashcards New Functions This Week The data are available as a .csv file. R Functions This Week The data are available as a .csv file. All R Functions So Far The data are available as a .csv file. "],["downloads.html", "Downloads Documents Data Sets – in alphabetical order R Scripts", " Downloads Yes, all solutions and all RScripts are available to you without silly time restrictions. The reason is that I have come to realise that I am operating a module in a university, and not in a kindergarten. Of course, you can download and look at the solutions before you have given the exercises a go yourself first. By all means, cheat. But I can’t promise you that you will learn very much. It’s your choice. Documents PO11Q Bibliography Statistical Tables Worksheet Week 3 Solutions Worksheet Week 4 Solutions Worksheet Week 5 Solutions Worksheet Week 7 Solutions Worksheet Week 8 Solutions Case Study Week 8 Solutions Data Sets – in alphabetical order EU.xlsx Example.xlsx ks2 london_exercises mortality WDI_PO91Q.csv R Scripts Week 2 Solutions Week 3 Solutions Week 4 Solutions Week 7 Solutions Week 8 Solutions "],["glossary-9.html", "Glossary", " Glossary Unless otherwise noted, the definitions are taken from Reiche (forthcoming). Table 22: Glossary for PO91Q Term Description analysis A detailed evaluation of data to discover their structure and relevant information to answer a research question asymmetry The notion that while X causes Y, Y does not cause X. It is established with temporal priority, manipulated events, and/or the independence of causes (see Brady (2011)) attribute A component or characteristic of a concept background concept The broad constellation of meanings and understandings associated with the concept (Adcock &amp; Collier, 2001, p. 531) categorical Describing the qualitative categories of a characteristic, for example different religions causal In order to establish a causal relationship, the following criteria must be met concurrently: Relevance of the variables within the broader theoretical and empirical context of the research Clear Theoretical Framework Clear conceptualization Exclusion of alternative explanations Asymmetry Significant (and sufficiently strong) statistical association central limit theorem In random sampling with a large sample size – where n=30 is usually sufficient – the sampling distribution of the sample mean \\(\\bar{y}\\) will be approximately normally distributed, irrespective of the shape of the population distribution concept Abstract ideas which form the building blocks of theories (Clark et al., 2021, p. 150) conceptualization Formulating a systematized concept through reasoning about the background concept, in light of the goals of research (Adcock &amp; Collier, 2001, p. 531) confidence interval A confidence interval constructs an interval of numbers which will contain the true parameter of the population (e.g. the mean) in \\((1-\\alpha)\\) times of cases. \\(\\alpha\\) is usually chosen to be small, so that our confidence interval has a probability of 95% or 99%. confidence level The confidence level is the probability with which the confidence interval is believed to contain the true parameter of the population and is defined as \\((1-\\alpha)\\) conflation A variable does not belong to the attribute in question, but to a different one (Munck &amp; Verkuilen, 2002, pp. 13–14) constant A variable which does not vary continuous Can assume any value within defined measurement boundaries critical value The critical value is a threshold that determines the boundary for rejecting the null hypothesis (H\\(_0\\)) in a hypothesis test. It is a point on the probability distribution of the test statistic beyond which the null hypothesis is rejected. The critical value is chosen based on the significance level (\\(\\alpha\\)) of the test, which represents the probability of making a Type I error (i.e., rejecting a true null hypothesis). cross-sectional data Look at different units (or cross-sections) \\(i\\) at a single point in time data Derives from the Latin which means . For our purposes it is a collection of numbers (or quantities) for the purpose of analysis data set A collection of numerical values for individual observations, separated into distinctive variables degrees of freedom Degrees of freedom express constraints on our estimation process by specifying how many values in the calculation are free to vary democracy A system in which the population chooses and holds accountable elected representatives through fair, free, and contested, multi-party elections. The human rights, civil rights, and civil liberties of individuals are protected by law dependent variable Is dependent through some statistical or stochastic process on the value of an independent variable descriptive statistics Summarise information about the centre and variability of a variable deviation The deviation \\(d\\) of an observation \\(y_{i}\\) from the sample mean \\(\\bar{y}\\) is the difference between them: \\(d=y_{i}-\\bar{y}\\) dichotomous Can only assume two mutually exclusive, but internally homogeneous qualitative categories discrete The result of a counting process distribution Refers to the display of the values a variable can assume, together with their respective absolute or relative frequency generalisability The ability to apply the findings made on the basis of a representative sample to the population histogram Displays through rectangles the frequency with which the values of a continuous variable occur in specific ranges hypothesis In statistics, a hypothesis is a formal statement about a population parameter or relationship between variables. Hypotheses guide statistical tests to determine whether data support or refute them. The hypothesis suggesting an effect or difference is called the alternative hypothesis. The alternative hypothesis is always paired with a null-hypothesis, suggesting no effect or difference. independent variable Influences or helps us predict the level of a dependent variable. It is often treated as fixed, or “given” in statistical analysis, and is sometimes also called “explanatory variable” interpretation The explanation of results to answer the research question interquartile range The difference between the 3\\(^{\\text{rd}}\\) and the 1\\(^{\\text{st}}\\) quartiles literature review An analytical summary of the literature relating to a particular topic with the objective of identifying a gap and thus motivating a research question mean Is equal to the sum of the observations divided by the number of observations measurement Refers to the selection of a measure or variable median Separates the lower half from the upper half of observations method A tool for systematic investigation mode Is the most frequently occurring value non-probability sampling In non-probability sampling not every unit has the same probability of being sampled normal distribution The Normal Distribution is a bell-shaped probability distribution which is symmetrical around the mean outlier Defined as a value larger than the third quartile plus 1.5 times the interquartile range, or the first quartile minus 1.5 times the interquartile range p-value The p-value indicates the probability of obtaining a result equal to, or even more extreme than the observed value, assuming the null hypothesis is true. Common thresholds for significance are 0.05, 0.01, and 0.001. A smaller p-value suggests stronger evidence against the null hypothesis. The p-value is denoted as \\(p\\). parameter A parameter is the value a statistic would assume in the long run. It is also called the Expected Value percentile In ordered data, the percentile refers to the value of a variable below which a certain proportion of observations falls population Collection of all cases which possess certain pre-defined characteristics population distribution The probability distribution of the population primary data Primary data are data you have collected yourself probability Refers to how many times out of a total number of cases a particular event occurs. We can also see it as the chance of a particular event occurring probability sampling In probability sampling all units have the same probability of entering the sample. In addition, all possible combinations of n cases must have the same probability to be selected QM The process of testing theoretical propositions through the analysis of numerical data in order to provide an answer to a research question quartile Divides ordered data into four equal parts and indicates the percentage of observations that falls into the respective quartile and below range The difference between the largest and the smallest observation redundancy Two or more variables measure the same sub-attribute (Munck &amp; Verkuilen, 2002, p. 13) reliability Refers to the extent to which repeated measurement produces the same results representative sample A sample which contains all characteristic of the population in accurate proportions research question A specific enquiry relating to a particular topic or subject. It forms the starting point of the research cycle sample A sub-group of the population sample distribution The probability distribution of a sample sampling The process of selecting sampling units from the population sampling distribution The probability distribution of a sample statistics, such as the mean. It can be derived from repeated sampling, or by estimation sampling error The extent to which the mean of the population and the mean of the sample differ from one another sampling method The way the sample is created secondary data Secondary data are data which have been collected by somebody else significance level The significance level, denoted by \\(\\alpha\\), is the threshold used in hypothesis testing to determine if a result is statistically significant. It represents the probability of rejecting the null hypothesis when it’s actually true (a Type I error). Common levels are 0.05 or 0.01, indicating 5% or 1% risk. We will cover this properly in Week 9. significance test A significance test is a statistical method used to determine whether observed data provide enough evidence to reject a null hypothesis. It calculates a probability of observing data as extreme as, or more extreme than, the actual sample results, assuming the null hypothesis is true Social Sciences Are concerned with the study of society and seek to scientifically describe and explain the behaviour of actors standard deviation The standard deviation s is defined as \\[\\begin{equation*}s=\\sqrt{\\frac{\\text{sum of squared deviations}}{\\text{sample size} -1}}=\\sqrt{\\frac{\\Sigma(y_{i} - \\bar{y})^2}{n-1}}\\end{equation*}\\] standard error The standard deviation of the sampling distribution. It is defined as:\\[\\begin{equation*}\\sigma_{\\bar{y}} = \\frac{\\sigma}{\\sqrt{n}}\\end{equation*}\\] symmetry In the context of causation, symmetry is understood as the law-like regularity of events. There needs to be a recipe (causal mechanism) which regularly produces effects from causes (see Brady (2011)) systematized concept A specific formulation of a concept used by a given scholar or group of scholars; commonly involves an explicit definition (Adcock &amp; Collier, 2001, p. 531) t-distribution The t-Distribution is bell-shaped and symmetrical around a mean of zero. Its shape is dependent on the degrees of freedom in the estimation process. test statistic A test statistic is a value calculated from the sample data that is used to decide whether to reject the null hypothesis (H\\(_0\\)) in a hypothesis test. It quantifies the degree to which the observed data diverges from what is expected under the null hypothesis. In a t-test, the test statistic is a t-value, which measures the distance between the sample mean and the (hypothesised) population mean, expressed in units of standard errors. theory A formal set of ideas that is intended to explain why something happens or exists (Oxford Learner’s Dictionaries, n.d.) Type I Error A Type I Error occurs when a null hypothesis (H\\(_0\\)) that is actually true is incorrectly rejected. It is also known as a false positive errors, as it suggests that an effect of difference exists, when, in fact, it does not. The probability of committing a Type I Error is denoted by the significance level (\\(\\alpha\\)) of the test, which is typically set before conducting the test (e.g. \\(\\alpha\\) = 0.05). This means that there is a 5% chance of rejecting the true null hypothesis. Type II Error A Type II Error occurs when a null hypothesis (H\\(_0\\)) that is actually false is incorrectly accepted (or not rejected). It is also known as a false negative error, as it suggests that no effect or difference exists when, in fact, there is one. validity The extent to which the measure (variable) you choose genuinely represents the concept in question. The word comes from the Latin word “validus” which means “strong” variable An element of a conceptual component which varies. We also call these “measures” variance Is equal to the squared standard deviation z-score The z-score, sometimes also referred to as z-value, expresses in units of standard deviation how far an observation of interest falls away from the mean. It is defined as \\[\\begin{equation*}z = \\frac{\\text{observation} - \\text{mean}}{\\text{standard deviation}}=\\frac{y-\\mu}{\\sigma}\\end{equation*}\\] "],["list-of-references.html", "List of References", " List of References Adcock, R., &amp; Collier, D. (2001). Measurement Validity: A Shared Standard for Qualitative and Quantitative Research. American Political Science Review, 95(3), 529–546. Agresti, A. (2018). Statistical Methods for the Social Sciences (Fifth). Harlow: Pearson. Arel-Bundock, V. (2022). modelsummary: Data and Model Summaries in R. Journal of Statistical Software, 103(1), 1–23. Boix, C., Miller, M., &amp; Rosato, S. (2018). Boix-Miller-Rosato Dichotomous Coding of Democracy, 1800-2015 (Version V3). Harvard Dataverse. Brady, H. E. (2011). Causation and Explanation in Social Science. In R. Goodin (Ed.), The Oxford Handbook of Political Science. Oxford University Press. Clark, T., Foster, L., Sloan, L., &amp; Bryman, A. (2021). Bryman’s Social Research Methods (Sixth). Oxford: Oxford University Press. European Comission. (n.d.). Eurostat – Your Key to European Statistics. available online at https://ec.europa.eu/eurostat/data/database. GOV.UK. (2013). National Statistics: Income and tax by Parliamentary constituency. available online at https://www.gov.uk/government/statistics/income-and-tax-by-parliamentary-constituency-2010-to-2011. House of Commons Library. (n.d.). Data Dashboard. available online at https://commonslibrary.parliament.uk/type/data-dashboard/. King, G. (1995). Replication, replication. PS: Political Science and Politics, 28(3), 541–559. London Data Store. (2010). London Parliamentary Constituency Profiles 2010. available online at https://data.london.gov.uk/dataset/london-parliamentary-constituency-profiles. London Data Store. (2013). Ward Profiles and Atlas. available online at https://data.london.gov.uk/dataset/ward-profiles-and-atlas. Marshall, M. G., &amp; Gurr, T. R. (2020). Polity V Project: Political Regime Characteristics and Transitions, 1800-2018. available online at http://www.systemicpeace.org/inscrdata.html. Munck, G. L., &amp; Verkuilen, J. (2002). Conceptualizing and Measuring Democracy: Evaluating Alternative Indices. Comparative Political Studies, 35(5), 5–34. Oxford Learner’s Dictionaries. (n.d.). available online at https://www.oxfordlearnersdictionaries.com/. Reiche, F. (forthcoming). Introduction to Quantitative Methods for the Social Sciences. Oxford: Oxford University Press. Stimson, J. A. (n.d.). Professional Writing in Political Science: A Highly opinionated Essay. available online at http://stimson.web.unc.edu/files/2018/02/Writing.pdf. Tufte, E. R. (2001). The Visual Display of Quantitative Information (Second). Cheshire, Conn: Graphics Press. University of Manchester, Cathie Marsh Institute for Social Research (CMIST), UK Data Service, Office for National Statistics. (2019). Crime Survey for England and Wales, 2013-2014: Unrestricted Access Teaching Dataset. available online at https://doi.org/10.5255/UKDA-SN-8011-1. World Bank. (2024). World Development Indicators. available online at https://datacatalog.worldbank.org/dataset/world-development-indicators. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
