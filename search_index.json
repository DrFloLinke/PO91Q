[["index.html", "PO91Q: Seminar Companion Preface", " PO91Q: Seminar Companion Dr Flo Reiche Department of Politics and International Studies University of Warwick Last Updated 31 March, 2025 Preface Welcome to “Fundamentals in Quantitative Research Methods”. This module has two main aims: introduce to secondary data acquisition, management and analysis in the social sciences; prepare to attend further statistical training and make use of statistics in future research works, academic (such as master’s or PhD dissertations) or not. The module does not require any prior knowledge of mathematics or statistics. You only need to understand the importance of statistics for empirical social sciences, and show willingness to learn about them. This is the online companion to the seminars on PO91Q. It hosts all of the material needed for the seminars and replaces any physical worksheets. This is environmentally friendlier, and has the great advantage that when working with R, it allows you to copy/paste code directly from the code chunks in this online companion. I hope you find this useful! "],["companion-features.html", "Companion Features", " Companion Features You will find embedded in the text four different types of boxes which serve different purposes: This box appears whenever I want you to stop at a particular point in the worksheet and to flag up to me that you are done. This appears when you need to be careful with your coding in R to avoid problems. Some explanations that will hopefully make your work with this webpage or learning the material itself easier. We will start working with R in week 5, and I have recorded some videos to ease you into working with the program. A brief question which tests your understanding of the previous material. A definition we encountered in the lecture. "],["accessibility.html", "Accessibility", " Accessibility For those of you who prefer a dark background, like me, you can select this option from the menu at the top of the page. Click the “A” symbol, and then you can choose between “white”, “sepia”, or “night”. The companion uses the font “Lexend”. Lexend fonts are intended to reduce visual stress and so improve reading performance. Initially they were designed with dyslexia and struggling readers in mind, but Bonnie Shaver-Troup, creator of the Lexend project, soon found out that these fonts are also great for everyone else. "],["introduction-to-r.html", "Introduction to R Installation R - Getting Started RScript First Steps in R The Working Directory R Packages Saving", " Introduction to R Installation Today we start working with R and the first step is to install the program. Please follow these instructions: Go to https://cran.r-project.org/mirrors.html and select a server from which you want to download R. It is convention to do this from the server which is nearest to you. Follow on-screen instructions and install the program. Go to https://rstudio.com/products/rstudio/download/ and download RStudio Desktop which is free. Install the program. Now open RStudio - you do not need to open R itself, as we will be operating it through RStudio. Whilst you need to install both R and RStudio, we will never be working with R directly. Instead, we will be operating it through RStudio. R - Getting Started In this worksheet and also in all other presentations and documents I use on this module, I am using two different fonts: Font for plain text A typewriter font for R functions, values, etc. I am also regularly including “screenshots” of operations in R with their output. Whenever you see these, please replicate them on your own computer. To start, let’s have a look at RStudio itself. When you open the programme, you are presented with the following screen: Figure 1: RStudio It has – for now – three components to it. On the left hand-side you see the so-called Console into which you can enter the commands, and in which also most of the results will be displayed. On the right hands side, you see the Workspace which consists of an upper and a lower window. The upper window has three tabs in it. The tab Environment will provide you with a list of all the data sets you have loaded into R, and also of the objects and values you create (more on that later). Under the History tab, you find a history (I know, who would have thought it) of all the commands you have used. This can be very useful to retrace your steps. In the Connections tab you can connect to online sources. We will not use this tab. In the lower window, you have five tabs. Under Files you find the file structure of your computer. Once you have set a working directory (more on that in a moment), you can also view the files in your working directory here which gives you a good overview of the files you need to refer to for a particular project. The Plots tab will display the graphs we will be producing. Packages form the heart and soul of R and they make the program as powerful as it is (again, more on that later). RStudio also has a Help function, which is rarely very illuminating. I usually search for stuff online on “stackexchange”, as there is a large community of R users out there who share their knowledge and solutions to problems. We won’t use the last tab Viewer. Introduction to R Studio If you can’t get enough of my delightful German accent, then I have some videos for you in which I go through the respective components of the worksheet on screen. Here is the first: RScript If you read the previous section carefully, you will have noticed that I wrote that you can enter the commands” in the Console. You can, but you shouldn’t. What you should be using instead is an RScript. An RScript is a list of commands you use for a project (an essay, your dissertation, an article) to calculate quantities of interest, such as descriptive statistics in the form of mean, median and mode, and produce graphs. One of the foundations of scientific research is “reproducibility”“, or”replicability”. This means that “sufficient information exists with which to understand, evaluate, and build upon a prior work if a third party could replicate the results without any additional information from the author.” King (1995, p. 444, emphasis removed) This principle applies in academia more generally, because only if you understand what a person has done before you, you can pick their work up whether they left it, and push the boundaries of knowledge further. But a bit closer to home, it is also relevant for conducting quantitative research in assessments. We require you to submit an RScript (or a “do file” if you use Stata) now together with your actual essay. This is not only to check what you have done; data preparation is often the most time-consuming part (as you will soon discover), and this is a way to gain recognition for this work. So it is actually to your advantage, and not a mere plagiarism check. The creation of an RScript will allow you to open the raw data, and by running the script, to bring it to exactly where you left off. This saves you saving data sets which can take up a lot of work. If you back the script up properly, you also have an insurance against losing all your work a day before the assessment is due. To create an RScript, click File \\(\\rightarrow\\) New File \\(\\rightarrow\\) RScript. A fourth window opens, and your screen will now look something like this: Figure 2: The RScript Window You can now write your commands in the RScript, where a new line (for now) means a new command. If you want to execute a command, put the cursor on the line the command is on and press “command” / “enter” simultaneoulsy on a Mac and “Ctrl” / “Enter” on Windows. If you precede a line with #, you can write annotations to yourself, for example explaining what you do with a particular command. More on this in the next sub-section. Figure 3 shows the start of the RScript for this worksheet. I prefer a dark background, it’s easier on the eyes, especially when you work with R for long periods. You can change the settings in: Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Appearance \\(\\rightarrow\\) Twilight. Figure 3: Example of an RScript More Themes If you copy and paste the following code chunks into your “Console” and run one at a time, you will have even more themes1 to choose from: install.packages( &quot;rsthemes&quot;, repos = c(gadenbuie = &#39;https://gadenbuie.r-universe.dev&#39;, getOption(&quot;repos&quot;)) ) rsthemes::install_rsthemes() You can also download Flo’s Dark Theme2 and then “add” it at the bottom of the “Appearance” menu. Appearance RScript Structure Well, I am German, and I like things neat and tidy, so I feel almost compelled to discuss how to properly organise an RScript. But apart from genetical dispositions, a well-organised RScript is also very much in the spirit of reproducibility. It simply makes sense to structure an RScript in such a way that another researcher is able to easily read and understand it. First of all, which commands to include? If you introduce me to your current girlfriend or boyfriend, I have no interest in learning about all your past relationships; they have not worked out. In a similar fashion, nobody wants to read through lines of code that are irrelvant. So you will only include in the RScript those commands which produce the output you actually include in the essay or article. I stated above that if you precede a line with #, you can write annotations to yourself. This is also a useful way to structure an RScript, for example into exercise numbers, sections of an essay /article, or different stages of data preparation (which we will be doing in due course). RScript Structure First Steps in R But enough of the preliminary talk, let’s get started in R. In principle, you can think of R as a massive and powerful calculator. So I will use it as such to start of with. If you want to know what the sum of 5 and 3 is, you type: 5+3 and execute the line as previously explained. In everything that is to follow, commands will be shown in boxes with the output underneath preceded by a number in square brackets. So, including result, the calculation would look like this: 5+3 [1] 8 where the [1] indicates that the 8 is the first component of the result. In this case, we only have one component, so it’s superfluous really, but we will soon encounter situations in which results can have a number of different items. You can copy the code from this page by hovering over the code chunk and clicking the icon in the top-right hand corner. You can then paste it into your RScript. A fundamental component of R is objects. You can define an object by way of a reversed arrow, and you can assign values, characters, or functions to them. If we want to assign the sum of 5 and 3 to an object called result, for example, we call3 result &lt;- 5+3 If we now call the object, R will return its value, 8. result [1] 8 Make a habit of adding a note underneath each code chunk in your RScript (preceded with a #) in which you translate the code into plain English. This is especially useful for the lengthy complex chunks. The Working Directory It is imperative that you create a suitable filing system to organise the materials for all of your modules. At the very least you should have a folder called “University” or similar, in which you have a sub-folder for each module you take. In those modules in which you are working with R, you need to extend this system a little. I have created a schematic of what I have in mind in Figure 4. Figure 4: Folder Structure You see that there is a sub-folder for each week of the module (I have only done three for illustrative purposes), and that each of these folders is divided into lecture and seminar in turn. Into these you can place the lecture and seminar materials, respectively. Create this system now for PO11Q. R works with so-called Working Directories. You can think of these as drawers from which R takes everything it needs to conduct the analysis (such as the data set), and into which it puts everything it produces (such as graph plots). As this will be an R-specific drawer within the seminar, create yet another sub-folder in your seminar folder, and call it something suitable, such as “PO11Q_Seminar_Week 1”. Do NOT call this “Working Directory”, as you will have many of those, rendering this name completely meaningless. Save the file EU.xlsx into this folder. Data are taken from European Comission (n.d.). Please set up this structure now. If I find you using a random folder on your desktop named “working directory” in the coming weeks, I am going to implode! I mean it. Now we need to tell R to use this folder. If you know the file structure of your computer you can simply use the setwd() command, and enter the path. Here is an example from my computer: setwd(&quot;~/Warwick/Modules/PO91Q/Seminars/Week 1/R Week 1&quot;) If you don’t know the file structure of your computer, then you can click Session \\(\\rightarrow\\) Set Working Directory \\(\\rightarrow\\) Choose Directory. Working Directory R Packages It would be difficult to overstate the importance of packages in R. The program has a number of “base” functions which enable the user to do many different basic things, but packages are extensions that allow you to do pretty much anything and everything with this software - this is one of the reasons why I love it so much. The first package we need to use will enable us to load an Excel sheet into R. It is called readxl. You can install any package with the command install.packages() where the package name goes, wrapped in quotation marks, into the brackets: install.packages(&quot;readxl&quot;) We can then load this package into our library with the library() command. library(readxl) Once you close R at the end of a session, the library will be reset. When you reopen R, you have to load the packages you require again. But you do not have to install them again. Saving Please now save this RScript into the same folder (working directory) as the raw data. When R asks before closing, there is no need to save the workspace or the data, as running the RScript on the raw data will bring you precisely to where you left off. Source: https://www.garrickadenbuie.com/project/rsthemes/↩︎ This is a variation of the Dracula Theme.↩︎ To “call” means to execute a command.↩︎ "],["exercises.html", "Exercises Core Exercises Going Further", " Exercises Core Exercises Execute the following lines of command. Each time try and explain what is happening, using statistical terms as in the slides and textbooks. If you know another statistical software, spot commonalities and differences with R. Experiment variations on your own to make sure your interpretations are right. a&lt;-1 class(a) a b&lt;-2 b c&lt;-a+b c d&lt;-2*c d e&lt;-c(10,20,30) class(e) e e*10 f&lt;-c(&quot;Small&quot;,&quot;Medium&quot;,&quot;Big&quot;) class(f) f e[1] f[3] mean(e) mean(f) sd(e) ?sd example(sd) g&lt;-c(e,f) g h&lt;-rbind(e,f) i&lt;-cbind(e,f) class(h) h[1,] i[,2] h[1,2] ls() ls.str() Download the “European Social Survey” dataset from https://www.europeansocialsurvey.org/data. You will need to register and receive a confirmation email. Select round 9 (2018), SPSS version (.sav). Download also: the questionnaire the variable list the “showcards” the Source project instructions Take a bit of time to make sure you understand what each of these documents is for. Can you tell: what is(are) the population(s) of reference? how was(were) the sample(s) composed? how do interviewers reach respondents? what is the method for questionnaire administration? Use SPSS to open the dataset. SPSS is pre-installed on Warwick PCs. Otherwise, download and install it for free (after logging in): https://www2.warwick.ac.uk/services/its/servicessupport/software/list/spss In the dataset open in SPSS, identify key statistical elements: Cases and their ID Variables and values Categorical, ordinal and numeric variables We can now pick up where we left on the worksheet. Make sure that you have created an appropriate folder structure and set your working directory. Also ensure you have loaded the foreign package. setwd(&quot;~/Warwick/Modules/PO91Q/Seminars/Week 1/Worksheet&quot;) # This is my WD library(foreign) # Loads the package for importing data ESS&lt;-read.spss(&quot;ESS9e03.sav&quot;,to.data.frame=TRUE,max.value.labels=10) # Imports the SPSS data with the right R format # (ignore Warning messages in red) # (&#39;to.data.frame&#39; converts the SPSS dataset into an R dataset) # (&#39;use.missings=TRUE&#39; means you do not classify the values # coded as missing in SPSS as missing in R) attach(ESS) View(ESS) # Compare with the SPSS view # Can you sort the data? # Can you filter the data? nrow(ESS) ncol(ESS) dim(ESS) names(ESS) str(ESS) head(ESS) # Can you imagine what the 6 functions above are for? # Afterwards, check your answers using &#39;?function&#39; # to get the help file for each function. Execute: table(cntry). What uses could you make of this variable in research? Going Further In ESS questionnaire, showcards and variable list, identify the questions about: languages spoken by respondent R R’s educational background R’s relationship to politics How many questions about each topic can you find? How suitable would this survey be to study these three topics? Select one of the three topics above according to your interest. Select a set of three variables of particular interest (called X, Y and Z below). Make sure you understand the meaning of X, Y and Z. X Y Z What do these return? table(X) plot(X) Download the “British Social Attitudes” Survey 2016 from the UK Data service website. You will need to register, validate your registration through email, indicate that this is within an academic course and agree with conditions. Choose .tab and .sav format. Open the .tab dataset with Excel and record as .xlsx file Open the .sav dataset with SPSS Import from the SPSS file into R (same as above with ESS) Try to understand the differences between the 3 softwares, their respective advantages and disadvantages. How does the BSA compare with the ESS regarding the three topics listed above? Download the World Values Study 7 from Gesis: https://dbk.gesis.org/dbksearch/sdesc2.asp?no=4804. Select all relevant files. How does the EVS compare with the ESS and BSA regarding the 3 topics above? "],["homework-for-week-2.html", "Homework for Week 2", " Homework for Week 2 Finish working through this worksheet. Add a note underneath each code chunk in your RScript (by starting the line with #), translating the code into plain English. This will help you learn the vocabulary and grammar of R quicker. If you are unsure what individual functions mean, you can find a .csv file with a full list for each week underneath the flashcards (see below, but here is an example). Read the required literature for week 2. Work thoroughly through chapters 7 and 8 of the Fogarty book to make sure you are familiar with all the relevant commands to produce descriptive statistics and graphs with R. Work through this week’s flashcards to familiarise yourself with the relevant R functions. Find an example for each NEW function and apply it in R to ensure it works "],["glossary.html", "Glossary", " Glossary Table 1: Glossary Week 1 Term Description analysis A detailed evaluation of data to discover their structure and relevant information to answer a research question data Derives from the Latin which means . For our purposes it is a collection of numbers (or quantities) for the purpose of analysis interpretation The explanation of results to answer the research question literature review An analytical summary of the literature relating to a particular topic with the objective of identifying a gap and thus motivating a research question method A tool for systematic investigation QM The process of testing theoretical propositions through the analysis of numerical data in order to provide an answer to a research question research question A specific enquiry relating to a particular topic or subject. It forms the starting point of the research cycle Social Sciences Are concerned with the study of society and seek to scientifically describe and explain the behaviour of actors theory A formal set of ideas that is intended to explain why something happens or exists (Oxford Learner’s Dictionaries, n.d.) "],["flashcards.html", "Flashcards", " Flashcards From this week onwards you will find here some flashcards which should help you learn the purpose of R functions on PO91Q. These are divided into: New Functions this week, containing only those functions that we encounter for the first time All Functions this week, containing all functions we have used in that particular week All Functions up to now, containing all functions used on the module up to that particular week. You can download a .csv file containing the functions and the descriptions for each of these underneath the flashcard window. Should you wish to create your own flashcards, for example, with more difficult-to-remember functions, I have written some instructions how to go about it. R Functions This Week The data are available as a .csv file. "],["data-manipulation-in-r.html", "Data Manipulation in R Self-Reflection Questions – Group Work Opening your Data Set Viewing the Data Variable Types in R Sub-Setting Data Ordering Data Grouping Data Combining Ordering and Grouping Data Descriptive Statistics Graphs Basic Graphs Advanced Graphs Even More Advanced Graphs Organising Code in the RScript Exercises Captions for Tables and Figures in Word", " Data Manipulation in R Self-Reflection Questions – Group Work Why is it good practice to always use an RScript, rather than simply entering your commands into the console in R? Give an example of an unordered factor variable and an ordered one. Why are descriptive statistics useful? Give an example of a situation where you would need to transform a character variable into a factor variable. Why should you always create a new variable, instead of overwriting an existing one? These questions are taken from Reiche (forthcoming). Please stop here and don’t go beyond this point until we have compared notes on your answers. Opening your Data Set We are now ready to open a data set in R - where it is called a “data frame”. For this, we create a new object EU, and ask R to read “Sheet 1”” of the Excel file “EU.xlsx” which we placed in the working directory earlier library(readxl) EU &lt;- read_excel(&quot;EU.xlsx&quot;, sheet=&quot;Sheet1&quot;) We can now use our data in R! Loading the Data Set Please do not use the “Import Dataset” button in the Environment, but do this properly, manually. We sometimes need to set options for importing data sets, and the “pointy, clicky” approach won’t be able to offer you what you need. Viewing the Data Unless you have been cheeky and opened the file in Excel to have a look, you have no idea yet, what the data look like. So it’s a good idea to view the data frame before doing anything with it. You can use the View() command to see the data frame: View(EU) If you only want to see the first 6 observations of each variable, use the head() command: head(EU) # A tibble: 6 × 5 country pop18 access area GDP_2015 &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Belgium 11413058 1951 30280 4.66e11 2 Bulgaria 7050034 2007 108560 1.22e11 3 Czechia 10610055 2004 77230 3.19e11 4 Denmark 5781190 1973 42430 2.46e11 5 Germany 82850000 1951 348540 3.60e12 6 Estonia 1319133 2004 42390 3.51e10 If you simply want to know the variable names in the data frame, type: names(EU) [1] &quot;country&quot; &quot;pop18&quot; &quot;access&quot; &quot;area&quot; &quot;GDP_2015&quot; The next one is a very important command, because it reveals not only the variable names and their first few observations, but also the nature of each variable (numerical, character, etc.). It is the str() command, where “str” stands for structure: str(EU) tibble [28 × 5] (S3: tbl_df/tbl/data.frame) $ country : chr [1:28] &quot;Belgium&quot; &quot;Bulgaria&quot; &quot;Czechia&quot; &quot;Denmark&quot; ... $ pop18 : num [1:28] 11413058 7050034 10610055 5781190 82850000 ... $ access : num [1:28] 1951 2007 2004 1973 1951 ... $ area : num [1:28] 30280 108560 77230 42430 348540 ... $ GDP_2015: num [1:28] 4.66e+11 1.22e+11 3.19e+11 2.46e+11 3.60e+12 ... You can see that R has recognised most variables as numerical, one is displayed as a character variable. This is appropriate for some variables, such as pop18, but not for the ordinal variable access which is ordinal. We need to recode it, and all other variables we are unhappy with. Variable Types in R R distinguishes between a number of different variable types and here is a broad overview of them. This will help you in deciding which descriptive statistics to calculate, or into which variable type you need to recode (next step) to achieve what you want. There are two general types: numeric – numbers character (also called string) – letters Within numeric we can distinguish between the following: factor - nominal ordered factor - ordinal integer - numeric, but only “whole” numbers (discrete) numeric - any number (interval or ratio) Numerical variables are already in the data set, we have to attend to nominal and ordinal variables. Nominal Variables In terms of the variable types we encountered in the lecture this week, the country name is a nominal variable. So we need to tell R to turn this into a factor variable. We do this as follows: EU$country = factor(EU$country) Ordinal Variables As mentioned above, the variable access should be ordinal, and therefore has to be turned into an ordered factor. The command which follows is almost identical to producing a factor variable, only that we add the option ordered = TRUE at the end: EU$access_fac = factor(EU$access, ordered = TRUE) If you are familiar with European Studies, you will know that each accession wave has got a particular name. The 1973 enlargement, for example, is called the “First Enlargement”, the 1981 wave the Mediterranean Enlargement, and so forth. Let us create a new variable which uses these names instead of the years. This process is a little more involved, and requires a new package to be installed and loaded: dplyr. This package is part of the so-called tidyverse which is a suite of packages designed to make working with R simpler and commands shorter. You can install all of them by calling install.packages(\"tidyverse\"). We then load the tidyverse with: library(tidyverse) The command which follows takes a little explaining. We start by stating the dataframe we wish to assign the result to, EU. Then we name the data frame that contains the data we wish to manipulate, here also EU. The symbol which follows, \\%&gt;\\%, reads as “and then”, and is called a “pipe”. So we take the data frame EU “and then” carry out a function called mutate. It creates a new variable. This function in turn defines the new variable wave by recoding the variable access_fac. The command then specifies all categories of the “old” variable access_fac and what their respective values in the “new” variable wave are going to be. The categories in each are set in quotation marks, as they are factor / character categories. EU &lt;- EU %&gt;% mutate(wave = recode(access_fac, &#39;1951&#39;=&quot;Founding&quot;, &#39;1973&#39;= &quot;First&quot;, &#39;1981&#39;= &quot;Mediterranean&quot;, &#39;1986&#39; = &quot;Mediterranean&quot;, &#39;1995&#39; = &quot;Cold War&quot;, &#39;2004&#39; = &quot;Eastern&quot;, &#39;2007&#39; = &quot;Eastern&quot;, &#39;2013&#39; = &quot;Balkans&quot;)) Please note that some colleagues in the department have decided to take a random dislike to the tidyverse as one of currently 22,250 packages4 and might therefore require you to use base R in their modules. I am still using the tidyverse however, as: I think it is nonsense to exclude one package in particular my textbook, which is going to be the main textbook for this module once it is published, uses the tidyverse GGPLOT2 which is part of the tidyverse simplifies code for generating figures significantly and will do for all but the most specific requirements a lot of support on stackexchange is geared toward the tidyverse as a lot of US-based data scientists work with this package, and so you will find it easier to solve problems But to keep everybody happy, I am providing the base R code whenever possible in a collapsible section like this one: Base R Solution EU$wave &lt;- NA EU$wave[EU$access_fac==&#39;1951&#39;] &lt;- &quot;Founding&quot; EU$wave[EU$access_fac==&#39;1973&#39;] &lt;- &quot;First&quot; EU$wave[EU$access_fac==&#39;1981&#39;] &lt;- &quot;Mediterranean&quot; EU$wave[EU$access_fac==&#39;1986&#39;] &lt;- &quot;Mediterranean&quot; EU$wave[EU$access_fac==&#39;1995&#39;] &lt;- &quot;Cold War&quot; EU$wave[EU$access_fac==&#39;2004&#39;] &lt;- &quot;Eastern&quot; EU$wave[EU$access_fac==&#39;2007&#39;] &lt;- &quot;Eastern&quot; EU$wave[EU$access_fac==&#39;2013&#39;] &lt;- &quot;Balkans&quot; EU$wave &lt;- factor(EU$wave, ordered = TRUE) Here, we first create a new, empty variable called wave in the EU data set. We then create new values , for example Founding in the variable EU$wave for the condition (this is what the square brackets [ ] do) that the variable access_fac in the EU data set, equals a specific value. For Founding this is is 1951. The last step is to turn the wave variable into an ordered factor. But back to the recoding exercise itself. As the original variable access_fac was already an ordered factor, R (or the mutate function to be precise) also returns wave as an ordered factor. Had we not done this, wave would have been an unorderd factor (aka nominal variable). You can specify in an option to the mutate function whether you want the factor to be ordered or not: EU &lt;- EU %&gt;% mutate(wave = recode(access_fac, &#39;1951&#39;=&quot;Founding&quot;, &#39;1973&#39;= &quot;First&quot;, &#39;1981&#39;= &quot;Mediterranean&quot;, &#39;1986&#39; = &quot;Mediterranean&quot;, &#39;1995&#39; = &quot;Cold War&quot;, &#39;2004&#39; = &quot;Eastern&quot;, &#39;2007&#39; = &quot;Eastern&quot;, &#39;2013&#39; = &quot;Balkans&quot;), ordered=TRUE) An alternative procedure, producing exactly the same result is to use the cut() function on the access variable which literally cuts up a variable into chunks at the points we specify. This only works on numerical variables! We are fine to use it here, as we didn’t change access, and it is still numerical. Incidentally, this shows you the benefit of always creating a new variable instead of overwriting the original: there is no “back” button in R, if you mess up, you will have the pleasure to start from the beginning. Again, we use the mutate function, this time naming our new variable wave1 (so as not to overwrite the wave variable we created with the recode() function). This time, we cut up the original variable at the accession years, and specify the levels, this time as labels. Labels denominate the output, whilst level are input. A factor only knows levels which is set by the label function. Here we have already created the levels with the cut() function, and assign labels to these in the second step. EU &lt;- EU %&gt;% mutate(wave1=cut(access, breaks=c(1950, 1951, 1973, 1986, 1995, 2007, 2013), labels=c(&quot;Founding&quot;,&quot;First&quot;, &quot;Mediterranean&quot;, &quot;Cold War&quot;, &quot;Eastern&quot;, &quot;Balkans&quot;))) levels(EU$wave) [1] &quot;Founding&quot; &quot;First&quot; &quot;Mediterranean&quot; &quot;Cold War&quot; &quot;Eastern&quot; [6] &quot;Balkans&quot; Base R Solution EU$wave &lt;- cut(EU$access, breaks=c(1950, 1951, 1973, 1986, 1995, 2007, 2013), labels=c(&quot;Founding&quot;,&quot;First&quot;, &quot;Mediterranean&quot;, &quot;Cold War&quot;, &quot;Eastern&quot;, &quot;Balkans&quot;)) Recoding a Factor Variable Recoding Ordered Factor Variables Binary Dummy Very often in political science we have yes/no scenarios, such as democracy yes or no, civil war, yes or no, etc. To analyse these scenarios, we can create so-called “dummy variables”. In the present example, let’s specify for each country whether it has been a founding member of the EU. It is a factor variable and so we do this exactly the same way as our initial recoding of the wave variable above: EU &lt;- EU %&gt;% mutate(founding = recode(access_fac, &#39;1951&#39;=&quot;Yes&quot;, &#39;1973&#39; = &quot;No&quot;, &#39;1981&#39; = &quot;No&quot;, &#39;1986&#39; = &quot;No&quot;, &#39;1995&#39; = &quot;No&quot;, &#39;2004&#39; = &quot;No&quot;, &#39;2007&#39; = &quot;No&quot;, &#39;2013&#39; = &quot;No&quot;)) # OR, much shorter EU &lt;- EU %&gt;% mutate(founding = factor(ifelse(access_fac==&quot;1951&quot;, &quot;Yes&quot;, &quot;No&quot;), levels =c(&quot;Yes&quot;, &quot;No&quot;))) # the result is the same str(EU$founding) Factor w/ 2 levels &quot;Yes&quot;,&quot;No&quot;: 1 2 2 2 1 2 2 2 2 1 ... Base R Solution Using ifelse, this is very similar, only the pipe disappears: EU$founding &lt;- factor(ifelse(EU$access_fac==&quot;1951&quot;, &quot;Yes&quot;, &quot;No&quot;), levels =c(&quot;Yes&quot;, &quot;No&quot;)) The ifelse function is very handy, and is therefore worth explaining in a little more detail. It reads as ifelse('condition', 'if condition met, then', 'otherwise'). So, the above code reads as “if the value of the variable access_fac is equal to 1951, then code the observation as ‘Yes’, otherwise as ‘No’”. Sub-Setting Data When we start analysing data, we rarely need all data at the same time. We might not need some variables, at all, for example, or we only want to work with certain observations, such as those countries in the “founding” wave. In these cases, we can subset the data. I will show you some examples of subsetting now. By Variable If you are sure you won’t need a variable (remember, there is no back button), you can simply drop (i.e. delete) it. Let’s do this with the area variable: EU$area &lt;- NULL If we are dropping multiple variables, we can either perform this operation each time, or use another command which allows us to operate with multiple variables at the same time. The select() command comes from the tidyverse package and specifies which variables we wish to keep: EU_pop &lt;- select(EU, country, pop18, access_fac, founding) This creates a new data frame called EU_pop containing only the variables country, pop18, access_fac, and founding. Base R Solution The package documentation offers some basic instructions how to convert the tidyverse (or dyplyr, to be precise) code into base R. But here is the solution for the previous code chunk: EU_pop &lt;- subset(EU, country, pop18, access_fac, founding) We can, however, use the same command and tell R which variables to drop by adding a minus sign in from of the variables we want to delete. The following command produces exactly the same result as the one before: EU_pop1 &lt;- select(EU, -access, -GDP_2015) By Observation Instead of dropping and keeping variables, we can do the same thing to individual observations. Here, we use the slice() command (like a cake) and specify which slices we want to drop or keep. For example to drop the Benelux countries we would delete observations 1, 16 and 19: EU_nobenelux &lt;- slice(EU, -1, -16, -19) Base R Solution EU_pop &lt;- EU[c(-1, -16, -19),] Alternatively, if we were only interested in Benelux countries we would subset to only those observations: EU_benelux &lt;- slice(EU, 1, 16, 19) Base R Solution EU_pop &lt;- EU[c(1, 16, 19),] Keep if a variable has a certain value One of the most useful commands is filter(), as it allows us to keep all observations for which the value of a variable is of a particular number. For example if we wanted to conduct an analysis with all countries which have a population in excess of 10 million we could subset by: EU_pop_large &lt;- filter(EU, pop18 &gt; 10000000) Base R Solution EU_pop_large &lt;- subset(EU, pop18 &gt; 10000000) Here is a list of some operators you can use for this purpose: Operator Description &lt; less than &lt;= less than or equal to &gt; greater than &gt;= greater than or equal to == exactly equal to != not equal to !x Not x x | y x OR y x &amp; y x AND y Subsetting Data Ordering Data The data set in its original state is purposely not ordered by any criterion, such as alphabetical order of countries, etc. But we can use R to do exactly that. Let us work with a subset containing only three variables: EU_subset &lt;- select(EU, country, pop18, access) It would be lovely if the command for ordering data would be called order(), but it is called arrange()5. Let’s order countries by ascending population in a new data frame called eu_order: eu_order &lt;- arrange(EU_subset, pop18) Base R Solution eu_order &lt;- EU_subset[order(EU_subset$pop18),] We can now display the first 10 rows with the following command: eu_order[1:10,] # A tibble: 10 × 3 country pop18 access &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Malta 475701 2004 2 Luxembourg 602005 1951 3 Cyprus 864236 2004 4 Estonia 1319133 2004 5 Latvia 1934379 2004 6 Slovenia 2066880 2004 7 Lithuania 2808901 2004 8 Croatia 4105493 2013 9 Ireland 4838259 1973 10 Slovakia 5443120 2004 The content in the brackets refers to the rows (before the comma), and to the columns (after the comma). As we only want certain rows and displaying all variables, I have left the space after the comma blank. We can do the same thing in descending order by calling: eu_order &lt;- arrange(EU_subset, desc(pop18)) eu_order[1:10,] # A tibble: 10 × 3 country pop18 access &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Germany 82850000 1951 2 France 67221943 1951 3 United Kingdom 66238007 1973 4 Italy 60483973 1951 5 Spain 46659302 1986 6 Poland 37976687 2004 7 Romania 19523621 2007 8 Netherlands 17181084 1951 9 Belgium 11413058 1951 10 Greece 10738868 1981 Base R Solution eu_order &lt;- EU_subset[order(desc(EU_subset$pop18)),] A neat feature of R is that it allows us to order observations by more than one variable. So for example, we could order them by ascending accession wave first, and then by ascending population in 2018 as follows: eu_order &lt;- arrange(EU_subset, access, pop18) eu_order[1:10,] # A tibble: 10 × 3 country pop18 access &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Luxembourg 602005 1951 2 Belgium 11413058 1951 3 Netherlands 17181084 1951 4 Italy 60483973 1951 5 France 67221943 1951 6 Germany 82850000 1951 7 Ireland 4838259 1973 8 Denmark 5781190 1973 9 United Kingdom 66238007 1973 10 Greece 10738868 1981 Base R Solution eu_order &lt;- EU_subset[order(EU_subset$access,EU_subset$pop18),] # or slightly shorter eu_order &lt;- EU_subset[order(with(EU_subset, access,pop18)),] Grouping Data Looking at the last example, a question that might spring up is in which accession wave the joining countries brought the largest population increase on average to the EU. We can calculate summary statistics for a particular group by, well, grouping them. The first step is to group data into rows with the same value: eu_access &lt;- group_by(EU_subset, access) By the way: whenever you have grouped anything, and finished analysing data in this grouped version it is essential that you ungroup the data afterwards, so that you don’t unintentionally keep using the groups: ungroup(EU_subset) # A tibble: 28 × 3 country pop18 access &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Belgium 11413058 1951 2 Bulgaria 7050034 2007 3 Czechia 10610055 2004 4 Denmark 5781190 1973 5 Germany 82850000 1951 6 Estonia 1319133 2004 7 Ireland 4838259 1973 8 Greece 10738868 1981 9 Spain 46659302 1986 10 France 67221943 1951 # ℹ 18 more rows But let’s calculate the average population size per accession wave in an elegant command which combines multiple steps by using pipes: eu_popaccess &lt;- EU_subset %&gt;% group_by(access) %&gt;% summarise(avg = mean(pop18)) eu_popaccess # A tibble: 8 × 2 access avg &lt;dbl&gt; &lt;dbl&gt; 1 1951 39958677. 2 1973 25619152 3 1981 10738868 4 1986 28475164. 5 1995 8151880. 6 2004 7327746. 7 2007 13286828. 8 2013 4105493 Base R Solution eu_popaccess1 &lt;- aggregate(pop18 ~ access, data = EU_subset, FUN = mean ) You now see a new variable called avg which contains the average population increase for each wave. In which wave did the joining countries have the largest population on average? Combining Ordering and Grouping Data The question was easy to answer here, as we only have a few accession waves. It starts to get unwieldy though, the more groups we have, but we can let R do the job by combining first grouping, and then ordering. So we take the grouped data frame eu_popaccess and order it by descending avg: eu_popaccess_order &lt;- arrange(eu_popaccess, desc(avg)) eu_popaccess_order # A tibble: 8 × 2 access avg &lt;dbl&gt; &lt;dbl&gt; 1 1951 39958677. 2 1986 28475164. 3 1973 25619152 4 2007 13286828. 5 1981 10738868 6 1995 8151880. 7 2004 7327746. 8 2013 4105493 Base R Solution eu_popaccess_order &lt;- eu_popaccess[order(desc(eu_popaccess$avg)),] Descriptive Statistics We have covered quite a large number of descriptive statistics, so far. These are: Mean Median Mode Standard Deviation Variance Quartiles and Percentiles Range Interquartile Range They are a lot of effort to calculate by hand, especially for larger data sets, but R can do these with a few intuitive commands. First up is the mean. mean(EU$pop18) [1] 18311106 Then the median: median(EU$pop18) [1] 9300319 You can get information on the quartiles (remember that the median is the second quartile), the mean, as well as the minimum and maximum through one, simple command: summary(EU$pop18) Min. 1st Qu. Median Mean 3rd Qu. Max. 475701 3781345 9300319 18311106 17766718 82850000 Let’s now move to measures of variability. First up is the range; you can either calculate this with two commands by finding out the minimum and maximum separately, or just ask R to give you both values straight away: min(EU$pop18) [1] 475701 max(EU$pop18) [1] 82850000 range(EU$pop18) [1] 475701 82850000 The stadard deviation is rather long-winded to calculate by hand, but the R command is short and sweet: sd(EU$pop18) [1] 23787945 As you know, the variance is the squared standard deviation, but you can calculate it with its own command in R, too: var(EU$pop18) [1] 5.658663e+14 Descriptive Statistics Now that you have all the relevant tools at hand, complete the following tasks: Generate descriptive statistics for 3 of our variables. Recode the variable ‘GDP_2015’ into a ordered factor called ‘gdp_level’ with three levels called “low”, “medium”, and “high” with cut-off points of your own choosing. Produce a tabulation for ‘gdp_level’. Graphs R is probably the most powerful statistics programme for creating graphs. As this is an introductory level module, and we only have so much time available in the seminars, I will only be able to introduce you to the most commonly used ones; in the first instance histograms and boxplots. I will then introduce you to the package ggplot2 which is simply the best invention since sliced bread, as it gives you pretty much endless optionality in customising graphs to show exactly what you want. Whenever you produce a graph and you use it in an essay, your dissertation, or article, it is crucial that the graph is able to communicate its message independently from the text. So, a reader should be able to understand the graph and be able to appreciate fully its message without having to read the text. In a similar fashion, the text should always be written in such a way that a reader is able to understand it without having to look at the graph. This is a principle which equally applies to tables (more on this on PO12Q). If you do not follow this principle in the assessments on my modules, you will be marked down. Chapter 4 in “The Visual Display of Quantitative Information” by Tufte (2001) is on the reading list as an essential item, but there are some more principles he sets out at the start of the book (p. 13) which are worthwhile repeating here: Excellence in statistical graphics consists of complex ideas communicated with clarity, precision, and efficiency. Graphical displays should show the data induce the viewer to think about the substance rather than about methodology, graphic design, the technology of graphic production, or something else avoid distorting what the data have to say present many numbers in a small space make large data sets coherent encourage the eye to compare different pieces of data reveal the data at several levels of detail, from a broad overview to the fine structure serve a reasonably clear purpose: description, exploration, tabulation, or decoration be closely integrated with the statistical and verbal descriptions of a data set. Graphics reveal data. Indeed graphics can be more precise and revealing than conventional statistical computations. Basic Graphs Let’s start with a histogram of the variable pop18. The range of the pop18 variable is about 82 million - this is rather unwieldy to imagine and also to put onto axes of graphs, as they would mostly consist of zeros. So let’s express the population of each countries in million instead: EU$popmio &lt;- EU$pop18/1000000 We can now produce a histogram. Before doing this, it is sensible to think about the number of bars we want in the histogram. The smallest country has just shy of 500,000 inhabitants, whereas the largest has over 82 million. So, I would like the x-axis to run from zero to 100 (million) and divide this into 5 bars. Accordingly, we are introducing 4 breaks on the x-axis with the following command: hist(EU$popmio, breaks=4) This is certainly a histogram, but it does not conform to the principle of graphs that they should be able to communicate their message independently, yet. Take the label of the x-axis, for example, what does EU$popmio mean? You and I know, but somebody who doesn’t know R language wouldn’t. We can tell R to adjust the axis label, as well as the main title of the histogram as follows: hist(EU$popmio, breaks=4, xlab=&quot;Population in million&quot;, main=&quot;Histogram of EU Population (2018)&quot;) Histograms This is fine now. Ugly, but fine. I will show you how to do a boxplot next, and then I will take you through the process of making all this look a bit more jazzy. The command for the boxplot is very intuitive. The default in R is to arrange the boxplot vertically. I prefer them horizontally, and you can set this in an equally intuitive option. boxplot(EU$popmio, horizontal = TRUE) You will recognise the descriptives we calculated earlier with the summary() function: summary(EU$popmio) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.4757 3.7814 9.3003 18.3111 17.7667 82.8500 Explain the outliers on the right mathematically. Advanced Graphs The graphs we have produced so far are functional, but let’s be honest, they wouldn’t win any beauty contests. There is, as mentioned earlier, an amazing package called ggplot2 which changes this dramatically. You have already installed it as a part of the tidyverse. Otherwise, the function would be: install.packages(&quot;ggplot2&quot;) We can just load it: library(ggplot2) The “gg” in ggplot2 stands for “grammar of graphs”. You will be familiar with the term “grammar”” from learning a language already. In this context, we use grammar to build sentences by choosing and arranging a variety different components, such as subjects, verbs and objects. If you know how to do this properly, you can express exactly what you want to say. The grammar of graphs adopts this logic and specifies a number of different components which allow you to create a graph which is able to communicate exactly what you wish to show. ggplot2 has eight basic grammatical arguments: Table 2: GGPLOT Components Data Frame The data you wish to visualize. Aesthetic Mappings Here you specify how the data are assigned to colour, size, etc. For now, this is the variable for which we want to create a graphical distribution. Geom Short for “geometry”. Use a geom function to represent data points through geometric objects, such as points, lines, etc. Each function returns a layer. Stat You can include statistical summaries through this, such as smoothing, or regression lines. Position Position adjustments determine how to arrange geoms that would otherwise occupy the same space. Facets Facets divide a plot into sub-plots based on the values of one or more discrete variables. Scale Maps data values to the visual values of an aesthetic. For example female=pink, male=blue. Coordinates How do the numbers get translated onto the plot? We are not going to look at this on this module. I like to think of using these arguments like dressing myself in the morning. The minimum that common decency requires me to wear if I wish to leave the house is some underwear, some trousers, and a top. Depending on how I feel and what the weather is like, I can add more layers, like socks, a jumper, or a scarf. It is exactly the same with ggplot2. As a minimum to produce a plot you need a data frame, the aesthetic mapping and a geom. Once you have produced this minimalistic graph, you can modify it, by adding more components / arguments. As you can imagine the possibilities are almost endless, and we only have time to deal with the minimum here. This is not a problem, however, as most of the other grammatical arguments (Stats, Position, Facets and Scales) generally have sensible defaults. So how does this work in practice? Let us reproduce the histogram of the age variable. We start by calling ggplot2 and advise the function which data frame we wish to use (EU). In a second step, we add a geometry – in our case geom_histogram. Within the geometry, we need to specify for which variable we wish to create a distribution, or in the language of ggplot2 which variable we wish to map to the geom as our Aesthetic. To produce 5 bars again, we specify a bandwith of 20 million (this refers to popmio). ggplot(data=EU) + geom_histogram(mapping=aes(popmio), binwidth = 20) Annoyingly, ggplot places the axis ticks in the middle of each bar which is WRONG for histograms. They need to align with the boundaries of the bars. We do this by telling R the boundary of the plot: ggplot(data=EU) + geom_histogram(mapping=aes(popmio), binwidth = 20, boundary = 0) This has shifted the ticks to the left, but now R has decided to label the x-axis in steps of 25, whereas our bars have a bandwidth of 20. Once again, we have the variable name on the x-axis, instead of a label which anybody could understand. I also prefer “Frequency” on the y-axis, instead of “Count”. To address both of these concerns, we simply add a layer for each. First up are the axis ticks. Our variable is continuous, so we choose the scale_x_continuous option, and tell R to break the axis up into a sequence which starts with zero, ends at 100 and has steps of 20 in between. In the labs argument we adjust the labelling as intended: ggplot(data=EU) + geom_histogram(mapping=aes(popmio), binwidth = 20, boundary = 0) + scale_x_continuous(breaks = seq(0, 100, 20)) + labs(x=&quot;Population (in million)&quot;, y=&quot;Frequency&quot;) + theme_classic() I have also removed the background in line with the principles set out by Tufte (2001, p. 96) by adding theme_classic(). This is it. A graph which can communicate its message independently, and which looks aesthetically pleasing. In the present case we have the population, so displaying the frequency on the y-axis is sort of sensible, but usually we would be dealing with a sample. Here the count is not very telling and we would be using percentages, instead. Let’s do it! Even More Advanced Graphs Unfortunately, there is no easy, default way to do this in R, but necessitates a calculation within the ggplot command. Once more we call ggplot and use the EU data set, and select the geom geom_histogram. Again we specify the binwidth as 20 with a boundary of zero, and put popmio on the x-axis. Now comes the point where we need to do something new, because y is not equivalent to the frequency any more, but should be percentage. To achieve this we advise R to put the density there (which is the relative frequency from the the lecture in week 5), and multiply this density by 100 to get percentage. Nothing has changed on the scaling of the x-axis from the previous plot, so we can copy and paste the scale_x_continuous section, as well as the labelling of the x-axis. In this last step, we now also need to adjust the label of the y-axis, because this has now percentage on it, and not frequency. The result is this: ggplot(data = EU) + geom_histogram(binwidth = 20, boundary = 0, aes(x= popmio, y = (..count..)/sum(..count..)*100)) + scale_x_continuous(breaks = seq(0, 100, 20)) + labs(x=&quot;Population (in million)&quot;, y=&quot;percent&quot;) + theme_classic() Jazzy Graphs with GGPLOT Organising Code in the RScript Now is probably a good time to make you aware of how I have been organising code which runs over several lines. I could also have written the code of the last graph as ggplot(data = EU) + geom_histogram(binwidth = 20, boundary = 0, aes(x= popmio, y = (..count..)/sum(..count..)*100)) + scale_x_continuous(breaks = seq(0, 100, 20)) + labs(x=&quot;Population&quot;, y=&quot;percent&quot;) + theme_classic() but this would have made it rather difficult to disentangle and to spot the structure of the graph straight away. So it is also a good idea to structure the code in a logical way which allows a reader to understand it as easily as possible. R is very smart in the way it indents the next line after pressing “enter” in an RScript automatically to the appropriate position. You see for example that in ggplot(data = EU) + geom_histogram(binwidth = 20, boundary = 0, aes(x= popmio, y = (..count..)/sum(..count..)*100)) + scale_x_continuous(breaks = seq(0, 100, 20)) + labs(x=&quot;Population (in million)&quot;, y=&quot;percent&quot;) + theme_classic() the aes which belongs to the geom_histogram layer is indented just so it starts flush with the first argument (binwidth) within this layer. Exercises Using these commands, and moving beyond with the help of today’s reading, complete the following tasks: Produce two base-R graphs of different types (e.g. histogram, bar chart, box-and-whisker plot) for separate variables in the EU data set. Produce two ggplot graphs of different types (e.g. histogram, bar chart, box-and-whisker plot) for separate variables in the EU data set. Google to find more geoms. Captions for Tables and Figures in Word In essays, your dissertation and in articles, you will have to refer to tables and figures in the text. Now, you can do this by writing “the figure below”. But this is not very elegant. Also, what happens if you change the layout and all of a sudden “the figure below” becomes “the figure above”. This not only causes additional work because you have to edit the text and check all references to tables and figures once you are done (which is tedious beyond description), but there is also the risk that you miss one or a few in the process. MS Word has a nifty function that allows you to insert captions for figures and tables, and then to insert cross-references into the text which get updated automatically before you send the document to the printer. Here is how to do it: Say, you have a figure inserted into Word. You now click on it, then hover over the bottom right-hand square, and right-click with your mouse. From the resulting context menu you select “Insert Caption”. This results in the following window: Select whether the item you want to describe is a figure, or a table. Then make sure you place the caption “below” the item (this is default). Then type your caption into the box at the top, such as “Figure 1: Skewness of Distributions”. Make sure the caption is telling. The reader needs to know from the caption what the figure or table is about. When you click OK, the document looks like this: Now you start writing the text and come to the point where you refer to the figure in question. Here, all you have to do is to select “Insert” and “Cross-Reference”” and select the following options in the pop-up window: Your text will then look like this: You don’t have to worry now about the sequence of numbering any more. If you insert another figure above this one, and insert a cross-reference in the text again, the sequence is automatically updated and our former “Figure 1” becomes “Figure 2”. Tables and figures have separate sequences of numbering. One last word on the display of data in tables: DO NOT screenshot tables from R and insert them into your presentations. They look ugly and unprofessional. Make the effort and create a proper table, either in Word or Excel and populate it manually with the data from R. The insertion of captions and cross-references is the same as described above. https://cran.r-project.org/web/packages/↩︎ There is a command called order(), but it is not part of the tidyverse, and as this package is steadily on the rise in coding, I am only showing you this here.↩︎ "],["exercises-2.html", "Exercises Exploring the ESS – Core Exercises Exploring the ESS – Going Further", " Exercises Exploring the ESS – Core Exercises Open the ESS9 dataset in R and attach it. Identify the variable about “Left-right placement”. Check its formatting in the questionnaire: what do 77, 88 and 99 mean? What is the level of this variable? Summarise it with functions class, str, and head What do these functions return? Apply the same steps to the variable about the “European unification:”should go further or already gone too far”. Them display the first 20 values of the two variables above Check the structure of the ‘table’ function and tabulate the two variables Can you see a major difference between them regarding non positive answers (ie outside proposed scale)? Consider the function rm(list=ls()) What function rm stand for? How useful can this function be for future exercises? Univariate Statistics and Recoding Calculate the two means, using only valid values (check the mean function beforehand) In Left-right, regroup values, using three different methods: recode convert to factor format using as.factor assign value labels using levels 0-1 into “far left” 2-3 into “left”, 4-6 into “centre”, 7-8 into “right” 9-10 into “far right” Exploring the ESS – Going Further Using the new, transformed variables: Calculate means in the UK only. You may use the [variable == \"value\"] subscript How does this mean compare with the average in Europe? and with France? Present the frequencies and means of these three samples in one unique table Install and load the questionr package. Use its function ‘freq’ to calculate the same as above, but with percentages. Compare deviations from the mean in the UK and Germany using the function abs (absolute value, which means any value with all negative signs deleted) Compare mean, median, mode, variance and standard deviation in the UK and Germany Try 3 or more kinds or graphs with these two variables separately, using the most appropriate of original or transformed values. Assess and compare the relevance of each graph Tabulate the two variables in original format against each other using table(X,Y). Interpret the output. Repeat this with the transformed format: Interpret the output. Try the same using questionr::cprop Graph the two variables against each other using ‘plot’ (make sure you choose the right versions of the variables). What can you conclude from this graph? "],["homework-for-week-3.html", "Homework for Week 3", " Homework for Week 3 Students as Learners: Read ALL the indicative items for the concept you chose for the assessment. Students as Researchers: Draw the conceptualisation tree for the concept you selected for the assessment Choose at least three attributes for this concept Choose two measurements per attribute "],["glossary-1.html", "Glossary", " Glossary Table 3: Glossary Week 2 Term Description attribute A component or characteristic of a concept background concept The broad constellation of meanings and understandings associated with the concept (Adcock &amp; Collier, 2001, p. 531) concept Abstract ideas which form the building blocks of theories (Clark et al., 2021, p. 150) conceptualization Formulating a systematized concept through reasoning about the background concept, in light of the goals of research (Adcock &amp; Collier, 2001, p. 531) measurement Refers to the selection of a measure or variable reliability Refers to the extent to which repeated measurement produces the same results systematized concept A specific formulation of a concept used by a given scholar or group of scholars; commonly involves an explicit definition (Adcock &amp; Collier, 2001, p. 531) validity The extent to which the measure (variable) you choose genuinely represents the concept in question. The word comes from the Latin word “validus” which means “strong” variable An element of a conceptual component which varies. We also call these “measures” "],["flashcards-1.html", "Flashcards", " Flashcards New Functions This Week The data are available as a .csv file. R Functions This Week The data are available as a .csv file. All R Functions So Far The data are available as a .csv file. "],["downloads.html", "Downloads Documents Data Sets R Scripts", " Downloads Yes, all solutions and all RScripts are available to you without silly time restrictions. The reason is that I have come to realise that I am operating a module in a university, and not in a kindergarten. Of course, you can download and look at the solutions before you have given the exercises a go yourself first. By all means, cheat. But I can’t promise you that you will learn very much. It’s your choice. Documents PO11Q Bibliography Statistical Tables Data Sets EU.xlsx Example.xlsx R Scripts Week 2 Solutions "],["glossary-2.html", "Glossary", " Glossary Unless otherwise noted, the definitions are taken from Reiche (forthcoming). Table 4: Glossary for PO91Q Term Description analysis A detailed evaluation of data to discover their structure and relevant information to answer a research question asymmetry The notion that while X causes Y, Y does not cause X. It is established with temporal priority, manipulated events, and/or the independence of causes (see Brady (2011)) attribute A component or characteristic of a concept background concept The broad constellation of meanings and understandings associated with the concept (Adcock &amp; Collier, 2001, p. 531) categorical Describing the qualitative categories of a characteristic, for example different religions causal In order to establish a causal relationship, the following criteria must be met concurrently: Relevance of the variables within the broader theoretical and empirical context of the research Clear Theoretical Framework Clear conceptualization Exclusion of alternative explanations Asymmetry Significant (and sufficiently strong) statistical association central limit theorem In random sampling with a large sample size – where n=30 is usually sufficient – the sampling distribution of the sample mean \\(\\bar{y}\\) will be approximately normally distributed, irrespective of the shape of the population distribution concept Abstract ideas which form the building blocks of theories (Clark et al., 2021, p. 150) conceptualization Formulating a systematized concept through reasoning about the background concept, in light of the goals of research (Adcock &amp; Collier, 2001, p. 531) confidence interval A confidence interval constructs an interval of numbers which will contain the true parameter of the population (e.g. the mean) in \\((1-\\alpha)\\) times of cases. \\(\\alpha\\) is usually chosen to be small, so that our confidence interval has a probability of 95% or 99%. confidence level The confidence level is the probability with which the confidence interval is believed to contain the true parameter of the population and is defined as \\((1-\\alpha)\\) conflation A variable does not belong to the attribute in question, but to a different one (Munck &amp; Verkuilen, 2002, pp. 13–14) constant A variable which does not vary continuous Can assume any value within defined measurement boundaries critical value The critical value is a threshold that determines the boundary for rejecting the null hypothesis (H\\(_0\\)) in a hypothesis test. It is a point on the probability distribution of the test statistic beyond which the null hypothesis is rejected. The critical value is chosen based on the significance level (\\(\\alpha\\)) of the test, which represents the probability of making a Type I error (i.e., rejecting a true null hypothesis). cross-sectional data Look at different units (or cross-sections) \\(i\\) at a single point in time data Derives from the Latin which means . For our purposes it is a collection of numbers (or quantities) for the purpose of analysis data set A collection of numerical values for individual observations, separated into distinctive variables degrees of freedom Degrees of freedom express constraints on our estimation process by specifying how many values in the calculation are free to vary democracy A system in which the population chooses and holds accountable elected representatives through fair, free, and contested, multi-party elections. The human rights, civil rights, and civil liberties of individuals are protected by law dependent variable Is dependent through some statistical or stochastic process on the value of an independent variable descriptive statistics Summarise information about the centre and variability of a variable deviation The deviation \\(d\\) of an observation \\(y_{i}\\) from the sample mean \\(\\bar{y}\\) is the difference between them: \\(d=y_{i}-\\bar{y}\\) dichotomous Can only assume two mutually exclusive, but internally homogeneous qualitative categories discrete The result of a counting process distribution Refers to the display of the values a variable can assume, together with their respective absolute or relative frequency generalisability The ability to apply the findings made on the basis of a representative sample to the population histogram Displays through rectangles the frequency with which the values of a continuous variable occur in specific ranges hypothesis In statistics, a hypothesis is a formal statement about a population parameter or relationship between variables. Hypotheses guide statistical tests to determine whether data support or refute them. The hypothesis suggesting an effect or difference is called the alternative hypothesis. The alternative hypothesis is always paired with a null-hypothesis, suggesting no effect or difference. independent variable Influences or helps us predict the level of a dependent variable. It is often treated as fixed, or “given” in statistical analysis, and is sometimes also called “explanatory variable” interpretation The explanation of results to answer the research question interquartile range The difference between the 3\\(^{\\text{rd}}\\) and the 1\\(^{\\text{st}}\\) quartiles literature review An analytical summary of the literature relating to a particular topic with the objective of identifying a gap and thus motivating a research question mean Is equal to the sum of the observations divided by the number of observations measurement Refers to the selection of a measure or variable median Separates the lower half from the upper half of observations method A tool for systematic investigation mode Is the most frequently occurring value non-probability sampling In non-probability sampling not every unit has the same probability of being sampled normal distribution The Normal Distribution is a bell-shaped probability distribution which is symmetrical around the mean outlier Defined as a value larger than the third quartile plus 1.5 times the interquartile range, or the first quartile minus 1.5 times the interquartile range p-value The p-value indicates the probability of obtaining a result equal to, or even more extreme than the observed value, assuming the null hypothesis is true. Common thresholds for significance are 0.05, 0.01, and 0.001. A smaller p-value suggests stronger evidence against the null hypothesis. The p-value is denoted as \\(p\\). parameter A parameter is the value a statistic would assume in the long run. It is also called the Expected Value percentile In ordered data, the percentile refers to the value of a variable below which a certain proportion of observations falls population Collection of all cases which possess certain pre-defined characteristics population distribution The probability distribution of the population primary data Primary data are data you have collected yourself probability Refers to how many times out of a total number of cases a particular event occurs. We can also see it as the chance of a particular event occurring probability sampling In probability sampling all units have the same probability of entering the sample. In addition, all possible combinations of n cases must have the same probability to be selected QM The process of testing theoretical propositions through the analysis of numerical data in order to provide an answer to a research question quartile Divides ordered data into four equal parts and indicates the percentage of observations that falls into the respective quartile and below range The difference between the largest and the smallest observation redundancy Two or more variables measure the same sub-attribute (Munck &amp; Verkuilen, 2002, p. 13) reliability Refers to the extent to which repeated measurement produces the same results representative sample A sample which contains all characteristic of the population in accurate proportions research question A specific enquiry relating to a particular topic or subject. It forms the starting point of the research cycle sample A sub-group of the population sample distribution The probability distribution of a sample sampling The process of selecting sampling units from the population sampling distribution The probability distribution of a sample statistics, such as the mean. It can be derived from repeated sampling, or by estimation sampling error The extent to which the mean of the population and the mean of the sample differ from one another sampling method The way the sample is created secondary data Secondary data are data which have been collected by somebody else significance level The significance level, denoted by \\(\\alpha\\), is the threshold used in hypothesis testing to determine if a result is statistically significant. It represents the probability of rejecting the null hypothesis when it’s actually true (a Type I error). Common levels are 0.05 or 0.01, indicating 5% or 1% risk. We will cover this properly in Week 9. significance test A significance test is a statistical method used to determine whether observed data provide enough evidence to reject a null hypothesis. It calculates a probability of observing data as extreme as, or more extreme than, the actual sample results, assuming the null hypothesis is true Social Sciences Are concerned with the study of society and seek to scientifically describe and explain the behaviour of actors standard deviation The standard deviation s is defined as \\[\\begin{equation*}s=\\sqrt{\\frac{\\text{sum of squared deviations}}{\\text{sample size} -1}}=\\sqrt{\\frac{\\Sigma(y_{i} - \\bar{y})^2}{n-1}}\\end{equation*}\\] standard error The standard deviation of the sampling distribution. It is defined as:\\[\\begin{equation*}\\sigma_{\\bar{y}} = \\frac{\\sigma}{\\sqrt{n}}\\end{equation*}\\] symmetry In the context of causation, symmetry is understood as the law-like regularity of events. There needs to be a recipe (causal mechanism) which regularly produces effects from causes (see Brady (2011)) systematized concept A specific formulation of a concept used by a given scholar or group of scholars; commonly involves an explicit definition (Adcock &amp; Collier, 2001, p. 531) t-distribution The t-Distribution is bell-shaped and symmetrical around a mean of zero. Its shape is dependent on the degrees of freedom in the estimation process. test statistic A test statistic is a value calculated from the sample data that is used to decide whether to reject the null hypothesis (H\\(_0\\)) in a hypothesis test. It quantifies the degree to which the observed data diverges from what is expected under the null hypothesis. In a t-test, the test statistic is a t-value, which measures the distance between the sample mean and the (hypothesised) population mean, expressed in units of standard errors. theory A formal set of ideas that is intended to explain why something happens or exists (Oxford Learner’s Dictionaries, n.d.) Type I Error A Type I Error occurs when a null hypothesis (H\\(_0\\)) that is actually true is incorrectly rejected. It is also known as a false positive errors, as it suggests that an effect of difference exists, when, in fact, it does not. The probability of committing a Type I Error is denoted by the significance level (\\(\\alpha\\)) of the test, which is typically set before conducting the test (e.g. \\(\\alpha\\) = 0.05). This means that there is a 5% chance of rejecting the true null hypothesis. Type II Error A Type II Error occurs when a null hypothesis (H\\(_0\\)) that is actually false is incorrectly accepted (or not rejected). It is also known as a false negative error, as it suggests that no effect or difference exists when, in fact, there is one. validity The extent to which the measure (variable) you choose genuinely represents the concept in question. The word comes from the Latin word “validus” which means “strong” variable An element of a conceptual component which varies. We also call these “measures” variance Is equal to the squared standard deviation z-score The z-score, sometimes also referred to as z-value, expresses in units of standard deviation how far an observation of interest falls away from the mean. It is defined as \\[\\begin{equation*}z = \\frac{\\text{observation} - \\text{mean}}{\\text{standard deviation}}=\\frac{y-\\mu}{\\sigma}\\end{equation*}\\] "],["list-of-references.html", "List of References", " List of References Adcock, R., &amp; Collier, D. (2001). Measurement Validity: A Shared Standard for Qualitative and Quantitative Research. American Political Science Review, 95(3), 529–546. Brady, H. E. (2011). Causation and Explanation in Social Science. In R. Goodin (Ed.), The Oxford Handbook of Political Science. Oxford University Press. Clark, T., Foster, L., Sloan, L., &amp; Bryman, A. (2021). Bryman’s Social Research Methods (Sixth). Oxford: Oxford University Press. European Comission. (n.d.). Eurostat – Your Key to European Statistics. available online at https://ec.europa.eu/eurostat/data/database. King, G. (1995). Replication, replication. PS: Political Science and Politics, 28(3), 541–559. Munck, G. L., &amp; Verkuilen, J. (2002). Conceptualizing and Measuring Democracy: Evaluating Alternative Indices. Comparative Political Studies, 35(5), 5–34. Oxford Learner’s Dictionaries. (n.d.). available online at https://www.oxfordlearnersdictionaries.com/. Reiche, F. (forthcoming). Introduction to Quantitative Methods for the Social Sciences. Oxford: Oxford University Press. Tufte, E. R. (2001). The Visual Display of Quantitative Information (Second). Cheshire, Conn: Graphics Press. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
